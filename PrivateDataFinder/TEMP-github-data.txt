*******************
Molecule EC2 Plugin
*******************

.. image:: https://badge.fury.io/py/molecule-ec2.svg
   :target: https://badge.fury.io/py/molecule-ec2
   :alt: PyPI Package

.. image:: https://zuul-ci.org/gated.svg
   :target: https://dashboard.zuul.ansible.com/t/ansible/builds?project=ansible-community/molecule-ec2

.. image:: https://img.shields.io/badge/code%20style-black-000000.svg
   :target: https://github.com/python/black
   :alt: Python Black Code Style

.. image:: https://img.shields.io/badge/Code%20of%20Conduct-silver.svg
   :target: https://docs.ansible.com/ansible/latest/community/code_of_conduct.html
   :alt: Ansible Code of Conduct

.. image:: https://img.shields.io/badge/Mailing%20lists-silver.svg
   :target: https://docs.ansible.com/ansible/latest/community/communication.html#mailing-list-information
   :alt: Ansible mailing lists

.. image:: https://img.shields.io/badge/license-MIT-brightgreen.svg
   :target: LICENSE
   :alt: Repository License

Molecule EC2 is designed to allow use of AWS EC2 for provisioning of test
resources.

.. _quickstart:

Quickstart
==========

Installation
------------
.. code-block:: bash

   pip install molecule-ec2

Create a scenario
-----------------

With a new role
^^^^^^^^^^^^^^^
.. code-block:: bash

   molecule init role -d ec2 my-role

This will create a new folder *my-role* containing a bare-bone generated
role like you would do with ``ansible-galaxy init`` command.
It will also contain a molecule folder with a default scenario
using the ec2 driver (using ansible amazon.aws.ec2_instance collection).
Install the collection using
`ansible-galaxy install -r test_requirements.yml`.

In a pre-existing role
^^^^^^^^^^^^^^^^^^^^^^
.. code-block:: bash

   molecule init scenario -d ec2

This will create a default scenario with the ec2 driver in a molecule folder,
located in the current working directory.

Example
-------
This is a molecule.yml example file

.. code-block:: yaml

   dependency:
      name: galaxy
   driver:
      name: ec2
   platforms:
     - name: instance
       image_owner: "099720109477"
       image_name: ubuntu/images/hvm-ssd/ubuntu-bionic-18.04-amd64-server-*
       instance_type: t2.micro
       vpc_subnet_id: 
       tags:
         Name: molecule_instance
   provisioner:
     name: ansible
   verifier:
     name: ansible

All you need to do is fill in the subnet-id you want
to create your test instance into.
Then run

.. code-block:: bash

   molecule test

.. note::
   To make this work, you need to export your AWS credentials, as well as the AWS region you want to use, in your environment.

   .. code-block:: bash

      export AWS_ACCESS_KEY_ID=ACCESS_API_KEY
      export AWS_SECRET_KEY=SECRET_API_KEY
      export AWS_REGION=us-east-1

   You can read more about managing AWS credentials with Ansible modules
   in the official documentation of the `Ansible AWS modules `_

Documentation
=============

Details on the parameters for the platforms section are detailed in
``__.

Read the molecule documentation and more at https://molecule.readthedocs.io/.

.. _get-involved:

Get Involved
============

* Join us in the ``#ansible-molecule`` channel on `Freenode`_.
* Join the discussion in `molecule-users Forum`_.
* Join the community working group by checking the `wiki`_.
* Want to know about releases, subscribe to `ansible-announce list`_.
* For the full list of Ansible email Lists, IRC channels see the
  `communication page`_.

.. _`Freenode`: https://freenode.net
.. _`molecule-users Forum`: https://groups.google.com/forum/#!forum/molecule-users
.. _`wiki`: https://github.com/ansible/community/wiki/Molecule
.. _`ansible-announce list`: https://groups.google.com/group/ansible-announce
.. _`communication page`: https://docs.ansible.com/ansible/latest/community/communication.html

.. _authors:

Authors
=======

Molecule EC2 Plugin was created by Sorin Sbarnea based on code from
Molecule.

.. _license:

License
=======

The `MIT`_ License.

.. _`MIT`: https://github.com/ansible/molecule/blob/master/LICENSE

The logo is licensed under the `Creative Commons NoDerivatives 4.0 License`_.

If you have some other use in mind, contact us.

.. _`Creative Commons NoDerivatives 4.0 License`: https://creativecommons.org/licenses/by-nd/4.0/
import boto3
from flask import make_response
import json, os, uuid
from app import make_json_response
from query import get_manifest_data, get_metadata
from manifest_handler import ManifestHandler

class AwsHandler(ManifestHandler):
    """
    Sends manifest/metadata file to Amazon Web Services (AWS) S3 bucket, and
    returns a unique string that is required for the user to access the file on
    the bucket.

    """

    def __init__(self, aws_api_key=None, aws_secret_key=None, aws_bucket_name=None, aws_running_on_ec2=None, **kwargs):
        # initialize baseClass first
        super(AwsHandler, self).__init__(**kwargs)

        # now populate subClass properties
        self.aws_api_key = aws_api_key
        self.aws_secret_key = aws_secret_key
        self.aws_bucket_name = aws_bucket_name
        self.aws_running_on_ec2 = aws_running_on_ec2

        if aws_api_key is None and aws_secret_key is None and \
        aws_bucket_name is None and aws_running_on_ec2 is None:
            # Build filepath to config file
            dir_path = os.path.dirname(os.path.realpath(__file__))
            config_filepath = os.path.join(dir_path, 'config.ini')

            # Get AWS config
            import ConfigParser
            config = ConfigParser.ConfigParser()
            config.read(config_filepath)
            self.aws_api_key = config.get('cloud-options', 'aws-api-key')
            self.aws_secret_key = config.get('cloud-options', 'aws-secret-key')
            self.aws_bucket_name = config.get('cloud-options', 'aws-bucket-name')
            self.aws_running_on_ec2 = json.loads(config.get('cloud-options', 'aws-running-on-ec2').lower())

        else:
            raise("Error: AWS arguments must be provided via config.ini")


    def handle_manifest(self, request):
        """
        Send manifest_id file to S3 bucket
        Returns JSON containing 'manifest_id' if successfully uploaded to bucket,
            if error, returns JSON containing 'error'
        """
        string = ""

        ids = json.loads(request.data)['ids']
        data = get_manifest_data(ids) # get all the relevant properties for this file

        for result in data:
            string += result

        response_obj = self._upload_file(file_contents=string)

        response_str = json.dumps(response_obj)
        response = make_response(response_str)
        return make_json_response(response) #Stringified {'manifest_id': 'xxx-xxxxxxx-xxxx-xxxxx'}

    def handle_metadata(self, request):
        """
        Send metadata file to S3 bucket
        Returns JSON containing 'manifest_id' if successfully uploaded to bucket,
            if error, returns JSON containing 'error'
        """
        ids = json.dumps(json.loads(request.data)['ids'])
        data = get_metadata(ids)

        response_obj = self._upload_file(file_contents=data)

        response_str = json.dumps(response_obj)
        response = make_response(response_str)
        return make_json_response(response) #Stringified {'manifest_id': 'xxx-xxxxxxx-xxxx-xxxxx'}

    def _upload_file(self, file_contents):
        """
        Accesses AWS client and uploads file data.
        On upload success, seturns UID so user can later access the file.
        On upload error, returns error message. 
        """
        #Generate UID for naming file in bucket
        uid = str(uuid.uuid4())
        file_name = self.file_name + '_' + uid + '.tsv'

        session = boto3.Session()
        s3_client = None

        if not self.aws_running_on_ec2:
            s3_client = boto3.client(
                's3',
                aws_access_key_id = self.aws_api_key,
                aws_secret_access_key = self.aws_secret_key
            )
        else:
            credentials = session.get_credentials()

            # Credentials are refreshable, so accessing your access key / secret key
            # separately can lead to a race condition. Use this to get an actual matched
            # set.
            credentials = credentials.get_frozen_credentials()

            # Get the service client
            s3_client = boto3.client(
                's3',
                aws_access_key_id = credentials.access_key,
                aws_secret_access_key = credentials.secret_key,
                aws_session_token = credentials.token
            )

        try:
            response = s3_client.put_object(
        	Bucket = self.aws_bucket_name,
        	Body = file_contents,
        	Key = file_name,
        	ServerSideEncryption = 'AES256'
            )
            return {'manifest_id': uid}
        except:
            return {'error': 'Unable to upload file to AWS bucket.'}
# AWS-Api-Gateway-Authz Terraform

This Terraform implementation allows to provision a quickstart setup of all AWS resources required for testing purposes and is intended for users who are familiar with Terraform.

The following library will create a new VPC with the following resources: an API gateway, a Lambda function serving as an authorizer , an Elasticache Redis instance and an EC2 instance hosting the OPA/PDP docker.

## Instructions

###  Terraform Params:

Update the terraform.tfvars file with the following mandatory parameters:  
1.AWS_SECRET_KEY= "XXXXX"  
2.AWS_ACCESS_KEY= "XXXXXX"  
3.PRIVATE_KEY_PATH = "XXXXXX"  
3.PRIVATE_KEY_PATH = "XXXXXX"  

\* Create a new, unencrypted key, to allow Terraform to establish it on the EC2 instance.  

\* Install the required opa_auth_lambda dependencies by running:
```
cd aws/opa_auth_lambda ; pip3 install -r requirements.txt -t .  
```
\* The optional POLICY_PATH variable denotes the relative path to the policy package. By default, the policy path would be `authz`. In case your policy package name was changed from `authz`, make sure you update this variable. 


As listed on the variables file, it is also possible to modify the Region & Availability zone (default being eu-west-2).  
You will need to populate the ami map according to your chosen region with the relevant amazon linux x64 architecture ami.

###  Running Terrraform:   
Once all parameters have been set, run the following:
```
Terraform init  
Terraform plan -out  
Terraform apply ./
```
### Setup authorizer on method request:  
Navigate via AWS console to the API Gateway Service.   
Click on "Method Request" (see attached screenshot)   
![Alt text](./method-request.png?raw=true "Method")  

Under Settings>Authorization choose gateway-opa-auth(see attached screenshot)  

![Alt text](./authorization.png?raw=true "Method") 



### Start up the PDP:

After the terraform-apply process has completed successfully, do the following:

1. Locate your newly created Redis instance under the Elasticache service on AWS and copy the value from it's primary endpoint field.  

2. Retrieve the EC2 instance public DNS (filter via tag  Name = "EC2_PDP" ) from the AWS console and use your configured key to ssh to it.
Then, on the EC2 instance, run the following command:
```
docker run \
    -e RATE_LIMITER_REDIS_ENDPOINT= \
    -e RATE_LIMITER_REDIS_PASSWORD= \
    -e RATE_LIMITER_DURATION= \
    -e API_KEY= \
    -e API_SECRET= \
    -e CONTROL_PLANE_ADDR="https://api.poc.build.security/v1/api/pdp" \
    -p 8181:8181 \
    --name pdp \
    buildsecurity/pdp
```
Verify the container is running & that the PDP has retrieved the policy successfully.
# Distributed crawling infrastructure

This software allows you to crawl and scrape the Internet in scale.

It supports basic crawling via http as well as sophisticated crawling with the help
of a heavily customized headless chrome browser controlled via puppeteer.

The aim is to be able to scrape/crawl websites that try to lock out automated bots. In our opinion, as long as the overall network throughput is conservative and the crawler doesn't drain any resources or is placing a burden on websites, it should be allowed to
extract information from **public datasets**.

Platforms don't own the data that they collect from their customers. On the same time they generate a lot of wealth with said data. This is an attempt to give developers more access to data in the public domain again.

If you want to get access to data crawled by plain http requests, please have a look at the [common crawl project](https://commoncrawl.org/). However, if you need to access data that is only shown with activated JavaScript or a modified browsing fingerprint that evades common detection techniques, this project might be for you.

## Vision

The vision of this project is to provide a **open-source, general purpose crawling infrastructure** that enables it's users to

- crawl any website by specifying a simple crawling function ([Examples](https://github.com/NikolaiT/scrapeulous))
- crawl with distributed machines
- allocate and destroy crawling endpoints based on crawling need (only rent computing instances when you need them)
- use the cheapest infrastructure as crawling endpoints (currently AWS Spot Instances)
- leverage cloud technology and big data technologies
- configure browsers in a way that it's (nearly) impossible for anti-detection technologies to find out that the crawler is a machine
- integrate external captcha solving services
- use any proxy provider that you want

## Scraping Service - [Scrapeulous.com](https://scrapeulous.com/)

This project is a open source tool and will remain a open source tool in the future. We need the collaborative brain of the open source community in order to create a state of the art crawling software.

However, some people would want to quickly have a service that lets them scrape public data from Google or any other website. For this reason, we created the SaaS service [scrapeulous.com](https://scrapeulous.com/).

## Architecture

The architecture of the crawling infrastructure is quite complex and is summarized in the diagram below:
![architecture](docs/diagram/arch_diagram2.png "crawling infra")

## Technical Introduction

Crawling soon becomes a very complicated endeavor. There are a couple of sub problems:

### Cat and mouse game between bots and anti-bot companies

The basic goal is to make your crawler indistinguishable from a human that controls a browser. This is a very
complicated task, since anti-bot companies observe and process a wide variety of data such as:

+ IP addresses and geolocation (mobile, data-center, residential IP address?)
+ The browser fingerprint (OS, plugins, Canvas, WebRTC, ...)
+ Mouse movements and the kybernetics of how the browser is handled

This is a never ending fight between the cat (detection companies) and the mouse (crawler).

We don't want to impose a burden on websites, we just want fair access to data.

[Current research](https://hal.inria.fr/hal-02441653/document) demonstrates how complicated this push and pull game has become.

### Robust queuing and handling distributed crawlers

Crawling is distributed onto several machines/servers. Therefore, there needs to be some kind of advanced algorithms that
handles queues and schedules new tasks in an efficient way to avoid potential bottlenecks.

### Infrastructure

Crawling endpoints must be able to be allocated fully automatic and based on crawling requirements. Furthermore, the cheapest server infrastructure must be rented (currently AWS Spot instances I guess).

As an alternative, crawling endpoints can be run on serverless cloud computing providers such as AWS Lambda or Microsoft Azure Functions to obtain scalability and avoid fixed costs.

The downside is that we cannot keep browsers open when we are making use of a on-demand serverless architecture.

### Big Data

When crawling many million urls, you cannot simply store the results in a CSV file. Data needs to be stored in the cloud (for example AWS S3) and there needs to be some kind of streaming post processing.


## Todo List for the near Future

**We need a lot of help** with the following issues:

0. See if switching from docker swarm to kubernetes has advantages and benefits
1. Stay on top of the cat and mouse game:
    - Find new evasion techniques. Test how Google & Bing blocks. Is it solely based on IP addresses?
    - Make fully use of [uncaptcha](https://github.com/ecthros/uncaptcha)
    - Integrate intelligence of the [research paper from Antoine Vastel et al.](https://hal.inria.fr/hal-02441653/document)
    - Make use of newest contributions from [puppeteer-stealth](https://github.com/berstend/puppeteer-extra/tree/master/packages/puppeteer-extra-plugin-stealth)
    - Use some detection evasion techniques from the no longer maintained [headless-chrome-crawler](https://github.com/yujiosaka/headless-chrome-crawler)   

2. Use the most recent version of chromium & puppeteer in AWS Lambda and Azure Functions with the package [chrome-aws-lambda](https://github.com/alixaxel/chrome-aws-lambda) [done]

3. Testing, testing, testing! Test the code base much better and expand *worker* tests such as found here: `crawler/test/`


## Documentation

+ [Full documentation for the APIs]()
+ [A tutorial how to scrape google and extract email addresses from the urls]()

## Quick Installation & Compilation

You need to have a recent version of nodejs, npm and yarn installed on your development environment.

Install typescript globally:

```bash
sudo npm install -g typescript
```

### Master

Includes the RESTful API that accepts new crawl tasks and loads the items into mongodb queue.

Location: `master/src/`

Compile & install the project locally:

```bash   
cd master/

npm install

# switch to the library and install and compile it first!
cd ../lib/
tsc

# go back to the master and compile it
cd ../master/
tsc

# now we can run the Api express server
node dist/master/src/server.js
```

To view and understand how the API works, please visit the swagger API documentation at the url: **http://localhost:9001/swagger/**

The master also includes the crawling scheduler. It's purpose is to maintain the crawling throughput of each
task created via the API.

To run the daemon, execute the following command:

```
node dist/master/scheduler/daemon.js
```

### Crawler

This is the actual crawling module. The crawling module either runs within a docker swarm cluster, kubernetes cluster or on AWS Lambda or Google Cloud Functions or Azure Functions. It's up to you which backend do you want to use to execute your crawler nodes.

Upload the crawler to AWS lambda with:

```
npm run deploy
```

You will need to have serverless installed globally:

```
npm install -g serverless
```


### Test the Crawler

Testing the crawler module is an extremely important task, because failure in the crawling logic is fatal when you are
having huge crawl task over millions of items in the queue.

In order to test the crawler, use the following commands to create the Docker Crawler image.

```bash
cd crawler/
npm install

# install & compile lib
cd ../lib
npm install
tsc

cd ../crawler
./build.sh
```

After the image was successfully built, you can run the integration test with the following command:

```bash
mocha --timeout 300000 -r ts-node/register test/integration_tests.ts
```

## Tutorial

You want to run your own scaleable crawling infrastructure? That is only possible if you have the following resources:

1. You own a AWS/Azure/Google Cloud account
2. You have the requirements to execute long running crawl tasks over hundred thousand and millions of items
3. You know the implications of what you are doing

This tutorial is divided into three parts.

1. Install the distributed crawling infrastructure within the AWS cloud infrastructure
2. Start a crawl task that will crawl the Html of the top 10.000 websites and store the cleaned Html documents on s3. For the top 10k websites, we use the scientific [tranco list](https://tranco-list.eu/): A Research-Oriented Top Sites Ranking Hardened Against Manipulation. This list offers several improvements over the old Alexa top 1M website ranking list. For more information, please visit their website.
3. As a concluding task, we run business logic on the stored Html files. For example, we extract all urls from the Html documents or we run some analytics on the meta tags found in the `` section of the documents.

In order to follow this tutorial, you will at least **require an
AWS account**. We will make use of the following AWS services:

+ AWS Lambda as a crawling backend
+ AWS S3 to store crawled Html data
+ An AWS EC2 instance used as a master server that schedules the crawl task and hosts the mongodb that we use a queue


### Setting up the infrastructure

All actions are performed in the AWS web dashboard and we choose the region **us-east-1 (north Virgina)** as a default region.

First we need to install a Ubuntu 18.04 server on Amazon AWS EC2 with docker support. Additionally, we will assign a elastic IP address to the newly created instance.

Therefore, we login to our AWS console and go to Services -> EC2 and then we press the Button *Launch Instance* and search for *Ubuntu 18.04 LTS* on the **AWS Marketplace** tab.

We select the Ubuntu 18.04 LTS - Bionic image. This is what you should see:
![alt text](docs/images/ami_instance.png "Ami Instance Selection")

We will select this AMI image and select the machine size of `t2.medium` (2vCPU and 4GiB memory). Maybe it's also possible to use the size `t2.small`, we haven't tested it.

This is what you should see after setting up the instance:
![alt text](docs/images/ec2_setup.png "Setting up the instance")

Then we click on **launch** and for the last step we have to create a key pair to access our instance. We download this PEM file and store it on our local file system for later (I named the PEM file `tutorial.pem`).

Before we can access our instance, we assign an elastic IP address to our launched instance.

We navigate to Services -> EC2 -> Elastic IPs and we click on **Allocate Elastic IP address** and we create a new elastic IP from Amazon's pool. Then we assign this elastic IP address to the previously created EC2 instance. You should remember this public IP address for the remainder of the tutorial. Let's assume the elastic IP address is: `34.193.81.78`.

As a last step, we assign a permissive Security Group to the allocated instance. In my case, I just allowed all traffic from all sources on all port ranges by default. It's not really secure, but I will destroy the instance anyway after a couple of hours.

If you want to restrict TCP/IP traffic with the firewall, the following ports need to be open: 22, 80, 9001, 8080.

Now that our instance is launched, we can access it with the following shell command

```bash
chmod 0700 ~/keypairs/tutorial.pem

ssh -i ~/keypairs/tutorial.pem ubuntu@34.193.81.78
```

### Creating AWS keys for production use

The crawling infrastructure needs AWS Api keys for AWS Lambda and S3 in order to work properly.

In the AWS Dashboard, go to Services -> IAM and create a new user with the name *crawling-user* and check the selected field *programmatic access*.

Click on the tab *Attach existing policies directly* and add the following access policies:

+ AWSLambdaFullAccess
+ AmazonS3FullAccess
+ CloudWatchFullAccess

As a last step, download the generated `credentials.csv` file. Store this credentials file for later.

### Commands on the Server

Create a user for the master server. In this tutorial, we will use the user `ubuntu` for deploying the crawl master server.

```bash
# become root user
sudo su

adduser master

usermod -aG sudo master

su - master
```

As a next step, [Install docker and docker swarm with digitalocean instructions](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04).

check that docker is correctly installed

```bash
sudo systemctl status docker
```

Add the `ubuntu` user to the docker group:

```bash
sudo usermod -aG docker ubuntu

# confirm adding the user was a success
# logout/login from the current shell session first
id -nG
```

#### Installing nodejs, yarn and typescript on the server

Installing Node on Ubuntu 18.04 tutorial: https://linuxize.com/post/how-to-install-node-js-on-ubuntu-18.04/

The following commands need to be executed:

```bash
curl -sL https://deb.nodesource.com/setup_10.x | sudo -E bash -

sudo apt-get install -y nodejs

node --version
v10.20.1

npm --version
6.14.4
```

Then install the typescript compiler globally:

```bash
sudo npm install -g typescript

tsc --version
Version 3.8.3
```

Then we install yarn with the [following instructions](https://classic.yarnpkg.com/en/docs/install/#debian-stable):

```bash
curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add -
echo "deb https://dl.yarnpkg.com/debian/ stable main" | sudo tee /etc/apt/sources.list.d/yarn.list

sudo apt update && sudo apt install yarn

yarn --version
```

### Clone the project and install & compile

Go to cozy place on your local file system and download the crawling infrastructure project with the command:

```bash
git clone https://github.com/NikolaiT/Crawling-Infrastructure.git

cd Crawling-Infrastructure/
```

Now compile the project locally. We need to have a recent node version installed for that. Then we install typescript globally with the command

```bash
sudo npm install -g typescript
```

Now we are ready to install & compile the project locally:

```bash
cd master/

npm install

# switch to the library and install and compile it first!
cd ../lib/
tsc

# go back to the master and compile it
cd ../master/
tsc
```

After that step, the crawling infrastructure should be compiled successfully.

### Deploy the Master server

Now we need to configure the deployment. We edit the file `master/deploy/env/deploy.env` file by specifying the following contents.

```bash
# edit the IP address of the AWS EC2 instance that you just created
export SERVER=master@34.193.81.78
export PEMFILE=/path/to/saved/pemfile.pem

# those are the paths to the deployed crawling infrastructure
# on the remote master server
export REMOTE_MASTER_DIR=/home/ubuntu/master/
export REMOTE_LIBRARY_DIR=/home/ubuntu/lib/
```

Now we update the environment configuration file for the master server in production mode. This environment file includes
all the settings that the master server scheduler & Api need in order to work properly. We edit the file `master/env/skeleton_production.env` and update & modify the missing parameters.

For example, our environment file should look similar to the following:

```env
NODE_ENV=production

AWS_ACCESS_KEY=awsAccessKeyGeneratedPreviously
AWS_SECRET_KEY=awsSecretKeyGeneratedPreviously
AWS_REGION=us-east-1

API_PORT=9001
API_HOST=0.0.0.0

CRAWL_WORKER_PORT=3333

MONGO_INITDB_ROOT_USERNAME=admin
MONGO_INITDB_ROOT_PASSWORD=someSuperSecureMongoDBPassword

MONGO_DATA_DIR=/data/db
MONGO_TEST_DATA_DIR=/data/test_db
MONGO_LOG_FILE=/var/log/mongodb/mongodb.log

MONGODB_CONNECTION_URL=mongodb://admin:someSuperSecureMongoDBPassword@34.193.81.78/

MASTER_IP=34.193.81.78

API_URL=https://34.193.81.78:9001/
API_KEY=someCrawlingInfraApiKey
DEMO_API_KEY=someDemoCrawlingInfraApiKey

DOCKER_STACK_NAME=Master

USE_REMOTE_DB=1
```

The variables `AWS_ACCESS_KEY` and `AWS_SECRET_KEY` contain the AWS credentials that we created in the earlier step.
The variables `MONGO_INITDB_ROOT_USERNAME` and `MONGO_INITDB_ROOT_PASSWORD` are the credentials for the mongodb database.

You can choose arbitrary credentials for the variables `MONGO_INITDB_ROOT_PASSWORD`, `API_KEY` and `DEMO_API_KEY`.

As a last step, we rename the file from `master/env/skeleton_production.env` to `master/env/production.env`.

Also, create an empty `master/env/development.env` file. It is required by docker swarm.

Now we are ready to deploy the project to our recently created master server with the commands:

```bash
cd master

./deploy/deploy.sh deploy
```

The above command will compile the source code locally and deploy to the remote server and initialize a docker swarm there.

Now we have to create a default configuration on master server with the following commands:

```bash
node ctrl.js --action cfg --what create
```

You can test if deployment was successful by executing the following command:

```bash

$ ./deploy/test.sh
https://34.193.81.78:9001/
{
  "scheduler_uptime": "",
  "scheduler_version": "v1.3",
  "api_version": "1.0.2",
  "machines_allocated": 0,
  "http_machines_allocated": 0,
  "num_total_items": 0,
  "num_tasks": 0
```

#### Deploying the crawler to AWS Lambda

As a last deployment step, we need to deploy our crawler to AWS Lambda. [AWS Lambda](https://aws.amazon.com/de/lambda/) is a serverless computational service that lets you run your code for a maximum of five minutes. The AWS Lambda Api offers scalability and a pay-per-used-resources billing scheme. Without AWS Lambda, we would need to rent our own VPS servers to do the actual crawling work. Controlling own crawling servers is also supported by this project, but for the sake of this tutorial we will use AWS Lambda as crawling backend.

First we switch the directory to `worker/`.

Then we need to [install the serverless](https://serverless.com/framework/docs/providers/aws/guide/installation/) framework globally on our machine:

```bash
sudo npm install -g serverless
```

and we need to install typescript globally with the command:

```bash
sudo npm install -g typescript
```

Then we have to define to what regions we want to deploy our functions to. Update the file `crawler/deploy_all.js` and edit the functions starting on line 20 of the script.

```JavaScript
function deploy_crawl_worker() {
    console.log(systemSync('npm run build'));

    let regions = [
        'us-west-1',
        'us-west-2',
        'us-east-2',
        'us-east-1',
    // ...
```

After that we have to actually create S3 buckets on those regions, otherwise our crawled data could not be correctly stored. You can create AWS buckets programmatically with the script `scripts/create_buckets.sh` with the command:

```bash
./scripts/create_buckets.sh
```

Now that we have created those buckets, it's time to update the available regions on our master server. We change the directory to `master/` and issue the following commands:

```bash
cd master;

export $(grep -v '^#' env/production.env | xargs -0);

node ctrl.js --action cfg --what update_functions
```

Now the master server has the correct functions configured.

After those steps, it's finally time to upload our crawling code to AWS Lambda.

We can do this with the following commands:

```bash
# change back to worker directory
cd ../crawler;

export $(grep -v '^#' env/crawler.env | xargs -0);

node deploy_all.js worker
```

### Configure the Master server

By executing the following command, you open the web interface to administer the crawling infrastructure:

```bash
./launch_frontend_interface.sh
```

After that, we need to change some configuration parameters. We click on the **Config** tab in the menu and update
the options **Browser Lambda ARN** and **Http Lambda ARN**. We need to specify our correct AWS Account id. So for example, we save the
value `arn:aws:lambda:{region}:7685894858585:function:crawler-dev-browser-crawler` for **Browser Lambda ARN**.

### Testing the Installation

Now the crawling infrastructure should be ready to work. We will test our work by simply creating a crawl job that obtains the IP address by visiting the url `https://ipinfo.io/json` and return it.

You can issue the following command in order to create this test task:

```bash
export $(grep -v '^#' env/production.env | xargs -0);

node ctrl.js --action create_test_task
```

and after a couple of moments the task should be finished and we can download the results from the S3 storage. We go to the crawling dashboard for that with

```bash
./launch_frontend_interface.sh
```

and then go to Tasks and click on **Download Sample** on the task that was just created. We will get a tar.gz file with the crawling results.

### Creating the top 10k crawl task

The file that contains the top 10k urls can be found here: https://raw.githubusercontent.com/NikolaiT/scrapeulous/master/items/top10k.txt

With the following command, you can start the crawl task that launches the crawling of the top 10.000 urls:

```bash
node ctrl.js --action create_10k_task
```

Alternatively, you can also create the crawl task via the web frontend that is started with the command `./launch_frontend_interface.sh`.

In order to create the task, you have to enter the following parameters in the web frontend.

![alt text](docs/images/crawl_task_config.png "Configuration for the crawl task of the top 10k websites")

### Analyzing the results

After the crawl task is completed, we will download the Html files that were stored in S3 storage. In order to obtain the download instructions, we
go to the web frontend to the *tasks* tab and then click on Actions -> Get Download instructions.

This shows us a bash script that contains commands to download the files from the cloud. We save and execute this script:

```bash

chmod +x download.sh

./download.sh
```

After the script is done downloading the files, all Html files are stored in the directory `/tmp/storage/`. After having saved the crawling results, we can actually run some business logic on it.

We will create a simple & concise node program in order to analyze and process the results.

#### Url analysis

The first simple task is to find all links in the Html documents and count the domains. We want to find out which domain occurs most across all the top 10.000 websites.

There is a node script that does exactly this job and it can be found in `master/other/domain_count.js`. Whene executing said script, we obtain
the following results:

```bash
/master$ node other/domain_count.js
jrj.com.cn 869
www.theguardian.com 678
china.com.cn 540
www.gov.cn 522
uol.com.br 518
www.mama.cn 408
www.wayfair.com 406
en.softonic.com 402
nypost.com 367
fortune.com 356
www.politico.com 348
www.nvidia.com 347
www.cnbc.com 329
www.suara.com 327
www.wsj.com 305
www.fandom.com 305
www.liputan6.com 267
www.foxnews.com 252
www.forbes.com 247
elpais.com 187
ext.hp.com 187
blog.csdn.net 186
twitter.com 182
www.buzzfeed.com 180
www.state.gov 175
www.theverge.com 172
www.ebay.com 170
www.facebook.com 167
www.linkedin.com 165
dailymail.co.uk 163
www.microsoft.com 152
www.babytree.com 143
www.virginmedia.com 136
finance.17ok.com 132
archive.org 131
```

The results above contain the Html of around 300 websites in that top 10k list. Therefore we haven't analyzed the full 10.000 Html files.

#### Meta Tag analysis

It's also interesting to analyze the meta tags in the stored html files.
###### Default settings, override in .env.local file that is auto-generated on install ######
DATABASE_URL=postgresql://magick:magick_default_pw@localhost:5432/magick
SHADOW_DATABASE_URL=postgresql://magick_shadow:magick_shadow_default_pw@localhost:5433/magick_shadow
PORTAL_DATABASE_URL=postgresql://magick:magick_default_pw@localhost:5432/magick?schema=portal
PORTAL_SHADOW_DATABASE_URL=postgresql://magick_shadow:magick_shadow_default_pw@localhost:5433/magick_shadow?schema=portal

# Memory to use for build cache
NODE_OPTIONS="--max-old-space-size=8192"

# Server settings
PAGINATE_DEFAULT=100
PAGINATE_MAX=1000

# Change this in production to something impossible to guess
JWT_SECRET=secret

DUMMY_TOKEN=eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIn0..v5i3ZOjE6D6i3np7.9EIC2-3ykdKdBWy7QVJwLrtvVJYUS4dJPclH00lYUQr7zJt157MLvkS5Tkz_XUXlnd9PkF7h2_EiMmyzEJbm8QXXFAtNLEAs76RYuCcPtDtXjC-IMb9-Ag-r8Oxq2x1teuyhmVjBNIOqbaw6Q_Rks9ZABsa5AvDr0eTYicp9eHs8BUYQltb0hXfh6nkuNAybdAsyGcXecgHVTgqLAZ6odT3W3VxOy2BXjZq2bnodF4UkHwrGoVgDxVyGux3FFz6lhGjpuKAbQDQWHyqU9jQWjqDMAw93wggeMskWMcGoyopYTqcC0OXAmPBGKYWqnhhoOkcEa3KOX4tSSbjj5HcxaZMAKBJe-ndu2PvCa4weVdV6QHl0cpyctgWTz4E7SHOYMgQF-JKuFbG4HUfR7YKFwgD9HtwGtnUkoL03N6tI4d1v-KU2uqyz75yF-YSRpDjnNReHmCjqaIXsv4rxck5hqa5ax8d1VuXtMucvxFT3QtZlt0oekhzfcOWrQV3QdNqU7cEX_OJ_10w3jlmzXkIsmZOjQqHTErBciYi3-qCMzFMtGiBYFil4aFdaEVtwsNCTqpo-jpU5VzoVHXz6-06xKRDLxfWI9RJIYdsy5kWe0jGiutSU7y-1Iec21Zk4r8u011zhOMetd6GmYoAv_-IgiZlJWUyWAp4xtv7ZFBSOpkQjqUlnFq_VGZvRcEExIVpQBRMfcUbQh3rh9nKpe0x7IolMka37DYSUP7IJf231HxsEP8zH2Nk3IP9-_eEqx9QH11MYvIDWJrQe8ijZWFzBw3lwkHvAyeB3HstAE5gWqnSEJ2wJeOI9aWhH2qpPLLjJsInvKep98CebyA.Mk1hveE9GqVWzgScFPrLCQ

# Client settings
###### VITE_APP_ variables are used by the client ######
VITE_APP_TRUSTED_PARENT_URL=http://localhost:4000

# Posthog settings
VITE_APP_POSTHOG_ENABLED=false
VITE_APP_POSTHOG_API_KEY=key
POSTHOG_API_KEY=key

VITE_APP_PRODUCTION=false
PRODUCTION=false

VITE_APP_STANDALONE=true
STANDALONE=true

REDISCLOUD_HOST=localhost
REDIS_URL=redis://localhost:6379

# OpenMeter endpoint location
OPENMETER_ENABLED=false
OPENMETER_TOKEN=token
OPENMETER_ENDPOINT=localhost:8888
OPENMETER_SOURCE=magick-local

UNSTRUCTURED_ENDPOINT=https://api.unstructured.io/general/v0/general
# UNSTRUCTURED_KEY=


NEXT_AWS_ACCESS_KEY=key
NEXT_AWS_SECRET_KEY=secret
NEXT_AWS_REGION=region
NEXT_AWS_BUCKET_NAME=magick
NEXT_AWS_BUCKET_ENDPOINT=http://localhost:9090
NEXT_PUBLIC_BUCKET_PREFIX=http://localhost:9090/magick

PORTAL_AGENT_KEY=key
APT_ROOT_URL=http://0.0.0.0

PLUGINS_DIRECTORY=./plugins
# TODO: Setup this with magickml email?
VERTEXAI_PROJECT=project
VERTEXAI_LOCATION=location

CREDENTIALS_ENCRYPTION_KEY=key
CREDENTIALS_ALGORITHM=aes-256-ctr

VITE_APP_FRIGADE_KEY=api_public_dYjL8ANoRnPaVLCUSecsttxFvNw12ToMFTFJIumS7Gi55QoqvQ6Zg3uFLTX7FO9n
NEXT_PUBLIC_FRIGADE_KEY=api_public_dYjL8ANoRnPaVLCUSecsttxFvNw12ToMFTFJIumS7Gi55QoqvQ6Zg3uFLTX7FO9n
APP_URL = http://localhost:4000
NEXT_PUBLIC_APP_URL=http://localhost:4000
NEXTAUTH_URL=http://localhost:4000

AGENT_API_KEY=apiKey
API_ACCESS_KEY=apiKey

IDE_SERVER_URL=http://localhost:3030

SERVICE_NAME=portal-local
LITELLM_LOCAL_MODEL_COST_MAP=True
include "application.conf"

db.default.driver=org.postgresql.Driver  
   
######### production                                                                         
db.default.url="postgres://bhmgbggkihqvkg:TXt7KkRvWahDt8Zwd-Fbq50IJB@ec2-54-243-49-204.compute-1.amazonaws.com:5432/dfne8jb100hr6a"
 
 #####staging
#db.default.url="postgres://xsxvfndzsqqkex:QWf82yI8uWzgHgvFj3DjTQ7hk3@ec2-54-197-238-242.compute-1.amazonaws.com:5432/dd07ddn2tr7uik"

#For Production
#consumer_key=fZXaMRKzszsXXTFmVqpw
#consumer_secret=nyDIYF2bTGICLpvxDYi3ggRENhK6fr7YhO14aLeaQ
#access_token="489503333-UD2BfhBrzCZiflVZjVfiGV13C61Sw8mOpJmxrvgq"
#access_token_secret="06BcUAf40UfoTWPa4WcIowtRQZSNjL8S2njK4ZSc4tQ"

# For Staging
consumer_key=XllwEaP8U1n05wP4cG9TA
consumer_secret=sPMkL7xJMCKrlHcawkhJKYAHSegOvTdVT6BXAF6aJmM
access_token="1906989254-b5KDbZmJWkkgNuSVNUjDrbRm7jbJTNGXmfr7506"
access_token_secret="0jlvzrkc9O1o0MVXb19WefzCzWSy7KAsFv8LQS9iE2Vfh"

#Google Keys
google_api_key=958859318162.apps.googleusercontent.com
google_api_secret=3sh9SjTOzAIonGoR70cRFKMb

#Emailer Settings
 smtp.host=smtp.gmail.com
 smtp.port=25
 smtp.tls=true
 smtp.ssl=true
 smtp.user="my_email_id"
 smtp.password="my_password"

 #AWS_CREDENTIALS
aws_bucket_name="my_bucket_name"
aws_access_key="my_access_key"
aws_secret_key="my_secret_key"# Application
PORT=8080
APP_HOST='127.0.0.1'

# Environment
NODE_ENV=

# Log
LOGGING_DIR=logs
LOGGING_LEVEL=debug
LOGGING_MAX_FILES=5

# Database
DB_HOST=127.0.0.1
DB_USER=
DB_NAME=
DB_PASSWORD=

# Auth
SALT_ROUNDS=
ACCESS_TOKEN_SALT=
REFRESH_TOKEN_SALT=
ACCESS_TOKEN_DURATION=
REFRESH_TOKEN_DURATION=

# AWS
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
AWS_REGION=
S3_BUCKET_NAME=
S3_BUCKET_PATH=
EC2_PEM_PATH=
EC2_HOST=
COGNITO_USER_POOL_ID=
COGNITO_CLIENT_ID=

# 3rd-party-API
API_KEY=
# Notes for Internal Contributors

The notes here are primarily targeted at internal (CircleCI) contributors to the orb but could be of reference to fork owners who wish to run the tests with their own AWS account.

## Building

### Required Project Environment Variables

The following [project environment variables](https://circleci.com/docs/2.0/env-vars/#setting-an-environment-variable-in-a-project) must be set for the project on CircleCI via the project settings page, before the project can be built successfully.

| Variable                       | Description                           |
| -------------------------------| --------------------------------------|
| `AWS_ACCESS_KEY_ID`            | Picked up by the AWS CLI              |
| `AWS_SECRET_ACCESS_KEY`        | Picked up by the AWS CLI              |
| `AWS_DEFAULT_REGION`           | Picked up by the AWS CLI. Set to `us-east-1`.              |
| `AWS_ACCOUNT_ID`               | AWS account id                        |
| `CIRCLECI_API_KEY`             | Used by the `queue` orb               |
| `AWS_RESOURCE_NAME_PREFIX_EC2` | Prefix used to name AWS resources for EC2 launch type integration tests. Set to `ecs-orb-ec2-1`.                                        |
| `AWS_RESOURCE_NAME_PREFIX_FARGATE` | Prefix used to name AWS resources for Fargate launch type integration tests. Set to `ecs-orb-fg-1`.                               |
| `AWS_RESOURCE_NAME_PREFIX_FARGATE_SPOT` | Prefix used to name AWS resources for Fargate Spot launch type integration tests. Set to `ecs-orb-fgs-1`.                               |
| `AWS_RESOURCE_NAME_PREFIX_CODEDEPLOY_FARGATE` | Prefix used to name AWS resources for Fargate launch type integration tests that use CodeDeploy. Set to `ecs-orb-cdfg-1`. |
| `SKIP_TEST_ENV_CREATION`       | Whether to skip test env setup        |
| `SKIP_TEST_ENV_TEARDOWN`       | Whether to skip test env teardown     |

## Tear down infra on CircleCI
If during development of this orb you execute a pipeline on CircleCI that fails to finish, you may need to manually tear down the infrastructure before being able to properly test again in a future pipeline.

This can be done most easily by SSHing into any of the `set-up-test-env` jobs and running the included teardown script.

1. SSH into `set-up-test-env` on CircleCI
2. `cd project/tests`
3. `./teardown.sh`

## Setting up / tearing down test infra locally

You can also set up the same test infrastructure set up by the build pipeline,
by running terraform locally.

This is also useful when you need to tear down infra manually after the pipeline failed. (For the ec2 and fargate tests, you can also tear down the infrastructure by going to the CloudFormation page on the AWS console and deleting the stacks prefixed with `ecs-orb`)

Set up AWS credentials and `AWS_DEFAULT_REGION`:

```
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...
AWS_ACCOUNT_ID=...
AWS_DEFAULT_REGION=us-east-1
```

Then set `AWS_RESOURCE_PREFIX` to the correct value:

*For EC2 tests*

```
AWS_RESOURCE_PREFIX=ecs-orb-ec2-1
cd tests/terraform_setup/ec2
```

*For Fargate tests*

```
AWS_RESOURCE_PREFIX=ecs-orb-fg-1
cd tests/terraform_setup/fargate
```

*For CodeDeploy tests*

```
AWS_RESOURCE_PREFIX=ecs-orb-cdfg-1
cd tests/terraform_setup/fargate_codedeploy
```

### Infra setup/teardown steps (for all)

```
terraform apply \
    -var "aws_access_key=${AWS_ACCESS_KEY_ID}" \
    -var "aws_secret_key=${AWS_SECRET_ACCESS_KEY}" \
    -var "aws_region=${AWS_DEFAULT_REGION}" \
    -var "aws_account_id=${AWS_ACCOUNT_ID}" \
    -var "aws_resource_prefix=${AWS_RESOURCE_PREFIX}"
```

```
terraform destroy \
    -var "aws_access_key=${AWS_ACCESS_KEY_ID}" \
    -var "aws_secret_key=${AWS_SECRET_ACCESS_KEY}" \
    -var "aws_region=${AWS_DEFAULT_REGION}" \
    -var "aws_account_id=${AWS_ACCOUNT_ID}" \
    -var "aws_resource_prefix=${AWS_RESOURCE_PREFIX}"
```

### Required Context and Context Environment Variables

The `orb-publishing` context is referenced in the build. In particular, the following [context environment variables](https://circleci.com/docs/2.0/env-vars/#setting-an-environment-variable-in-a-context) must be set in the `orb-publishing` context, before the project can be built successfully.

| Variable                       | Description                      |
| -------------------------------| ---------------------------------|
| `CIRCLE_TOKEN`                 | CircleCI API token used to publish the orb  |locals {
  single        = var.type == "single" ? true : false
  cluster       = var.type == "cluster" ? true : false
  autoscaling   = var.type == "autoscaling" ? true : false

  ssh_key_name    = var.ssh_key_create ? aws_key_pair.red5pro_ssh_key[0].key_name : data.aws_key_pair.ssh_key_pair[0].key_name
  ssh_private_key = var.ssh_key_create ? tls_private_key.red5pro_ssh_key[0].private_key_pem : file(var.ssh_private_key_path)
  ssh_private_key_path = var.ssh_key_create ? local_file.red5pro_ssh_key_pem[0].filename : var.ssh_private_key_path
  
  vpc_id     = var.vpc_create ? aws_vpc.red5pro_vpc[0].id : var.vpc_id_existing
  vpc_name   = var.vpc_create ? aws_vpc.red5pro_vpc[0].tags.Name : data.aws_vpc.selected[0].tags.Name
  subnet_ids = var.vpc_create ? tolist(aws_subnet.red5pro_subnets[*].id) : data.aws_subnets.all[0].ids

  mysql_rds_create   = local.autoscaling ? true : local.cluster && var.mysql_rds_create ? true : false
  mysql_host         = local.autoscaling ? aws_db_instance.red5pro_mysql[0].address : var.mysql_rds_create ? aws_db_instance.red5pro_mysql[0].address : "localhost"
  mysql_local_enable = local.autoscaling ? false : var.mysql_rds_create ? false : true

  elastic_ip = local.autoscaling ? null : var.elastic_ip_create ? aws_eip.elastic_ip[0].public_ip : data.aws_eip.elastic_ip[0].public_ip
  stream_manager_ip = local.autoscaling ? aws_lb.red5pro_sm_lb[0].dns_name : local.elastic_ip
}


################################################################################
# Elastic IP
################################################################################

resource "aws_eip" "elastic_ip" {
  count = local.autoscaling ? 0 : var.elastic_ip_create ? 1 : 0
}

data "aws_eip" "elastic_ip" {
  count     = local.autoscaling ? 0 : var.elastic_ip_create ? 0 : 1
  public_ip = var.elastic_ip_existing
  
}

resource "aws_eip_association" "elastic_ip_association" {
  count         = local.single || local.cluster ? 1 : 0
  instance_id   = local.single ? aws_instance.red5pro_single[0].id : aws_instance.red5pro_sm[0].id
  allocation_id = var.elastic_ip_create ? aws_eip.elastic_ip[0].id : data.aws_eip.elastic_ip[0].id
}


################################################################################
# SSH_KEY
################################################################################

# SSH key pair generation
resource "tls_private_key" "red5pro_ssh_key" {
  count     = var.ssh_key_create ? 1 : 0
  algorithm = "RSA"
  rsa_bits  = 2048
}

# Import SSH key pair to AWS
resource "aws_key_pair" "red5pro_ssh_key" {
  count      = var.ssh_key_create ? 1 : 0
  key_name   = var.ssh_key_name
  public_key = tls_private_key.red5pro_ssh_key[0].public_key_openssh
}

# Save SSH key pair files to local folder
resource "local_file" "red5pro_ssh_key_pem" {
  count           = var.ssh_key_create ? 1 : 0
  filename        = "./${var.ssh_key_name}.pem"
  content         = tls_private_key.red5pro_ssh_key[0].private_key_pem
  file_permission = "0400"
}
resource "local_file" "red5pro_ssh_key_pub" {
  count    = var.ssh_key_create ? 1 : 0
  filename = "./${var.ssh_key_name}.pub"
  content  = tls_private_key.red5pro_ssh_key[0].public_key_openssh
}

# Check current SSH key pair on the AWS
data "aws_key_pair" "ssh_key_pair" {
  count    = var.ssh_key_create ? 0 : 1
  key_name = var.ssh_key_name
}


################################################################################
# VPC - Check existing
################################################################################

data "aws_vpc" "selected" {
  count = var.vpc_create ? 0 : 1
  id    = var.vpc_id_existing
  lifecycle {
    postcondition {
      #enable_dns_support   = true
      #enable_dns_hostnames = true
      condition     = self.enable_dns_support == true && self.enable_dns_hostnames == true
      error_message = "ERROR! AWS VPC: ${var.vpc_id_existing} DNS resolution and DNS hostnames need to be enabled. Please check/fix it using AWS console."
    }
  }
}

data "aws_subnets" "all" {
  count = var.vpc_create ? 0 : 1
  filter {
    name   = "vpc-id"
    values = [var.vpc_id_existing]
  }
  lifecycle {
    postcondition {
      condition     = length(self.ids) >= 2
      error_message = "ERROR! AWS VPC: ${var.vpc_id_existing} doesn't have enough subnets. Minimum 2. Please try to use different VPC or create it by terraform."
    }
  }
}

data "aws_subnet" "all_subnets" {
  for_each = var.vpc_create ? toset([]) : toset(data.aws_subnets.all[0].ids)
  id       = each.value
  lifecycle {
    postcondition {
      condition     = self.map_public_ip_on_launch == true
      error_message = "ERROR! Subnet ${self.id} configured without assigning Public IP on launch instances. Please check/fix it using AWS console."
    }
  }
}

################################################################################
# VPC - Create new (VPC + Internet geteway + Subnets + Route table)
################################################################################

data "aws_availability_zones" "available" {
  count = var.vpc_create ? 1 : 0
  state = "available"

  filter {
    name   = "zone-type"
    values = ["availability-zone"]
  }
  
  lifecycle {
    postcondition {
      condition     = length(self.names) >= 2
      error_message = "ERROR! AWS availability zones less than 2. Please try to use different region."
    }
  }
}

resource "aws_vpc" "red5pro_vpc" {
  count                = var.vpc_create ? 1 : 0
  cidr_block           = var.vpc_cidr_block
  instance_tenancy     = "default"
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = merge({ "Name" = "${var.name}-vpc" }, var.tags, )
}

resource "aws_internet_gateway" "red5pro_igw" {
  count  = var.vpc_create ? 1 : 0
  vpc_id = aws_vpc.red5pro_vpc[0].id

  tags = merge({ "Name" = "${var.name}-igw" }, var.tags, )
}

resource "aws_subnet" "red5pro_subnets" {
  count                   = var.vpc_create ? length(data.aws_availability_zones.available[0].names) : 0
  vpc_id                  = aws_vpc.red5pro_vpc[0].id
  cidr_block              = element(var.vpc_public_subnets, count.index)
  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available[0].names[count.index]

  tags = merge({ "Name" = "${var.name}-subnet-${count.index}" }, var.tags, )
}

resource "aws_route" "red5pro_route" {
  count                  = var.vpc_create ? 1 : 0
  route_table_id         = aws_vpc.red5pro_vpc[0].main_route_table_id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.red5pro_igw[0].id
  depends_on             = [aws_internet_gateway.red5pro_igw[0]]
}

resource "aws_route_table_association" "red5pro_subnets_association" {
  count          = var.vpc_create ? length(aws_subnet.red5pro_subnets) : 0
  subnet_id      = aws_subnet.red5pro_subnets[count.index].id
  route_table_id = aws_vpc.red5pro_vpc[0].main_route_table_id
}

################################################################################
# Security Groups - Create new (MySQL + StreamManager + Nodes)
################################################################################

# Security group for Red5Pro Stream Manager (AWS VPC)
resource "aws_security_group" "red5pro_sm_sg" {
  count      = local.cluster || local.autoscaling ? 1 : 0
  name        = "${var.name}-sm-sg"
  description = "Allow inbound/outbound traffic for Stream Manager"
  vpc_id      = local.vpc_id

  dynamic "ingress" {
    for_each = var.security_group_stream_manager_ingress
    content {
      from_port        = lookup(ingress.value, "from_port", 0)
      to_port          = lookup(ingress.value, "to_port", 0)
      protocol         = lookup(ingress.value, "protocol", "tcp")
      cidr_blocks      = [lookup(ingress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(ingress.value, "ipv6_cidr_block", "::/0")]
    }
  }
  dynamic "egress" {
    for_each = var.security_group_stream_manager_egress
    content {
      from_port        = lookup(egress.value, "from_port", 0)
      to_port          = lookup(egress.value, "to_port", 0)
      protocol         = lookup(egress.value, "protocol", "-1")
      cidr_blocks      = [lookup(egress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(egress.value, "ipv6_cidr_block", "::/0")]
    }
  }
  tags = merge({ "Name" = "${var.name}-sm-sg" }, var.tags, )
}

# Security group for Red5Pro Nodes (AWS VPC)
resource "aws_security_group" "red5pro_node_sg" {
  count      = local.cluster || local.autoscaling ? 1 : 0
  name        = "${var.name}-node-sg"
  description = "Allow inbound/outbound traffic for Nodes"
  vpc_id      = local.vpc_id

  dynamic "ingress" {
    for_each = var.security_group_node_ingress
    content {
      from_port        = lookup(ingress.value, "from_port", 0)
      to_port          = lookup(ingress.value, "to_port", 0)
      protocol         = lookup(ingress.value, "protocol", "tcp")
      cidr_blocks      = [lookup(ingress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(ingress.value, "ipv6_cidr_block", "::/0")]
    }
  }
  dynamic "egress" {
    for_each = var.security_group_node_egress
    content {
      from_port        = lookup(egress.value, "from_port", 0)
      to_port          = lookup(egress.value, "to_port", 0)
      protocol         = lookup(egress.value, "protocol", "-1")
      cidr_blocks      = [lookup(egress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(egress.value, "ipv6_cidr_block", "::/0")]
    }
  }
  tags = merge({ "Name" = "${var.name}-node-sg" }, var.tags, )
}

# Security group for MySQL database (AWS RDS)
resource "aws_security_group" "red5pro_mysql_sg" {
  count      = local.mysql_rds_create ? 1 : 0
  name        = "${var.name}-mysql-sg"
  description = "Allow inbound/outbound traffic for MySQL"
  vpc_id      = local.vpc_id

  ingress {
    description      = "Access to MySQL from Stream Manager security group"
    from_port        = 3306
    to_port          = 3306
    protocol         = "tcp"
    security_groups  = [aws_security_group.red5pro_sm_sg[0].id, aws_security_group.red5pro_images_sg[0].id]
    }
  egress {
    description      = "Access from MySQL to 0.0.0.0/0"
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = merge({ "Name" = "${var.name}-mysql-sg" }, var.tags, )
}

# Security group for StreamManager and Node images (AWS RDS)
resource "aws_security_group" "red5pro_images_sg" {
  count      = local.cluster || local.autoscaling ? 1 : 0
  name        = "${var.name}-images-sg"
  description = "Allow inbound/outbound traffic for SM and Node images"
  vpc_id      = local.vpc_id

  ingress {
    description      = "Access to SSH from 0.0.0.0/0"
    from_port        = 22
    to_port          = 22
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
  }
  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = merge({ "Name" = "${var.name}-images-sg" }, var.tags, )
}

# Security group for Single Red5Pro server (AWS EC2)
resource "aws_security_group" "red5pro_single_sg" {
  count       = local.single && var.security_group_create ? 1 : 0
  name        = "${var.name}-single-sg"
  description = "Allow inbound/outbound traffic for Single Red5Pro server"
  vpc_id      = local.vpc_id

  dynamic "ingress" {
    for_each = var.security_group_single_ingress
    content {
      from_port        = lookup(ingress.value, "from_port", 0)
      to_port          = lookup(ingress.value, "to_port", 0)
      protocol         = lookup(ingress.value, "protocol", "tcp")
      cidr_blocks      = [lookup(ingress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(ingress.value, "ipv6_cidr_block", "::/0")]
    }
  }
  dynamic "egress" {
    for_each = var.security_group_single_egress
    content {
      from_port        = lookup(egress.value, "from_port", 0)
      to_port          = lookup(egress.value, "to_port", 0)
      protocol         = lookup(egress.value, "protocol", "-1")
      cidr_blocks      = [lookup(egress.value, "cidr_block", "0.0.0.0/0")]
      ipv6_cidr_blocks = [lookup(egress.value, "ipv6_cidr_block", "::/0")]
    }
  }

  tags = merge({ "Name" = "${var.name}-single-sg" }, var.tags, )
}

################################################################################
# MySQL - Create new (AWS RDS)
################################################################################

# MySQL DataBase subnet group (AWS RDS)
resource "aws_db_subnet_group" "red5pro_mysql_subnet_group" {
  count      = local.mysql_rds_create ? 1 : 0
  name       = "${var.name}-mysql-subnet-group"
  subnet_ids = local.subnet_ids
  tags       = merge({ "Name" = "${var.name}-mysql-subnet-group" }, var.tags, )
}

# MySQL DataBase parameter group (AWS RDS)
resource "aws_db_parameter_group" "red5pro_mysql_pg" {
  count  = local.mysql_rds_create ? 1 : 0
  name   = "${var.name}-mysql-pg"
  family = "mysql8.0"

  parameter {
    name  = "max_connections"
    value = "100000"
  }
  tags = merge({ "Name" = "${var.name}-mysql-pg" }, var.tags, )
}

# MySQL DataBase (AWS RDS)
resource "aws_db_instance" "red5pro_mysql" {
  count                  = local.mysql_rds_create ? 1 : 0
  identifier             = "${var.name}-mysql"
  allocated_storage      = 10
  engine                 = "mysql"
  engine_version         = "8.0"
  instance_class         = var.mysql_rds_instance_type
  username               = var.mysql_user_name
  password               = var.mysql_password
  port                   = var.mysql_port
  parameter_group_name   = aws_db_parameter_group.red5pro_mysql_pg[0].name
  skip_final_snapshot    = true
  db_subnet_group_name   = aws_db_subnet_group.red5pro_mysql_subnet_group[0].name
  vpc_security_group_ids = [aws_security_group.red5pro_mysql_sg[0].id]

  tags = merge({ "Name" = "${var.name}-mysql" }, var.tags, )
}

################################################################################
# Stream manager - (AWS EC2 instance)
################################################################################

# Get information about AWS image ID with Ubuntu
data "aws_ami" "latest_ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical
  filter {
    name   = "name"
    values = [lookup(var.ubuntu_version_aws_image, var.ubuntu_version, "what?")]
  }
  filter {
    name   = "virtualization-type"
    values = ["hvm"]
  }
}

# Stream Manager instance 
resource "aws_instance" "red5pro_sm" {
  count                  = local.cluster || local.autoscaling ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.stream_manager_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [local.cluster ? aws_security_group.red5pro_sm_sg[0].id : aws_security_group.red5pro_images_sg[0].id]

  root_block_device {
    volume_size = var.stream_manager_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_aws_cloud_controller
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_aws_cloud_controller)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export SM_API_KEY='${var.stream_manager_api_key}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_CLUSTER_KEY='${var.red5pro_cluster_key}'",
      "export NODE_PREFIX_NAME='${var.name}-node'",
      "export DB_LOCAL_ENABLE='${local.mysql_local_enable}'",
      "export DB_HOST='${local.mysql_host}'",
      "export DB_PORT='${var.mysql_port}'",
      "export DB_USER='${var.mysql_user_name}'",
      "export DB_PASSWORD='${var.mysql_password}'",
      "export AWS_DEFAULT_ZONE='${var.aws_region}'",
      "export AWS_ACCESS_KEY='${var.aws_access_key}'",
      "export AWS_SECRET_KEY='${var.aws_secret_key}'",
      "export AWS_SSH_KEY_NAME='${local.ssh_key_name}'",
      "export AWS_SECURITY_GROUP_NAME='${aws_security_group.red5pro_node_sg[0].name}'",
      "export AWS_VPC_NAME='${local.vpc_name}'",
      "export SSL_ENABLE='${var.https_letsencrypt_enable}'",
      "export SSL_DOMAIN='${var.https_letsencrypt_certificate_domain_name}'",
      "export SSL_MAIL='${var.https_letsencrypt_certificate_email}'",
      "export SSL_PASSWORD='${var.https_letsencrypt_certificate_password}'",
      "export COTURN_ENABLE='${var.stream_manager_coturn_enable}'",
      "export COTURN_ADDRESS='${var.stream_manager_coturn_address}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_mysql_local.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_stream_manager.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_coturn.sh",
      #"sudo rm -R /home/ubuntu/red5pro-installer",
      "sudo systemctl daemon-reload && sudo systemctl start red5pro",
      "nohup sudo -E /home/ubuntu/red5pro-installer/r5p_ssl_check_install.sh >> /home/ubuntu/red5pro-installer/r5p_ssl_check_install.log &",
      "sleep 2"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-stream-manager", }, var.tags, )
}


################################################################################
# Stream manager autoscaling - (AWS EC2)
################################################################################

# AWS Stream Manager autoscaling - Create image from StreamManager instance (AWS EC2 AMI)
resource "aws_ami_from_instance" "red5pro_sm_image" {
  count              = local.autoscaling ? 1 : 0
  name               = "${var.name}-stream-manager-image-${formatdate("DDMMMYY-hhmm", timestamp())}"
  source_instance_id = aws_instance.red5pro_sm[0].id
  depends_on         = [aws_instance.red5pro_sm[0]]
  lifecycle {
    ignore_changes = [name, tags]
  }

  tags = merge({ "Name" = "${var.name}-stream-manager-image-${formatdate("DDMMMYY-hhmm", timestamp())}" }, var.tags, )
}

# AWS Stream Manager autoscaling - Launch template
resource "aws_launch_template" "red5pro_sm_lt" {
  count                  = local.autoscaling ? 1 : 0
  name                   = "${var.name}-stream-manager-lt"
  image_id               = aws_ami_from_instance.red5pro_sm_image[0].id
  instance_type          = var.stream_manager_instance_type
  key_name               = local.ssh_key_name
  update_default_version = true

  network_interfaces {
    associate_public_ip_address = true
    security_groups             = [aws_security_group.red5pro_sm_sg[0].id]
  }

  block_device_mappings {
    device_name = "/dev/sda1"

    ebs {
      volume_size = var.stream_manager_volume_size
    }
  }
  tag_specifications {
    resource_type = "instance"

    tags = merge({ "Name" = "${var.name}-stream-manager" }, var.tags, )
  }
}

# AWS Stream Manager autoscaling - Placement group
resource "aws_placement_group" "red5pro_sm_pg" {
  count    = local.autoscaling ? 1 : 0
  name     = "${var.name}-stream-manager-pg"
  strategy = "partition" # cluster
}

# AWS Stream Manager autoscaling - Autoscaling group
resource "aws_autoscaling_group" "red5pro_sm_ag" {
  count               = local.autoscaling ? 1 : 0
  name                = "${var.name}-stream-manager-ag"
  desired_capacity    = var.stream_manager_autoscaling_desired_capacity
  max_size            = var.stream_manager_autoscaling_maximum_capacity
  min_size            = var.stream_manager_autoscaling_minimum_capacity
  placement_group     = aws_placement_group.red5pro_sm_pg[0].id
  vpc_zone_identifier = local.subnet_ids

  launch_template {
    id      = aws_launch_template.red5pro_sm_lt[0].id
    version = "$Latest"
  }

  lifecycle {
    ignore_changes = [
      target_group_arns,
    ]
  }
}

# AWS Stream Manager autoscaling - Target group
resource "aws_lb_target_group" "red5pro_sm_tg" {
  count       = local.autoscaling ? 1 : 0
  name        = "${var.name}-stream-manager-tg"
  target_type = "instance"
  port        = 5080
  protocol    = "HTTP"
  vpc_id      = local.vpc_id

  health_check {
    path = "/"
    port = 5080
  }
}

# AWS Stream Manager autoscaling - SSL certificate
data "aws_acm_certificate" "red5pro_sm_cert" {
  count    = local.autoscaling && var.https_certificate_manager_use_existing ? 1 : 0
  domain   = var.https_certificate_manager_certificate_name
  statuses = ["ISSUED"]
}

# AWS Stream Manager autoscaling - Aplication Load Balancer
resource "aws_lb" "red5pro_sm_lb" {
  count              = local.autoscaling ? 1 : 0
  name               = "${var.name}-stream-manager-lb"
  internal           = false
  load_balancer_type = "application"
  security_groups    = [aws_security_group.red5pro_sm_sg[0].id]
  subnets            = local.subnet_ids

  enable_deletion_protection = false

  tags = merge({ "Name" = "${var.name}-stream-manager-lb" }, var.tags, )
}

# AWS Stream Manager autoscaling - LB HTTP listener
resource "aws_lb_listener" "red5pro_sm_http" {
  count             = local.autoscaling ? 1 : 0
  load_balancer_arn = aws_lb.red5pro_sm_lb[0].arn
  port              = "5080"
  protocol          = "HTTP"

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.red5pro_sm_tg[0].arn
  }
}

# AWS Stream Manager autoscaling - LB HTTPS listener
resource "aws_lb_listener" "red5pro_sm_https" {
  count             = local.autoscaling && var.https_certificate_manager_use_existing ? 1 : 0
  load_balancer_arn = aws_lb.red5pro_sm_lb[0].arn
  port              = "443"
  protocol          = "HTTPS"
  ssl_policy        = "ELBSecurityPolicy-2016-08"
  certificate_arn   = data.aws_acm_certificate.red5pro_sm_cert[0].arn

  default_action {
    type             = "forward"
    target_group_arn = aws_lb_target_group.red5pro_sm_tg[0].arn
  }
}

# AWS Stream Manager autoscaling - Autoscaling attachment
resource "aws_autoscaling_attachment" "red5pro_sm_aa" {
  count             = local.autoscaling ? 1 : 0
  autoscaling_group_name = aws_autoscaling_group.red5pro_sm_ag[0].id
  lb_target_group_arn    = aws_lb_target_group.red5pro_sm_tg[0].arn
}


################################################################################
# Red5 Pro autoscaling nodes (AWS EC2 AMI)
################################################################################

# ORIGIN Node instance for AMI (AWS EC2)
resource "aws_instance" "red5pro_node_origin" {
  count                  = var.origin_image_create ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.origin_image_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [aws_security_group.red5pro_images_sg[0].id]

  root_block_device {
    volume_size = var.origin_image_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export SM_IP='${local.stream_manager_ip}'",
      "export NODE_CLUSTER_KEY='${var.red5pro_cluster_key}'",
      "export NODE_API_ENABLE='${var.red5pro_api_enable}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_INSPECTOR_ENABLE='${var.origin_image_red5pro_inspector_enable}'",
      "export NODE_RESTREAMER_ENABLE='${var.origin_image_red5pro_restreamer_enable}'",
      "export NODE_SOCIALPUSHER_ENABLE='${var.origin_image_red5pro_socialpusher_enable}'",
      "export NODE_SUPPRESSOR_ENABLE='${var.origin_image_red5pro_suppressor_enable}'",
      "export NODE_HLS_ENABLE='${var.origin_image_red5pro_hls_enable}'",
      "export NODE_HLS_OUTPUT_FORMAT='${var.origin_image_red5pro_hls_output_format}'",
      "export NODE_HLS_DVR_PLAYLIST='${var.origin_image_red5pro_hls_dvr_playlist}'",
      "export NODE_WEBHOOKS_ENABLE='${var.origin_image_red5pro_webhooks_enable}'",
      "export NODE_WEBHOOKS_ENDPOINT='${var.origin_image_red5pro_webhooks_endpoint}'",
      "export NODE_ROUND_TRIP_AUTH_ENABLE='${var.origin_image_red5pro_round_trip_auth_enable}'",
      "export NODE_ROUND_TRIP_AUTH_HOST='${var.origin_image_red5pro_round_trip_auth_host}'",
      "export NODE_ROUND_TRIP_AUTH_PORT='${var.origin_image_red5pro_round_trip_auth_port}'",
      "export NODE_ROUND_TRIP_AUTH_PROTOCOL='${var.origin_image_red5pro_round_trip_auth_protocol}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_VALIDATE='${var.origin_image_red5pro_round_trip_auth_endpoint_validate}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_INVALIDATE='${var.origin_image_red5pro_round_trip_auth_endpoint_invalidate}'",
      "export NODE_CLOUDSTORAGE_ENABLE='${var.origin_image_red5pro_cloudstorage_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_ACCESS_KEY='${var.origin_image_red5pro_cloudstorage_aws_access_key}'",
      "export NODE_CLOUDSTORAGE_AWS_SECRET_KEY='${var.origin_image_red5pro_cloudstorage_aws_secret_key}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_NAME='${var.origin_image_red5pro_cloudstorage_aws_bucket_name}'",
      "export NODE_CLOUDSTORAGE_AWS_REGION='${var.origin_image_red5pro_cloudstorage_aws_region}'",
      "export NODE_CLOUDSTORAGE_POSTPROCESSOR_ENABLE='${var.origin_image_red5pro_cloudstorage_postprocessor_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_ACL_POLICY='${var.origin_image_red5pro_cloudstorage_aws_bucket_acl_policy}'",
      "export NODE_EFS_ENABLE='${var.origin_image_red5pro_efs_enable}'",
      "export NODE_EFS_DNS_NAME='${var.origin_image_red5pro_efs_dns_name}'",
      "export NODE_EFS_MOUNT_POINT='${var.origin_image_red5pro_efs_mount_point}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node_apps_plugins.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_efs.sh",
      "sudo rm -R /home/ubuntu/red5pro-installer"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-node-origin-image" }, var.tags, )
}

# EDGE Node instance for AMI (AWS EC2)
resource "aws_instance" "red5pro_node_edge" {
  count                  = var.edge_image_create ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.edge_image_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [aws_security_group.red5pro_images_sg[0].id]

  root_block_device {
    volume_size = var.edge_image_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export SM_IP='${local.stream_manager_ip}'",
      "export NODE_CLUSTER_KEY='${var.red5pro_cluster_key}'",
      "export NODE_API_ENABLE='${var.red5pro_api_enable}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_INSPECTOR_ENABLE='${var.edge_image_red5pro_inspector_enable}'",
      "export NODE_RESTREAMER_ENABLE='${var.edge_image_red5pro_restreamer_enable}'",
      "export NODE_SOCIALPUSHER_ENABLE='${var.edge_image_red5pro_socialpusher_enable}'",
      "export NODE_SUPPRESSOR_ENABLE='${var.edge_image_red5pro_suppressor_enable}'",
      "export NODE_HLS_ENABLE='${var.edge_image_red5pro_hls_enable}'",
      "export NODE_WEBHOOKS_ENABLE='${var.edge_image_red5pro_webhooks_enable}'",
      "export NODE_WEBHOOKS_ENDPOINT='${var.edge_image_red5pro_webhooks_endpoint}'",
      "export NODE_ROUND_TRIP_AUTH_ENABLE='${var.edge_image_red5pro_round_trip_auth_enable}'",
      "export NODE_ROUND_TRIP_AUTH_HOST='${var.edge_image_red5pro_round_trip_auth_host}'",
      "export NODE_ROUND_TRIP_AUTH_PORT='${var.edge_image_red5pro_round_trip_auth_port}'",
      "export NODE_ROUND_TRIP_AUTH_PROTOCOL='${var.edge_image_red5pro_round_trip_auth_protocol}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_VALIDATE='${var.edge_image_red5pro_round_trip_auth_endpoint_validate}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_INVALIDATE='${var.edge_image_red5pro_round_trip_auth_endpoint_invalidate}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node_apps_plugins.sh",
      "sudo rm -R /home/ubuntu/red5pro-installer"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-node-edge-image" }, var.tags, )
}

# TRANSCODER Node instance for AMI (AWS EC2)
resource "aws_instance" "red5pro_node_transcoder" {
  count                  = var.transcoder_image_create ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.transcoder_image_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [aws_security_group.red5pro_images_sg[0].id]

  root_block_device {
    volume_size = var.transcoder_image_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export SM_IP='${local.stream_manager_ip}'",
      "export NODE_CLUSTER_KEY='${var.red5pro_cluster_key}'",
      "export NODE_API_ENABLE='${var.red5pro_api_enable}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_INSPECTOR_ENABLE='${var.transcoder_image_red5pro_inspector_enable}'",
      "export NODE_RESTREAMER_ENABLE='${var.transcoder_image_red5pro_restreamer_enable}'",
      "export NODE_SOCIALPUSHER_ENABLE='${var.transcoder_image_red5pro_socialpusher_enable}'",
      "export NODE_SUPPRESSOR_ENABLE='${var.transcoder_image_red5pro_suppressor_enable}'",
      "export NODE_HLS_ENABLE='${var.transcoder_image_red5pro_hls_enable}'",
      "export NODE_HLS_OUTPUT_FORMAT='${var.transcoder_image_red5pro_hls_output_format}'",
      "export NODE_HLS_DVR_PLAYLIST='${var.transcoder_image_red5pro_hls_dvr_playlist}'",
      "export NODE_WEBHOOKS_ENABLE='${var.transcoder_image_red5pro_webhooks_enable}'",
      "export NODE_WEBHOOKS_ENDPOINT='${var.transcoder_image_red5pro_webhooks_endpoint}'",
      "export NODE_ROUND_TRIP_AUTH_ENABLE='${var.transcoder_image_red5pro_round_trip_auth_enable}'",
      "export NODE_ROUND_TRIP_AUTH_HOST='${var.transcoder_image_red5pro_round_trip_auth_host}'",
      "export NODE_ROUND_TRIP_AUTH_PORT='${var.transcoder_image_red5pro_round_trip_auth_port}'",
      "export NODE_ROUND_TRIP_AUTH_PROTOCOL='${var.transcoder_image_red5pro_round_trip_auth_protocol}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_VALIDATE='${var.transcoder_image_red5pro_round_trip_auth_endpoint_validate}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_INVALIDATE='${var.transcoder_image_red5pro_round_trip_auth_endpoint_invalidate}'",
      "export NODE_CLOUDSTORAGE_ENABLE='${var.transcoder_image_red5pro_cloudstorage_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_ACCESS_KEY='${var.transcoder_image_red5pro_cloudstorage_aws_access_key}'",
      "export NODE_CLOUDSTORAGE_AWS_SECRET_KEY='${var.transcoder_image_red5pro_cloudstorage_aws_secret_key}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_NAME='${var.transcoder_image_red5pro_cloudstorage_aws_bucket_name}'",
      "export NODE_CLOUDSTORAGE_AWS_REGION='${var.transcoder_image_red5pro_cloudstorage_aws_region}'",
      "export NODE_CLOUDSTORAGE_POSTPROCESSOR_ENABLE='${var.transcoder_image_red5pro_cloudstorage_postprocessor_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_ACL_POLICY='${var.transcoder_image_red5pro_cloudstorage_aws_bucket_acl_policy}'",
      "export NODE_EFS_ENABLE='${var.transcoder_image_red5pro_efs_enable}'",
      "export NODE_EFS_DNS_NAME='${var.transcoder_image_red5pro_efs_dns_name}'",
      "export NODE_EFS_MOUNT_POINT='${var.transcoder_image_red5pro_efs_mount_point}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node_apps_plugins.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_efs.sh",
      "sudo rm -R /home/ubuntu/red5pro-installer"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-node-transcoder-image" }, var.tags, )
}

# RELAY Node instance for AMI (AWS EC2)
resource "aws_instance" "red5pro_node_relay" {
  count                  = var.relay_image_create ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.relay_image_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [aws_security_group.red5pro_images_sg[0].id]

  root_block_device {
    volume_size = var.relay_image_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export SM_IP='${local.stream_manager_ip}'",
      "export NODE_CLUSTER_KEY='${var.red5pro_cluster_key}'",
      "export NODE_API_ENABLE='${var.red5pro_api_enable}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_INSPECTOR_ENABLE='${var.relay_image_red5pro_inspector_enable}'",
      "export NODE_RESTREAMER_ENABLE='${var.relay_image_red5pro_restreamer_enable}'",
      "export NODE_SOCIALPUSHER_ENABLE='${var.relay_image_red5pro_socialpusher_enable}'",
      "export NODE_SUPPRESSOR_ENABLE='${var.relay_image_red5pro_suppressor_enable}'",
      "export NODE_HLS_ENABLE='${var.relay_image_red5pro_hls_enable}'",
      "export NODE_ROUND_TRIP_AUTH_ENABLE='${var.relay_image_red5pro_round_trip_auth_enable}'",
      "export NODE_ROUND_TRIP_AUTH_HOST='${var.relay_image_red5pro_round_trip_auth_host}'",
      "export NODE_ROUND_TRIP_AUTH_PORT='${var.relay_image_red5pro_round_trip_auth_port}'",
      "export NODE_ROUND_TRIP_AUTH_PROTOCOL='${var.relay_image_red5pro_round_trip_auth_protocol}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_VALIDATE='${var.relay_image_red5pro_round_trip_auth_endpoint_validate}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_INVALIDATE='${var.relay_image_red5pro_round_trip_auth_endpoint_invalidate}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node_apps_plugins.sh",
      "sudo rm -R /home/ubuntu/red5pro-installer"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-node-relay-image" }, var.tags, )
}


################################################################################
# Red5 Pro Single server (AWS EC2)
################################################################################

# Red5 Pro Single server instance (AWS EC2)
resource "aws_instance" "red5pro_single" {
  count                  = local.single ? 1 : 0
  ami                    = data.aws_ami.latest_ubuntu.id
  instance_type          = var.single_instance_type
  key_name               = local.ssh_key_name
  subnet_id              = element(local.subnet_ids, 0)
  vpc_security_group_ids = [ var.security_group_create ? aws_security_group.red5pro_single_sg[0].id : var.security_group_id_existing ]

  root_block_device {
    volume_size = var.single_volume_size
  }

  provisioner "file" {
    source        = "${abspath(path.module)}/red5pro-installer"
    destination   = "/home/ubuntu"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "file" {
    source        = var.path_to_red5pro_build
    destination   = "/home/ubuntu/red5pro-installer/${basename(var.path_to_red5pro_build)}"

    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  provisioner "remote-exec" {
    inline = [
      "sudo cloud-init status --wait",
      "export LICENSE_KEY='${var.red5pro_license_key}'",
      "export NODE_API_ENABLE='${var.red5pro_api_enable}'",
      "export NODE_API_KEY='${var.red5pro_api_key}'",
      "export NODE_INSPECTOR_ENABLE='${var.red5pro_inspector_enable}'",
      "export NODE_RESTREAMER_ENABLE='${var.red5pro_restreamer_enable}'",
      "export NODE_SOCIALPUSHER_ENABLE='${var.red5pro_socialpusher_enable}'",
      "export NODE_SUPPRESSOR_ENABLE='${var.red5pro_suppressor_enable}'",
      "export NODE_HLS_ENABLE='${var.red5pro_hls_enable}'",
      "export NODE_HLS_OUTPUT_FORMAT='${var.red5pro_hls_output_format}'",
      "export NODE_HLS_DVR_PLAYLIST='${var.red5pro_hls_dvr_playlist}'",
      "export NODE_WEBHOOKS_ENABLE='${var.red5pro_webhooks_enable}'",
      "export NODE_WEBHOOKS_ENDPOINT='${var.red5pro_webhooks_endpoint}'",
      "export NODE_ROUND_TRIP_AUTH_ENABLE='${var.red5pro_round_trip_auth_enable}'",
      "export NODE_ROUND_TRIP_AUTH_HOST='${var.red5pro_round_trip_auth_host}'",
      "export NODE_ROUND_TRIP_AUTH_PORT='${var.red5pro_round_trip_auth_port}'",
      "export NODE_ROUND_TRIP_AUTH_PROTOCOL='${var.red5pro_round_trip_auth_protocol}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_VALIDATE='${var.red5pro_round_trip_auth_endpoint_validate}'",
      "export NODE_ROUND_TRIP_AUTH_ENDPOINT_INVALIDATE='${var.red5pro_round_trip_auth_endpoint_invalidate}'",
      "export NODE_CLOUDSTORAGE_ENABLE='${var.red5pro_cloudstorage_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_ACCESS_KEY='${var.red5pro_cloudstorage_aws_access_key}'",
      "export NODE_CLOUDSTORAGE_AWS_SECRET_KEY='${var.red5pro_cloudstorage_aws_secret_key}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_NAME='${var.red5pro_cloudstorage_aws_bucket_name}'",
      "export NODE_CLOUDSTORAGE_AWS_REGION='${var.red5pro_cloudstorage_aws_region}'",
      "export NODE_CLOUDSTORAGE_POSTPROCESSOR_ENABLE='${var.red5pro_cloudstorage_postprocessor_enable}'",
      "export NODE_CLOUDSTORAGE_AWS_BUCKET_ACL_POLICY='${var.red5pro_cloudstorage_aws_bucket_acl_policy}'",
      "export NODE_EFS_ENABLE='${var.red5pro_efs_enable}'",
      "export NODE_EFS_DNS_NAME='${var.red5pro_efs_dns_name}'",
      "export NODE_EFS_MOUNT_POINT='${var.red5pro_efs_mount_point}'",
      "export SSL_ENABLE='${var.https_letsencrypt_enable}'",
      "export SSL_DOMAIN='${var.https_letsencrypt_certificate_domain_name}'",
      "export SSL_MAIL='${var.https_letsencrypt_certificate_email}'",
      "export SSL_PASSWORD='${var.https_letsencrypt_certificate_password}'",
      "export COTURN_ENABLE='${var.red5pro_coturn_enable}'",
      "export COTURN_ADDRESS='${var.red5pro_coturn_address}'",
      "cd /home/ubuntu/red5pro-installer/",
      "sudo chmod +x /home/ubuntu/red5pro-installer/*.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_install_server_basic.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_node_apps_plugins.sh",
      "sudo -E /home/ubuntu/red5pro-installer/r5p_config_coturn.sh",
      #"sudo rm -R /home/ubuntu/red5pro-installer",
      "sudo systemctl daemon-reload && sudo systemctl start red5pro",
      "nohup sudo -E /home/ubuntu/red5pro-installer/r5p_ssl_check_install.sh >> /home/ubuntu/red5pro-installer/r5p_ssl_check_install.log &",
      "sleep 2"
    ]
    connection {
      host        = self.public_ip
      type        = "ssh"
      user        = "ubuntu"
      private_key = local.ssh_private_key
    }
  }

  tags = merge({ "Name" = "${var.name}-single-server" }, var.tags, )
}


################################################################################
# Red5 Pro autoscaling nodes create images (AWS EC2 AMI)
################################################################################

# Origin node - Create image (AWS EC2 AMI)
resource "aws_ami_from_instance" "red5pro_node_origin_image" {
  count              = var.origin_image_create ? 1 : 0
  name               = "${var.name}-node-origin-image-${formatdate("DDMMMYY-hhmm", timestamp())}"
  source_instance_id = aws_instance.red5pro_node_origin[0].id
  depends_on         = [aws_instance.red5pro_node_origin[0]]
  lifecycle {
    ignore_changes = [name, tags]
  }

  tags = merge({ "Name" = "${var.name}-node-origin-image-${formatdate("DDMMMYY-hhmm", timestamp())}" }, var.tags, )
}

# Edge node - Create image (AWS EC2 AMI)
resource "aws_ami_from_instance" "red5pro_node_edge_image" {
  count              = var.edge_image_create ? 1 : 0
  name               = "${var.name}-node-edge-image-${formatdate("DDMMMYY-hhmm", timestamp())}"
  source_instance_id = aws_instance.red5pro_node_edge[0].id
  depends_on         = [aws_instance.red5pro_node_edge[0]]
  lifecycle {
    ignore_changes = [name, tags]
  }

  tags = merge({ "Name" = "${var.name}-node-edge-image-${formatdate("DDMMMYY-hhmm", timestamp())}" }, var.tags, )
}
# Transcoder node - Create image (AWS EC2 AMI)
resource "aws_ami_from_instance" "red5pro_node_transcoder_image" {
  count              = var.transcoder_image_create ? 1 : 0
  name               = "${var.name}-node-transcoder-image-${formatdate("DDMMMYY-hhmm", timestamp())}"
  source_instance_id = aws_instance.red5pro_node_transcoder[0].id
  depends_on         = [aws_instance.red5pro_node_transcoder[0]]
  lifecycle {
    ignore_changes = [name, tags]
  }

  tags = merge({ "Name" = "${var.name}-node-transcoder-image-${formatdate("DDMMMYY-hhmm", timestamp())}" }, var.tags, )
}
# Relay node - Create image (AWS EC2 AMI)
resource "aws_ami_from_instance" "red5pro_node_relay_image" {
  count              = var.relay_image_create ? 1 : 0
  name               = "${var.name}-node-relay-image-${formatdate("DDMMMYY-hhmm", timestamp())}"
  source_instance_id = aws_instance.red5pro_node_relay[0].id
  depends_on         = [aws_instance.red5pro_node_relay[0]]
  lifecycle {
    ignore_changes = [name, tags]
  }

  tags = merge({ "Name" = "${var.name}-node-relay-image-${formatdate("DDMMMYY-hhmm", timestamp())}" }, var.tags, )
}

################################################################################
# Stop instances which used for create images (AWS CLI)
################################################################################

# AWS Stream Manager autoscaling - Stop Stream Manager instance using aws cli
resource "null_resource" "stop_stream_manager" {
  count              = local.autoscaling ? 1 : 0
  provisioner "local-exec" {
    command = "aws ec2 stop-instances --instance-ids ${aws_instance.red5pro_sm[0].id} --region ${var.aws_region}"
    environment = {
      AWS_ACCESS_KEY_ID = "${var.aws_access_key}"
      AWS_SECRET_ACCESS_KEY = "${var.aws_secret_key}"
    }
  }
  depends_on = [aws_ami_from_instance.red5pro_sm_image[0]]
}

# Stop Origin Node instance using aws cli
resource "null_resource" "stop_node_origin" {
  count              = var.origin_image_create ? 1 : 0
  provisioner "local-exec" {
    command = "aws ec2 stop-instances --instance-ids ${aws_instance.red5pro_node_origin[0].id} --region ${var.aws_region}"
    environment = {
      AWS_ACCESS_KEY_ID = "${var.aws_access_key}"
      AWS_SECRET_ACCESS_KEY = "${var.aws_secret_key}"
    }
  }
  depends_on = [aws_ami_from_instance.red5pro_node_origin_image[0]]
}
# Stop Edge Node instance using aws cli
resource "null_resource" "stop_node_edge" {
  count              = var.edge_image_create ? 1 : 0
  provisioner "local-exec" {
    command = "aws ec2 stop-instances --instance-ids ${aws_instance.red5pro_node_edge[0].id} --region ${var.aws_region}"
    environment = {
      AWS_ACCESS_KEY_ID = "${var.aws_access_key}"
      AWS_SECRET_ACCESS_KEY = "${var.aws_secret_key}"
    }
  }
  depends_on = [aws_ami_from_instance.red5pro_node_edge_image[0]]
}
# Stop Transcoder Node instance using aws cli
resource "null_resource" "stop_node_transcoder" {
  count              = var.transcoder_image_create ? 1 : 0
  provisioner "local-exec" {
    command = "aws ec2 stop-instances --instance-ids ${aws_instance.red5pro_node_transcoder[0].id} --region ${var.aws_region}"
    environment = {
      AWS_ACCESS_KEY_ID = "${var.aws_access_key}"
      AWS_SECRET_ACCESS_KEY = "${var.aws_secret_key}"
    }
  }
  depends_on = [aws_ami_from_instance.red5pro_node_transcoder_image[0]]
}
# Stop Relay Node instance using aws cli
resource "null_resource" "stop_node_relay" {
  count              = var.relay_image_create ? 1 : 0
  provisioner "local-exec" {
    command = "aws ec2 stop-instances --instance-ids ${aws_instance.red5pro_node_relay[0].id} --region ${var.aws_region}"
    environment = {
      AWS_ACCESS_KEY_ID = "${var.aws_access_key}"
      AWS_SECRET_ACCESS_KEY = "${var.aws_secret_key}"
    }
  }
  depends_on = [aws_ami_from_instance.red5pro_node_relay_image[0]]
}

################################################################################
# Create node group (Stream Manager API)
################################################################################

resource "null_resource" "node_group" {
  count = var.node_group_create ? 1 : 0
  triggers = {
    trigger_name  = "node-group-trigger"
    SM_IP = "${local.stream_manager_ip}"
    SM_API_KEY = "${var.stream_manager_api_key}"
  }
  provisioner "local-exec" {
    when    = create
    command = "bash ${abspath(path.module)}/red5pro-installer/r5p_create_node_group.sh"
    environment = {
      NAME = "${var.name}"
      SM_IP = "${local.stream_manager_ip}"
      SM_API_KEY = "${var.stream_manager_api_key}"
      NODE_GROUP_REGION ="${var.aws_region}"
      NODE_GROUP_NAME = "${var.node_group_name}"
      ORIGINS = "${var.node_group_origins}"
      EDGES = "${var.node_group_edges}"
      TRANSCODERS = "${var.node_group_transcoders}"
      RELAYS = "${var.node_group_relays}"
      ORIGIN_INSTANCE_TYPE = "${var.node_group_origins_instance_type}"
      EDGE_INSTANCE_TYPE = "${var.node_group_edges_instance_type}"
      TRANSCODER_INSTANCE_TYPE = "${var.node_group_transcoders_instance_type}"
      RELAY_INSTANCE_TYPE = "${var.node_group_relays_instance_type}"
      ORIGIN_CAPACITY = "${var.node_group_origins_capacity}"
      EDGE_CAPACITY = "${var.node_group_edges_capacity}"
      TRANSCODER_CAPACITY = "${var.node_group_transcoders_capacity}"
      RELAY_CAPACITY = "${var.node_group_relays_capacity}"
      ORIGIN_IMAGE_NAME = "${try(aws_ami_from_instance.red5pro_node_origin_image[0].name, null)}"
      EDGE_IMAGE_NAME = "${try(aws_ami_from_instance.red5pro_node_edge_image[0].name, null)}"
      TRANSCODER_IMAGE_NAME = "${try(aws_ami_from_instance.red5pro_node_transcoder_image[0].name, null)}"
      RELAY_IMAGE_NAME = "${try(aws_ami_from_instance.red5pro_node_relay_image[0].name, null)}"
    }
  }
    provisioner "local-exec" {
    when    = destroy
    command = "bash ${abspath(path.module)}/red5pro-installer/r5p_delete_node_group.sh '${self.triggers.SM_IP}' '${self.triggers.SM_API_KEY}'"
  }

  depends_on = [aws_instance.red5pro_sm[0], aws_autoscaling_group.red5pro_sm_ag[0]]
}
# Ansible 

 Ansible :

*    Ansible  Slack 
*    Ansible 
*   



# 

Ansible 

# 

 IT Slack  Slack 

 GitHub `Chapter17/slack`[ 9 ](09.html#5L6AS0-0fda9dda24fc45e094341803448da041)** AWS  VPC`slack` Ansible 

# 

 Slack  Slack [https://slack.com/](https://slack.com/)

:

![](img/00159.jpeg)

`Incoming WebHooks`

 Webhook 

:

*   :#general
*   
*   :`Ansible`
*   :`Ansible`
*   :

:

![](img/00160.jpeg)

 Webhook URL :

`https://hooks.slack.com/services/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`

 Slack  Ansible 

# Ansible 

 VPC`group_vars/common.yml`:

```
---

environment_name: "VPC-Slack"
ec2_region: "eu-west-1"

slack:
  token: "TBCRVDMGA/BBCPTPNH1/tyudQIccviG7gh4JnfeoPScc"
  username: "Ansible"
  icon: "https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Robot_icon.svg/200px-Robot_icon.svg.png"
```

:

*   `token`: Webhook `https://hooks.slack.com/services/`
*   `username`: Ansible
*   `icon`:

 VPC `ec2_vpc_net` VPC  Slack  VPC :

```
- name: Send notification message via Slack all options
  slack:
    token: "{{ slack.token }}"
    msg: "Checking for VPC called '{{ environment_name }}'"
    username: "{{ slack.username }}"
    icon_url: "{{ slack.icon }}"
    link_names: 0
    parse: 'full'
```

`Checking for VPC called 'VPC-Slack'``token``username``icon`:

```
- name: ensure that the VPC is present
  ec2_vpc_net:
    region: "{{ ec2_region }}"
    name: "{{ environment_name }}"
    state: present
    cidr_block: "{{ vpc_cidr_block }}"
    resource_tags: { "Name" : "{{ environment_name }}", "Environment" : "{{ environment_name }}" }
  register: vpc_info
```

:`VPC-Slack` VPC  Ansible `VPC-Slack` VPC  Ansible  VPC :

```
- name: Send notification message via Slack all options
  slack:
    token: "{{ slack.token }}"
    msg: "VPC called '{{ environment_name }}' created with an ID of '{{ vpc_info.vpc.id }}'"
    username: "{{ slack.username }}"
    icon_url: "{{ slack.icon }}"
    link_names: 0
    parse: 'full'
  when: vpc_info.changed
```

`vpc_info` VPC `vpc_info`:

```
- name: Send notification message via Slack all options
  slack:
    token: "{{ slack.token }}"
    msg: "Found a VPC called '{{ environment_name }}' which has an ID of '{{ vpc_info.vpc.id }}'"
    username: "{{ slack.username }}"
    icon_url: "{{ slack.icon }}"
    link_names: 0
    parse: 'full'
  when: vpc_info.changed == false and vpc_info.failed == false
```

 Slack `Chapter17/slack`

# 

:

```
$ export AWS_ACCESS_KEY=AKIAI5KECPOTNTTVM3EDA
$ export AWS_SECRET_KEY=Y4B7FFiSWl0Am3VIFc07lgnc/TAtK5+RpxzIGTr
$ ansible-playbook -i production site.yml
```

 Slack :

![](img/00161.jpeg)

 VPC :

![](img/00162.jpeg)

 Ansible 

# 

Slack  Ansible 

# 

 Basecamp  Ansible :

```
- name: Send a message to Campfire
  campfire:
    subscription: "my_subscription"
    token: "my_subscription"
    room: "Demo"
    notify: "loggins"
    msg: "The task has completed and all is well"
```

# ()

 Webex Teams( Spark)Ansible :

```
- name: Send a message to Cisco Spark
  cisco_spark:
    recipient_type: "roomId"
    recipient_id: "{{ spark.room_id }}"
    message_type: "markdown"
    personal_token: "{{ spark.token }}"
    message: "The task has **completed** and all is well"
```

# 

CA Flowdock  GitHubBitbucketJenkins  Ansible:

```
- name: Send a message to a Flowdock inbox
  flowdock:
    type: "inbox"
    token: "{{ flowdock.token }}"
    from_address: "{{ flowdock.email }}"
    source: "{{ flowdock.source }}"
    msg: "The task has completed and all is well"
    subject: "Task Success"
```

# Hipchat

Hipchat  Atlassian :

```
- name: Send a message to a Hipchat room
  hipchat:
    api: "https://api.hipchat.com/v2/"
    token: "{{ hipchat.token }}"
    room: "{{ hipchat.room }}"
    msg: "The task has completed and all is well"
```

# 

Ansible  SMTP :

```
- name: Send an email using external mail servers
  mail:
    host: "{{ mail.smtp_host }}"
    port: "{{ mail.smtp_port }}"
    username: "{{ mail.smtp_username }}"
    password: "{{ mail.smtp_password }}"
    to: "Russ McKendrick "
    subject: "Task Success"
    body: "The task has completed and all is well"
  delegate_to: localhost
```

# Mattermost

Mattermost (Slack Webex Teams  Hipchat):

```
- name: Send a message to a Mattermost channel
  mattermost:
    url: "{{ mattermost.url }}"
    api_key: "{{ mattermost.api_key }}"
    text: "The task has completed and all is well"
    channel: "{{ mattermost.channel }}"
    username: "{{ mattermost.username }}"
    icon_url: "{{ mattermost.icon_url }}"
```

# 

 Ansible :

```
- name: Say a message on your Ansible host
  say:
    msg: "The task has completed and all is well"
    voice: "Daniel"
  delegate_to: localhost
```

# 

ServiceNow  ServiceNowInc . IT `snow_record` ServiceNow :

```
- name: Create an incident in ServiceNow
  snow_record:
    username: "{{ snow.username }}"
    password: "{{ snow.password }}"
    instance: "{{ snow.instance }}"
    state: "present"
    data:
      short_description: "The task has completed and all is well"
      severity: "3"
      priority: "3"
  register: snow_incident
```

# 

 syslog:

```
- name: Send a message to the hosts syslog
  syslogger:
    msg: "The task has completed and all is well"
    priority: "info"
    facility: "daemon"
    log_pid: "true"
```

# 

 Twilio  Ansible :

```
- name: Send an SMS message using Twilio
  twilio:
    msg: "The task has completed and all is well"
    account_sid: "{{ twilio.account }}"
    auth_token: "{{ twilio.auth }}"
    from_number: "{{ twilio.from_mumber }}"
    to_number: "+44 7911 123456"
  delegate_to: localhost
```

# 

Ansible Tower  Ansible AWX



`fetch``snow_record`

:

*   ****:[https://slack.com/](https://slack.com/)
*   ****:[https://basecamp.com/](https://basecamp.com/)
*   ** Webex ( Spark)**:[https://www.webex.com/products/teams/](https://www.webex.com/products/teams/)
*   ****:[https://www.flowdock.com/](https://www.flowdock.com/)
*   ****:[https://mattermost.com/](https://mattermost.com/)
*   **service now**:[https://www . service now . com/](https://www.servicenow.com/)
*   **Twilio**  [https://twilio.com/](https://twilio.com/)

# Ansible 

Ansible `say`:

```
---

- hosts: localhost
  gather_facts: false
  debugger: "on_failed"

  vars:
    message: "The task has completed and all is well"
    voice: "Daniel"

  tasks:
    - name: Say a message on your Ansible host
      say:
        msg: "{{ massage }}"
        voice: "{{ voice }}"
```

:`message``massage` Ansible 

# 

:

```
$ ansible-playbook playbook.yml
```

 localhost  Ansible `say`:

```
[WARNING]: Unable to parse /etc/ansible/hosts as an inventory source
[WARNING]: No inventory was parsed, only implicit localhost is available
[WARNING]: provided hosts list is empty, only localhost is available. Note that the implicit
localhost does not match 'all'
```

Ansible :

```
PLAY [localhost] ***********************************************************************************

TASK [Say a message on your Ansible host] **********************************************************
fatal: [localhost]: FAILED! => {"msg": "The task includes an option with an undefined variable. The error was: 'massage' is undefined\n\nThe error appears to have been in '/Users/russ/Documents/Code/learn-ansible-fundamentals-of-ansible-2x/chapter17/say/playbook.yml': line 12, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n tasks:\n - name: Say a message on your Ansible host\n ^ here\n"}
```

 shell  Ansible :

```
[localhost] TASK: Say a message on your Ansible host (debug)>
```

:

```
p result._result
```

**:

```
[localhost] TASK: Say a message on your Ansible host (debug)> p result._result
{'failed': True,
 'msg': u"The task includes an option with an undefined variable. The error was: 'massage' is undefined\n\nThe error appears to have been in '/Users/russ/Documents/Code/learn-ansible-fundamentals-of-ansible-2x/chapter17/say/playbook.yml': line 12, column 7, but may\nbe elsewhere in the file depending on the exact syntax problem.\n\nThe offending line appears to be:\n\n tasks:\n - name: Say a message on your Ansible host\n ^ here\n"}
[localhost] TASK: Say a message on your Ansible host (debug)>
```

:

```
p task.args
```

:

```
[localhost] TASK: Say a message on your Ansible host (debug)> p task.args
{u'msg': u'{{ massage }}', u'voice': u'{{ voice }}'}
[localhost] TASK: Say a message on your Ansible host (debug)>
```

:

```
p task_vars
```

 Ansible :

```
[localhost] TASK: Say a message on your Ansible host (debug)> p task_vars
{'ansible_check_mode': False,
 'ansible_connection': 'local',
 'ansible_current_hosts': [u'localhost'],
 'ansible_diff_mode': False,
 'ansible_facts': {},
 'ansible_failed_hosts': [],
 'ansible_forks': 5,
 'ansible_inventory_sources': [u'/etc/ansible/hosts'],
 'ansible_play_batch': [],
 'ansible_play_hosts': [u'localhost'],
 'ansible_play_hosts_all': [u'localhost'],
 'ansible_playbook_python': '/usr/bin/python',
 'ansible_python_interpreter': '/usr/bin/python',
 'ansible_run_tags': [u'all'],
 'ansible_skip_tags': [],
 'ansible_version': {'full': '2.5.5',
 'major': 2,
 'minor': 5,
 'revision': 5,
 'string': '2.5.5'},
 'environment': [],
 'group_names': [],
 'groups': {'all': [], 'ungrouped': []},
 'hostvars': {},
 'inventory_hostname': u'localhost',
 'inventory_hostname_short': u'localhost',
 u'message': u'The task has completed and all is well',
 'omit': '__omit_place_holder__0529a2749315462e1ae1a0d261987dedea3bfdad',
 'play_hosts': [],
 'playbook_dir': u'/Users/russ/Documents/Code/learn-ansible-fundamentals-of-ansible-2x/chapter17/say',
 u'voice': u'Daniel'}
[localhost] TASK: Say a message on your Ansible host (debug)>
```

`u`:`voice``message`:

```
p task_vars['message']
p task_vars['voice']
```

:

```
[localhost] TASK: Say a message on your Ansible host (debug)> p task_vars['message']
u'The task has completed and all is well'
[localhost] TASK: Say a message on your Ansible host (debug)> p task_vars['voice']
u'Daniel'
[localhost] TASK: Say a message on your Ansible host (debug)>
```

`msg`:

```
task.args['msg'] = '{{ message }}'
```

:

```
redo
```

**:

```
[localhost] TASK: Say a message on your Ansible host (debug)> task.args['msg'] = '{{ message }}'
[localhost] TASK: Say a message on your Ansible host (debug)> redo
changed: [localhost]

```

```
PLAY RECAP ************************************************************************************************************************************
localhost : ok=1 changed=1 unreachable=0 failed=0
```



`continue``quit`

# Ansible 

Ansible  20  15 

 Ansible  15 

# 

 Ansible  Ansible : Ansible 

# 



 Slack  Hubot  Jenkins :

![](img/00163.jpeg)

 Slack :

*@bot  linux *

 Ansible playbook  AWS  playbook :

*@bot *

 GIF:

![](img/00164.jpeg)

 HubotHubot  GitHub  chatbot Slack `hubot-slack`

`hubot-alias` *@bot  linux **build AWS launch OS = Linux*`hubot-yardmaster`

Jenkins  Jenkins Ansible  Jenkins Git  AWS  Jenkins  Jenkins [ 9 ](09.html#5L6AS0-0fda9dda24fc45e094341803448da041)**[ 10 ](10.html#62HIO0-0fda9dda24fc45e094341803448da041)**

 GIF  AMI  Ansible Slack 

 Hubot Jenkins 

# 

 DockerGitHubJenkins  Ansible AWX  GitHub 

 Jenkins  Ansible  Jenkins  Docker  AWX** GitHub**  GitHub**** AWX 

AWX  GitHub Webhooks  **Jenkins**  **Jenkins GitHub**  AWX 

 AWX 



AWX 

# 

 Ansible Ansible  Michael DeHaan :

"Anyone using Ansible for a few months is as good as anyone using Ansible for three years. It's a simple tool on purpose."

 Ansible 

 Ansible Galaxy  Ansible 

[ 1 ](01.html#J2B80-0fda9dda24fc45e094341803448da041)* Ansible*  Ansible Ansible 

# 

:

*   **hubot**:[https://hubot . github . com](https://hubot.github.com)
*   **hubot slack**:[https://github . com/slackapi/hubot-slack](https://github.com/slackapi/hubot-slack)
*   **Hubot **:[https://github . com/dtaniwaki/hubot ](https://github.com/dtaniwaki/hubot-alias)
*   **hubot yadmaster**:[https://github . com/hacklanta/hubot-yadmaster](https://github.com/hacklanta/hubot-yardmaster)
*   **Jenkins git**:[https://plugins . Jenkins . io/git](https://plugins.jenkins.io/git)
*   ****:[https://plugins.jenkins.io/ansible](https://plugins.jenkins.io/ansible)
*   **Jenkins github**:[https://plugins . Jenkins . io/github](https://plugins.jenkins.io/github)
*   ****:[https://plugins.jenkins.io/ansible-tower](https://plugins.jenkins.io/ansible-tower)# Required variables
# - Fill in before running tf
# ==========================================================

# AWS Access Key
AWS_ACCESS_KEY= "XXXXXXXX"

# AWS Secret Key
AWS_SECRET_KEY= "XXXXXXXX"

# Private key pass for ec2
PUBLIC_KEY_PATH = "XXXX"

# Private key pass for ec2
PRIVATE_KEY_PATH = "XXXXX"

# Policy path
# POLICY_PATH = ""

# Password used to log in to the `admin` account on the new Rancher server
# PDP_API_KEY= ""
# PDP_API_SECRET= ""

# Optional variables, uncomment to customize the setup
# -------------------------------------------------------------
description: |
  User variables allow your templates to be further configured with variables
  from the command-line, environment variables, or files. This lets you
  parameterize your templates so that you can keep secret tokens,
  environment-specific data, and other types of information out of your
  templates. This maximizes the portability and shareability of the template.
page_title: User Variables - Templates
sidebar_title: User Variables
---

`@include 'from-1.5/legacy-json-warning.mdx'`

# Template User Variables

User variables allow your templates to be further configured with variables
from the command-line, environment variables, Vault, or files. This lets you
parameterize your templates so that you can keep secret tokens,
environment-specific data, and other types of information out of your
templates. This maximizes the portability of the template.

Using user variables expects you to know how [configuration
templates](/docs/templates/legacy_json_templates/engine) work. If you don't know how
configuration templates work yet, please read that page first.

## Usage

In order to set a user variable, you must define it either within the
`variables` section within your template, or using the command-line `-var` or
`-var-file` flags.

Even if you want a user variable to default to an empty string, it is best to
explicitly define it. This explicitness helps reduce the time it takes for
newcomers to understand what can be modified using variables in your template.

The `variables` section is a key/value mapping of the user variable name to a
default value. A default value can be the empty string. An example is shown
below:

```json
{
  "variables": {
    "aws_access_key": "",
    "aws_secret_key": ""
  },

  "builders": [
    {
      "type": "amazon-ebs",
      "access_key": "{{user `aws_access_key`}}",
      "secret_key": "{{user `aws_secret_key`}}"
      // ...
    }
  ]
}
```

In the above example, the template defines two user variables: `aws_access_key`
and `aws_secret_key`. They default to empty values. Later, the variables are
used within the builder we defined in order to configure the actual keys for
the Amazon builder.

If the default value is `null`, then the user variable will be _required_. This
means that the user must specify a value for this variable or template
validation will fail.

User variables are used by calling the `{{user}}` function in the form of
`{{user 'variable'}}`. This function can be used in _any value_ but `type`
within the template: in builders, provisioners, _anywhere outside the `variables` section_.
User variables are available globally within the rest of the template.

## Environment Variables

Environment variables can be used within your template using user variables.
The `env` function is available _only_ within the default value of a user
variable, allowing you to default a user variable to an environment variable.
An example is shown below:

```json
{
  "variables": {
    "my_secret": "{{env `MY_SECRET`}}"
  }
}
```

This will default "my_secret" to be the value of the "MY_SECRET" environment
variable (or an empty string if it does not exist).

-> **Why can't I use environment variables elsewhere?** User variables are
the single source of configurable input to a template. We felt that having
environment variables used _anywhere_ in a template would confuse the user
about the possible inputs to a template. By allowing environment variables only
within default values for user variables, user variables remain as the single
source of input to a template that a user can easily discover using
`packer inspect`.

-> **Why can't I use `~` for home variable?** `~` is an special variable
that is evaluated by shell during a variable expansion. As Packer doesn't run
inside a shell, it won't expand `~`.

## Consul keys

Consul keys can be used within your template using the `consul_key` function.
This function is available _only_ within the default value of a user variable,
for reasons similar to environment variables above.

```json
{
  "variables": {
    "soft_versions": "{{ consul_key `my_image/softs_versions/next` }}"
  }
}
```

This will default `soft_versions` to the value of the key
`my_image/softs_versions/next` in consul.

The configuration for consul (address, tokens, ...) must be specified as
environment variables, as specified in the
[Documentation](https://www.consul.io/docs/commands#environment-variables).

## Vault Variables

Secrets can be read from [Vault](https://www.vaultproject.io/) and used within
your template as user variables. the `vault` function is available _only_
within the default value of a user variable, allowing you to default a user
variable to a vault secret.

An example of using a v2 kv engine:

If you store a value in vault using `vault kv put secret/hello foo=world`, you
can access it using the following template engine:

```json
{
  "variables": {
    "my_secret": "{{ vault `/secret/data/hello` `foo`}}"
  }
}
```

which will assign "my_secret": "world"

An example of using a v1 kv engine:

If you store a value in vault using:

    vault secrets enable -version=1 -path=secrets kv
    vault kv put secrets/hello foo=world

You can access it using the following template engine:

    {
      "variables": {
        "VAULT_SECRETY_SECRET": "{{ vault `secrets/hello` `foo`}}"
      }
    }

This example accesses the Vault path `secret/data/foo` and returns the value
stored at the key `bar`, storing it as "my_secret".

In order for this to work, you must set the environment variables `VAULT_TOKEN`
and `VAULT_ADDR` to valid values.

The api tool we use allows for more custom configuration of the Vault client via
environment variables.

The full list of available environment variables is:

```text
"VAULT_ADDR"
"VAULT_AGENT_ADDR"
"VAULT_CACERT"
"VAULT_CAPATH"
"VAULT_CLIENT_CERT"
"VAULT_CLIENT_KEY"
"VAULT_CLIENT_TIMEOUT"
"VAULT_SKIP_VERIFY"
"VAULT_NAMESPACE"
"VAULT_TLS_SERVER_NAME"
"VAULT_WRAP_TTL"
"VAULT_MAX_RETRIES"
"VAULT_TOKEN"
"VAULT_MFA"
"VAULT_RATE_LIMIT"
```

and detailed documentation for usage of each of those variables can be found
[here](https://www.vaultproject.io/docs/commands/#environment-variables).

## AWS Secrets Manager Variables

Secrets can be read from [AWS Secrets Manager](https://aws.amazon.com/secrets-manager/)
and used within your template as user variables. The `aws_secretsmanager` function is
available _only_ within the default value of a user variable, allowing you to default
a user variable to an AWS Secrets Manager secret.

### Plaintext Secrets

```json
{
  "variables": {
    "password": "{{ aws_secretsmanager `globalpassword` }}"
  }
}
```

In the example above it is assumed that the secret `globalpassword` is not
stored as a key pair but as a single non-JSON string value. Which the
`aws_secretsmanager` function will return as a raw string.

### Single Key Secrets

```json
{
  "variables": {
    "password": "{{ aws_secretsmanager `sample/app/password` }}"
  }
}
```

In the example above it is assumed that only one key is stored in
`sample/app/password` if there are multiple keys stored in it then you need
to indicate the specific key you want to fetch as shown below.

### Multiple Key Secrets

```json
{
  "variables": {
    "db_password": "{{ aws_secretsmanager `sample/app/passwords` `db` }}",
    "api_key": "{{ aws_secretsmanager `sample/app/passwords` `api_key` }}"
  }
}
```

In order to use this function you have to configure valid AWS credentials using
one of the following methods:

- [Environment Variables](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html)
- [CLI Configuration Files](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html)
- [Container Credentials](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html)
- [Instance Profile Credentials](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html)

## Using array values

Some templates call for array values. You can use template variables for these,
too. For example, the `amazon-ebs` builder has a configuration parameter called
`ami_regions`, which takes an array of regions that it will copy the AMI to.
You can parameterize this by using a variable that is a list of regions, joined
by a `,`. For example:

```json
{
  "variables": {
    "destination_regions": "us-west-1,us-west-2"
  },
  "builders": [
    {
      "ami_name": "packer-qs-{{timestamp}}",
      "instance_type": "t2.micro",
      "region": "us-east-1",
      "source_ami_filter": {
        "filters": {
          "name": "*ubuntu-xenial-16.04-amd64-server-*",
          "root-device-type": "ebs",
          "virtualization-type": "hvm"
        },
        "most_recent": true,
        "owners": ["099720109477"]
      },
      "ami_regions": "{{user `destination_regions`}}",
      "ssh_username": "ubuntu",
      "type": "amazon-ebs"
    }
  ]
}
```

## Setting Variables

Now that we covered how to define and use user variables within a template, the
next important point is how to actually set these variables. Packer exposes two
methods for setting user variables: from the command line or from a file.

### From the Command Line

To set user variables from the command line, the `-var` flag is used as a
parameter to `packer build` (and some other commands). Continuing our example
above, we could build our template using the command below. The command is
split across multiple lines for readability, but can of course be a single
line.

```shell-session
$ packer build \
    -var 'aws_access_key=foo' \
    -var 'aws_secret_key=bar' \
    template.json
```

As you can see, the `-var` flag can be specified multiple times in order to set
multiple variables. Also, variables set later on the command-line override any
earlier set variable of the same name.

**warning** If you are calling Packer from cmd.exe, you should double-quote
your variables rather than single-quoting them. For example:

`packer build -var "aws_secret_key=foo" template.json`

### From a File

Variables can also be set from an external JSON file. The `-var-file` flag
reads a file containing a key/value mapping of variables to values and sets
those variables. An example JSON file may look like this:

```json
{
  "aws_access_key": "foo",
  "aws_secret_key": "bar"
}
```

It is a single JSON object where the keys are variables and the values are the
variable values. Assuming this file is in `variables.json`, we can build our
template using the following command:

```text
On Linux :
$ packer build -var-file=variables.json template.json
On Windows :
packer build -var-file variables.json template.json
```

The `-var-file` flag can be specified multiple times and variables from
multiple files will be read and applied. As you'd expect, variables read from
files specified later override a variable set earlier.

Combining the `-var` and `-var-file` flags together also works how you'd
expect. Variables set later in the command override variables set earlier. So,
for example, in the following command with the above `variables.json` file:

```shell-session
$ packer build \
    -var 'aws_access_key=bar' \
    -var-file=variables.json \
    -var 'aws_secret_key=baz' \
    template.json
```

Results in the following variables:

| Variable       | Value |
| -------------- | ----- |
| aws_access_key | foo   |
| aws_secret_key | baz   |

# Sensitive Variables

If you use the environment to set a variable that is sensitive, you probably
don't want that variable printed to the Packer logs. You can make sure that
sensitive variables won't get printed to the logs by adding them to the
"sensitive-variables" list within the Packer template:

```json
{
  "variables": {
    "my_secret": "{{env `MY_SECRET`}}",
    "not_a_secret": "plaintext",
    "foo": "bar"
  },

  "sensitive-variables": ["my_secret", "foo"],
  ...
}
```

The above snippet of code will function exactly the same as if you did not set
"sensitive-variables", except that the Packer UI and logs will replace all
instances of "bar" and of whatever the value of "my_secret" is with
``. This allows you to be confident that you are not printing
secrets in plaintext to our logs by accident.

# Recipes

## Making a provisioner step conditional on the value of a variable

There is no specific syntax in Packer templates for making a provisioner step
conditional, depending on the value of a variable. However, you may be able to
do this by referencing the variable within a command that you execute. For
example, here is how to make a `shell-local` provisioner only run if the
`do_nexpose_scan` variable is non-empty.

```json
{
  "type": "shell-local",
  "command": "if [ ! -z \"{{user `do_nexpose_scan`}}\" ]; then python -u trigger_nexpose_scan.py; fi"
}
```

## Using HOME Variable

In order to use `$HOME` variable, you can create a `home` variable in Packer:

```json
{
  "variables": {
    "home": "{{env `HOME`}}"
  }
}
```

And this will be available to be used in the rest of the template, i.e.:

```json
{
  "builders": [
    {
      "type": "google",
      "account_file": "{{ user `home` }}/.secrets/gcp-{{ user `env` }}.json"
    }
  ]
}
```
::::::::::::::::::::::::::::::::::::::::::::DATABASE API KEY:::::::::::::::::::::::::::::::::::::::::::::::::::::::::;
//mongoURL//= mongodb+srv://xxxxx:skillseeds@123@testcluster.j5b7w.mongodb.net/disciplesbay
//mongoURL= mongodb://127.0.0.1:27017/disciplebay?retryWrites=true
//mongoURL=mongodb+srv://faskt:FasktTechnologies@2020@cluster0.rqyuo.mongodb.net/desciplebay?retryWrites=true&w=majority
mongoURL=mongodb+srv://coza:coza_global@cluster0.ebfiu.mongodb.net/coza_global?retryWrites=true&w=majority
::::::::::::::::::::::::::::::::::::::::::::NODEMAILER GMAIL API KEY::::::::::::::::::::::::::::::::::::::::::::::
GMAIL_USER=dungji.zacks.9@gmail.com
GMAIL_PASS=if(!User){ return true; }

JWT_SECRET=qQFkIPJhAYFWYKMciqk1EExA4n/xq1lWvxek5CJ4BbmbkGzxdzjKfHIf4292B2rR
//d
CLOUDINARY_CLOUD_NAME=removedvalue
CLOUDINARY_API_KEY=954963132436281 
CLOUDINARY_API_SECRET=1LN7orHh5fiDm2aZyfnol2aFYDE
SUPER_ADMIN=super_secret@deciplebay.test
SUPER_ADMIN_PASSWORD=replaced
SUPER_USERNAME=super_supter
SUPER_ADMIN_ROLE_NAME=super_secret
USER_ROLE_NAME=user
FBASE_DB='https://cozb-28b14.firebaseio.com'
AWS_ACCESS_KEY_ID=AKIARNCT3H26ZTTMGXXW
AWS_SECRET_KEY=pz+tOBgmTk2jyQ8fpw33ErPA9LH/bcsXAc3SGnJz
AWS_REGION=eu-west-2
S3_BUCKET=coza
/home/zacks/Downloads/android-studio-ide-201.6953283-linux/android-studio/bin
ssh -i ~/Downloads/coza.pem ubuntu@ec2-3-133-214-180.us-east-2.compute.amazonaws.com# Datadog technical exercise for Solutions Engineers

Datadog is a full service cloud-scale monitoring solution that gathers metrics and events generated by both your infrastructure
as well as your applications. From containers, to cloud providers, to bare metal machines, from container management, to databases, to web servers,
datadog is able to handle most modern infrastructure solutions.

Datadog provides a wide array of integrations that can handle common infrastructure needs. These integrations allow you to gather metrics 
quickly and in one central location. All metrics and statistics datadog gathers can be displayed in a single pane of glass which can be on a single
screen in a NOC or other operations center. These metrics can be turned into graphs and dashboards which can then be drilled down into further for 
a more detailed look at your infrastructure. Alerts can also be created to warn your teams that something is wrong before it becomes a larger problem.
Alerts can be sent in a variety of ways, including email, pagerduty, slack, hipchat, and others.

You can test datadog out yourself here [https://www.datadoghq.com/](https://www.datadoghq.com/) for a free 14 day trial.

## Contents

- [Prerequisites](#Prerequisites)
   - [Setup an AWS user for Terraform](#Setup-an-AWS-user-for-Terraform)
   - [Install Terraform](#Install-Terraform)
   - [Install Ansible](#Install-Ansible)

- [Data Collection](#Data-Collection)
   - [Level 0 - Setup a ubuntu instance](#Level--0--Setup-a-ubuntu-instance)
      - [Auto build EC2 instance with Terraform](#Auto-build-EC2-instance-with-Terraform)

   - [Level 1 - Collect your data](#Level--1--Collect-your-data)
      - [Auto installing the agent with Ansible](#Auto-installing-the-agent-with-Ansible)
      - [Bonus: What is the agent?](#Bonus--What-is-the-agent?)
      - [Adding tags](#Adding-tags)
      - [Auto install MySQL with Ansible](#Auto-install-MySQL-with-Ansible)
      - [Custom Agent Check](#Custom-Agent-Check)

   - [Level 2 - Visualizing your data](#Level--2--Visualizing-your-data)
      - [Clone your database integration dashboard](#Clone-your-database-integration-dashboard)
      - [Bonus: What is the difference between a timeboard and a screenboard?](#Bonus--What-is-the-difference-between-a-timeboard-and-a-screenboard?)
      - [Grab a snapshot of your test random graph, draw a box when above 0.90 and email](#Grab-a-snapshot-of-your-test-random-graph,-draw-a-box-when-above-0.90-and-email)

   - [Level 3 - Alerting on your data](#Level--3--Alerting-on-your-data)
      - [Monitoring your metrics, set an alert for test random for over 0.90](#Monitoring-your-metrics,-set-an-alert-for-test-random-for-over-0.90)
      - [Bonus: Make it multi-alert by host for scalability](#Bonus--Make-it-multi-alert-by-host-for-scalability)
      - [Set monitor name and message](#Set-monitor-name-and-message)
      - [Monitor alert Email](#Monitor-alert-Email)
      - [Bonus: Set scheduled downtime for monitor, make sure Email is notified](#Bonus--Set-scheduled-downtime-for-monitor,-make-sure-Email-is-notified)

- [Conclusion](#Conclusion)

# Prerequisites

## Setup an AWS user for Terraform

- Setting up an AWS user for Terraform [Amazon Web Services(AWS)](https://aws.amazon.com/) is the leading cloud compute provider. 
  They offer a wide range of infrastructure services.
- Create a user in IAM, there are three steps to follow to create an IAM user to use with Terraform. 
- One, give the user a name and make sure they have programmatic access only. There's no need for console access with Terraform.

![Create_AWS_User_Step1](screenshots/Create_AWS_User_Step1.png)

- Two, in order to have Terraform create our EC2 instance we'll need the proper AWS permissions. For this user they will need only 
  **AmazonEC2FullAccess** in order to create and destroy EC2 instances.

![Create_AWS_User_Step2_Permissions](screenshots/Create_AWS_User_Step2_Permissions.png)

- Finally, You will need both the access and secret keys for the user. These will be used at runtime so as to not save them in a file and cause a security risk.

![Create_AWS_User_Step3_Keys](screenshots/Create_AWS_User_Step3_Keys.png)

## Install Terraform

- [Installing Terraform](https://www.terraform.io/downloads.html) Terraform is much more then just a configuration management tool. It lets you define your 
  infrastructure as code. Download it for your appropriate OS.

- Download the correct version of Terraform for your OS, I have downloaded **terraform_0.10.3_darwin_amd64.zip** for MacOSX. Unzipping inflates the file you then move that file 
  to somewhere that is in your PATH. I have moved the file to **/usr/local/bin** which lets me access the terraform command directly.

## Install Ansible

- [Installing Ansible](http://docs.ansible.com/ansible/latest/intro_installation.html) Ansible is an open source configuration management tool. Powerful but yet extremely simple to use.
  Ansible can handle not only configuration management but application deployments, and task automation. Installation instructions are dependent on OS used.

- Since I am installing on MacOSX I will be using pip which is python's package manager. You can install pip by downloading this file [Pip](https://bootstrap.pypa.io/get-pip.py) 
  and running python get-pip.py. 

- Now that you have pip you can install ansible by simply running **sudo pip install ansible** this will install ansible and a series of other applications. We'll mainly concentrate on 
  ansible-playbook.

# Data Collection

## Level 0 Setup an Ubuntu Instance

## Auto build EC2 instance with Terraform

- We will auto build our EC2 instance that will run our database, as well as our datadog agent. This will be a t2.large EC2 instance in AWS. We will build this with Terraform

- The code below will build our instance for us.

```terraform
provider "aws" {
  access_key    = "${var.access_key}"
  secret_key    = "${var.secret_key}"
}   
   
resource "aws_instance" "Datadog_Tech_Example" {
  ami                         = "ami-cd0f5cb6"
  instance_type               = "t2.large"
  associate_public_ip_address = true
  key_name                    = "DD_Testing"
  vpc_security_group_ids = [
      "sg-033ebf73"
  ]

  tags {
    Name = "Datadog_Tech_Example"
  }

  provisioner "local-exec" {
    command = "sleep 120; ANSIBLE_HOST_KEY_CHECKING=False AWS_ACCESS_KEY=${var.access_key} AWS_SECRET_KEY=${var.secret_key} ansible-playbook /Users/hack/dd_solution_engineer/ansible/Tasks/main.yml -u 	ubuntu --private-key /Users/hack/.ssh/DD_Testing.pem -i '${aws_instance.Datadog_Tech_Example.public_ip},'"
  }
}
```

- A short overview of the code above, we set the provider to be AWS (terraform can also be used with other cloud providers such as google). For the access and secret keys we will use the ones we
  generated earlier for the user. These will be input during run-time. We are going to build an aws_instance resource called **Datadog_Tech_Example**, we are using the Ubuntu 16.04 AMI,
  its size will be t2.large, we will give it a public IP address, and access it using a previously created ssh key. The newly created instance will be associated with a previously created
  security group that grants SSH access as well as access for the datadog agent to communicate on port 8125. Finally we tag it with a Name so we can see it in the AWS Console. We will go over the
  provisioner portion in the next step when we install both a MySQL database as well as the datadog agent itself.

## Level 1 Collect your data

## Auto installing the agent with Ansible

- We will use Ansible to install the agent on our host automatically. This will tie in with Terraform, we will create an ansible playbook that will be run by Terraform when our EC2 instance
  is created

```yaml
---
- hosts: all
  become: true
  gather_facts: False
  tasks:

    - name: Run dd-agent install script
      raw: DD_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX bash -c "$(curl -L https://raw.githubusercontent.com/DataDog/dd-agent/master/packaging/datadog-agent/source/install_agent.sh)"

    - name: Copy the data dog conf file
      template:
        src: ../Templates/datadog.conf.j2
        dest: /etc/dd-agent/datadog.conf
        mode: 0640

    - name: Copy the data dog mysql conf.d yaml file
      template:
        src: ../Templates/mysql.yaml.j2
        dest: /etc/dd-agent/conf.d/mysql.yaml
        mode: 0644

    - name: Copy the sample random conf.d yaml file
      template:
        src: ../Templates/sample_random.yaml.j2
        dest: /etc/dd-agent/conf.d/sample_random.yaml
        mode: 0644

    - name: Copy the sample random python script
      template:
        src: ../Templates/sample_random.py.j2
        dest: /etc/dd-agent/checks.d/sample_random.py
        mode: 0644

    - name: Stop datadog agent
      command: /etc/init.d/datadog-agent stop

    - name: Start datadog agent
      command: /etc/init.d/datadog-agent start
```

- Reviewing this playbook, we use **hosts: all** as we wont know the EC2 hostname until creation, become sudo to run as root, we do not need to gather facts at this time.
  We then run the datadog installation one-liner (with API-KEY X'd out here). We have a copy of a datadog conf file setup already from an install by hand, we 
  copy this over with the correct permissions. We do the same thing with our database yaml file (in this case MySQL) our sample random yaml as well as our 
  actual sample random script. These are placed in their respective directories (conf.d for our yamls, checks.d for our agent check). We then restart our agent for these 
  additions to take effect.

## Bonus: What is the agent?

> The datadog agent is a collector, it collects data, events and metrics about your infrastructure and applications. It does this via pre-written checks and integrations that can be 
  used to monitor the majority of major platforms including cloud providers, databases, caching solutions, and web servers. The agent collects these metrics and events, and sends them 
  to datadog to be presented to you in a manner that will let you address pressing infrastructure issues whether they are immediate or something that will be taken care of in the future.
  Additionally custom checks can be written to monitor custom applications or to collect data that is in a non standard format.

## Adding tags

- Adding tags is a great way to identify and filter your infrastructure in datadog. Its as easy as updating the datadog.conf file, in the main section under host tags. Here is an example.

  ![Tag_Set_In_DDConf](screenshots/Tag_Set_In_DDConf.png)

  We can set individual tags as well as by environment or role or however you designate your infrastructure. Here we set a random tag **hacktag**, then an env tag and we put this host in
  the dev environment. We give this host the role of a web server, but we can make this what we want it
  to be if we have multiple database servers we can give it a role of database.

  Because we have installed this via Ansible and the agent has been restarted these tags are available from the start. You can see them if you click infrastructure -> infrastructure list 
  and then click your host. You can see an example below.

  ![Hostmap_tags_page](screenshots/Hostmap_tags_page.png)

  You can also see them via infrastructure -> Hostmap and clicking on your host. With the power of tags you are able to quickly search, distinguish, and filter your infrastructure per your needs. 

## Auto install MySQL with Ansible

- We will also be installing our database (MySQL) with Ansible. There are a couple of extra steps needed since this is the first file run when we build our instance. 
  Below is the file we will use to build out our database.

```yaml
---
- hosts: all
  become: true
  gather_facts: False
  tasks:
  - name: Load in vars
    include_vars:
      file: ../Defaults/main.yml

  - name: Install python 2 for ubuntu 1604
    raw: test -e /usr/bin/python || (apt -y update && apt install -y python-minimal)

  - name: apt-get update on first boot
    apt:
       update_cache: yes

  - name: Install MySql packages
    apt:
     name: "{{ item }}"
     state: installed
     update_cache: yes
    with_items: "{{ ubuntu_1604_mysql_pkgs }}"

  - name: Set MySql root password before installing
    debconf: name='mysql-server' question='mysql-server/root_password' value='{{ mysql_root_password | quote }}' vtype='password'

  - name: Confirm MySql root password before installing
    debconf: name='mysql-server' question='mysql-server/root_password_again' value='{{ mysql_root_password | quote }}' vtype='password'

  - name: Create datadog user in mysql db
    command: mysql -e "create user 'datadog'@'localhost' identified by '{{ mysql_datadog_user_password | quote }}';"

  - name: Grant replication rights to datadog user
    command: mysql -e "GRANT REPLICATION CLIENT ON *.* TO 'datadog'@'localhost' WITH MAX_USER_CONNECTIONS 5;"

  - name: Grant full metrics to datadog user
    command: mysql -e "GRANT PROCESS ON *.* TO 'datadog'@'localhost';"

  - name: Grant access to performance schema to datadog user
    command: mysql -e "GRANT SELECT ON performance_schema.* TO 'datadog'@'localhost';"
```

- Since we run main task first when building the instance and since the first task run is to install MySQL we need to have two additions to this yaml file. First we load some vars which contain a list
  of the MySQL packages we will be installing. Secondly, we need to install python 2 for ubuntu 1604 as Ansible will throw an error if we only use python 3. We now update the cache on our ubuntu host, 
  then install the correct MySQL packages. As MySQL is being installed it will prompt the user for root password we have that automated as well by Ansible we set the password in the same file 
  that contains the vars and call that variable here identified by **mysql_root_password**. This needs to be confirmed and then we move on to creating the datadog user which will be used to report
  on the metrics of our database. The same var file contains the the datadog user password under the **mysql_datadog_user_password** variable. We grant the datadog user the proper permissions to 
  monitor our database and once the instance is up it with MySQL running it will be fully monitored.

## Custom Agent Check

- Creating a custom agent check is super easy with datadog, you can find more detailed instructions here [Datadog check creation](https://docs.datadoghq.com/guides/agent_checks/), but we can see a quick
  example below. 

- Our example will be a simple random sample check in python. The file that contains the logic of our check (in this example sample_random.py) will be placed in the **/etc/dd-agent/checks.d** directory. 
  While the configuration file for our check will live in **/etc/dd-agent/conf.d**. The configuration file will be in yaml while our checks can be written in python. There are API plugins for Ruby as well.

- Here is our sample_random.py file [sample_random.py.j2](ansible/Templates/sample_random.py.j2)
```python
from checks import AgentCheck
import random

class RandomCheck(AgentCheck):
	def check(self, instance):
		randNum = random.random()
		self.gauge('test.support.random', randNum)
```

- Since we have automated the install this check along with its configuration file is automatically installed in the correct directories once the agent is installed. You can see the results below.

![Custom_Check](screenshots/Custom_Check.png)

## Level 2 - Visualizing your data

## Clone your database integration dashboard

- Human beings are extremely good at recognizing patterns and irregularities within those patterns. This makes visualizing the data of an entire company's infrastructure so important, trying to
  read plain text log files to capture events simply will not scale as your company grows. 

- One of the benefits of datadog is the ability to create dashboards to gather these patterns in a visual manner for us. These can in turn be even more customized to fit different areas of your
  organization. There are two examples below, first is a very basic dashboard for our MySQL database and our sample_random script. These only contain four widgets, checking cpu time performance
  on our database instance as well as user performance time. The other two widgets are to check the change in our sample_random script and a graph of the sample_random output.

![Original_Dashboard](screenshots/Original_Dashboard.png)

- Second, if say a colleague uses your dashboard but needs additional information that pertains to their specific needs you can clone your dashboard and then edit this newly created dashboard.
  Below we have cloned our original dashboard above and added two new widgets, one to check on any "status:error" within our database, and the second to measure the number of connections there
  are to our database.

![Cloned_Dashboard](screenshots/Cloned_Dashboard.png)

- Cloning and adjusting your dashboards to fit the needs of the different teams monitoring your infrastructure can help make your responses to incidents more flexible and detailed.

## Bonus: What is the difference between a timeboard and a screenboard?

> A timeboard is used to correlate metrics and events to assist with troubleshooting. They are usually laid out in a specific way which in turn does not allow for much modification. 
  A screenboard is a more traditional screen you would see in a NOC or other operations center. They can be customized to a much greater degree then a timeboard. These are mainly used
  for displaying status data and providing a broad overview of your infrastructure.

## Grab a snapshot of your test random graph, draw a box when above 0.90 and email

![Graph_Over_Nine](screenshots/Graph_Over_Nine.png)

## Level 3 - Alerting on your data

## Monitoring your metrics, set an alert for test random for over 0.90

- Creating monitors to alert when something is wrong in your infrastructure is central to how datadog works. Below we setup a monitor to check when our Sample_Random script is above 0.90. 

![Monitor_Random_Over_Nine](screenshots/Monitor_Random_Over_Nine.png)

## Bonus: Make it multi-alert by host for scalability

- Scalability is an important part of monitoring, no one wants to reinvent the wheel every time a new host is brought up in your environment. We can set our Sample_Random check to multi-alert
  by host when we create our monitor. 

![Monitor_Random_Setup_Each_Host](screenshots/Monitor_Random_Setup_Each_Host.png)

## Set monitor name and message

- Setting unique monitor names will help your teams quickly identify what exactly is alerting and where. If we look above at our monitor setup we can see in the top portion of the screen
  our monitor name and the message that will be sent to the email specified. These can be as important as the alert itself as a poorly named monitor or poorly written message can be 
  as confusing as no monitoring at all.

## Monitor alert Email

- Now that our monitor is setup we will begin receiving email alerts whenever our sample is above 0.90. We can see one below.

![Alert_Email](screenshots/Alert_Email.png)


## Bonus: Set scheduled downtime for monitor, make sure Email is notified

- Finally, we would not want to be bothered at 2am for an alert that is going off because of a scheduled maintenance. We can schedule downtime for our alerts when we know there is maintenance
  happening. We can schedule downtime by going to **Monitors -> Manage Downtime** and setting the length of time for a particlar monitor. This can be set to email a specified person or team like below.

![Downtime_Email](screenshots/Downtime_Email.png)

# Conclusion

- Having worked with datadog before I knew it was a great product, I'm glad to see that it has continued to add features and grow within the monitoring scene. Also, because I've used datadog before
  this was not extremely technically challenging but I love that this is the way the company interviews a candidate. I believe it really lets you as a potential employee and datadog as a company 
  see how a person works and figures out problems hands on.import json
import requests
from evaluations import system
from evaluations.decorator import Decorators

list_of_containers = []
for container in system.docker_containers_get('mist'):
    list_of_containers.append(container.short_id)


class Evaluation:
    def __init__(self, mist_aws_access_key, mist_aws_secret_key, mist_token, mist_home_directory,
                 mist_name='serh@zhaw.ch', mist_pass='123987', mist_endpoint='http://127.0.0.1',
                 mist_github_source='https://github.com/mistio/mist.io/releases/download/v3.0.0/docker-compose.yml',
                 mist_region='us-east-2', mist_provider_name='Test'):
        self.mist_home_directory = mist_home_directory
        self.mist_token = mist_token
        self.mist_aws_access_key = mist_aws_access_key
        self.mist_aws_secret_key = mist_aws_secret_key
        self.mist_name = mist_name
        self.mist_pass = mist_pass
        self.mist_github_source = mist_github_source
        self.mist_endpoint = mist_endpoint
        self.mist_region = mist_region
        self.mist_provider_name = mist_provider_name
        self.mist_headers = {'Authorization': self.mist_token}
        self.mist_provider_id = None

    @Decorators.tagging('*:System:Download')
    @Decorators.timing(output=True)
    def download_sources(self):
        system.wget_download(self.mist_home_directory, self.mist_github_source)
        system.docker_compose_command('pull', self.mist_home_directory)
        size, volume = system.docker_get_images_sizes('mist')
        return {'size': size, 'volume': volume}

    @Decorators.tagging('*:System:Start')
    @Decorators.timing()
    def start_docker_compose(self):
        system.docker_compose_command('up', self.mist_home_directory, method='Popen')
        system.wait_service(self.mist_endpoint)
        system.docker_create_user(self.mist_home_directory,
                                  login=self.mist_name, password=self.mist_pass)

    @Decorators.tagging('*:System:Stop')
    @Decorators.timing()
    def down_docker_compose(self):
        system.docker_compose_command('down', self.mist_home_directory)

    @Decorators.tagging('*:System:Remove')
    @Decorators.timing()
    def delete_sources(self):
        system.docker_delete_images()
        system.delete_dir(self.mist_home_directory)

    @Decorators.tagging('AWS:Provider:Create')
    @Decorators.docker_consumption(list_of_containers)
    @Decorators.timing()
    def create_amazon_provider(self, aws_secret_key=None, aws_access_key=None, region=None, name=None):
        if not name:
            name = self.mist_provider_name
        if not aws_access_key:
            aws_access_key = self.mist_aws_access_key
        if not aws_secret_key:
            aws_secret_key = self.mist_aws_secret_key
        if not region:
            region = self.mist_region
        output = requests.post(self.mist_endpoint + '/api/v1/clouds', headers=self.mist_headers, data=json.dumps(
            {'title': name,
             'provider': 'ec2',
             'region': region,
             'api_key': aws_access_key,
             'api_secret': aws_secret_key}
        ))
        self.mist_provider_id = json.loads(output.content)['id']

    @Decorators.tagging('*:Provider:List')
    @Decorators.docker_consumption(list_of_containers)
    @Decorators.timing()
    def list_of_providers(self):
        requests.get(self.mist_endpoint + '/api/v1/clouds', headers=self.mist_headers)

    @Decorators.tagging('*:Provider:Delete')
    @Decorators.docker_consumption(list_of_containers)
    @Decorators.timing()
    def delete_amazon_provider(self, provider_id=None):
        if not provider_id:
            if self.mist_provider_id:
                provider_id = self.mist_provider_id
            else:
                raise Exception("Provide provider id")
        requests.delete(self.mist_endpoint + '/api/v1/clouds' + '/' + provider_id, headers=self.mist_headers)
SECRET=Privari#12345
PORT=5001


READ_REPLICA_DATABASE_HOST=
DATABASE_HOST=
DATABASE_USERNAME=
DATABASE_PASSWORD=
DATABASE_NAME=
DATABASE_PORT=3306


REDIS_HOST=127.0.0.1
REDIS_PORT=6379

# CANDIDATE_ASSETS_BUCKET_NAME=candidate-assets
# AWS_REGION=ap-south-1
# AWS_ACCESS_KEY=
# AWS_SECRET_KEY=

# Sendgrid Key
SENDGRID_ACCOUNT_ONE_API_KEY=

# MONGO_DB_PORT=27017





# MONGO DB CONNECTION CONFIG
MONGO_DB_HOST=
MONGO_DB_USER=
MONGO_DB_PASSWORD=
MONGO_DB_CERTIFICATE=
MONGO_DB_DATABASE=

# can be one of LOCAL, MONGODB-AWS-EC2, MONGODB-AWS-CODE, TLS-CA, USER-AUTH
MONGO_DB_AUTH_STRATEGY=USER-AUTH
# [Aquarium Bait](https://github.com/adobe/aquarium-bait)

This project is an essential part of the [Aquarium](https://github.com/adobe/aquarium-fish/wiki/Aquarium)
system - it's purpose is creating the enterprise oriented, consistent and reliable environment
images for further using as an env source for the [Aquarium Fish](https://github.com/adobe/aquarium-fish/)
node.

You can use Aquarium Bait as a general image building system without the rest of the [Aquarium](https://github.com/adobe/aquarium-fish/wiki/Aquarium)
stack components and run the images manually (it's described how below for each driver).

It's not only useful for the CI where we need to ensure the environment is exactly the one we
needed, but also for general purpose to run the applications of your organization in the safe and
completely controlled conditions according to the specifications.

In a nutshell, it's a shell wrapper around Packer and Ansible, which organizes the image building
in a certain manner in order to achieve specific enterprise-required goals, especially
supportability. By replacing the nasty configuration management with complicated logic with layered
versioned image building and management - the configuration scripts have become just a list of
steps and doesn't require anymore to have a degree to figure out what's going on. From layer to
layer Bait allows you to record and reuse state, branching the images from Base OS to infinity
variations and quickly figure out which ones requires rebuilding saving your time (and network
bandwidth) to distribute the images.

## Goals

* Easy track of the changes in the images
* Complete automation of the images builds
* Organize way to build and store the images for various environments
* Build from scratch if possible to completely control the environment
* Strict isolation to not allow the OS/Apps on the image to interact with network
* Use various image size optimizations including linking and reusing
* Proper versioning of the images
* Ability to build locally with no access to remote services (if have required cached artifacts)
* Replace complicated `change management` with layered versioned image building & management

## Requirements

* Host:
   * MacOS
   * Linux
* Python 3 + venv
* [Packer v1.7.9](https://www.packer.io/downloads)
* Make sure you have at least 200GB of disk space to build an image.
* For MacOS images:
   * The installer timeouts are tuned for MacBook Pro '19 2.3 GHz 8-Core Intel Core i9 (connected
   to power adapter).
   * Can only be running on MacOS host (Apple license restrictions)
   * VMWare Fusion 12.2.0

## Image structure

The image forms a tree and reuses the parent disks to optimize the storage and the build process,
so the leaves of the trees will require the parents to be in place.

* **macos1015** - the base OS with low-level configs
   * macos1015-**xcode122** - the Xcode tools of a specific version
      * macos1015-xcode122-**ci** - jenkins user and autorunning jnlp agent

In this example the VMX packer spec is using `source_path` (`specs/vmx/macos1015/xcode122/ci.yml`)
to build the `ci` image on top of the previously created `xcode122` image which was created as a
child of the `macos1015` image. That's why `build_image.sh` wrapper is executing the directory tree
stages sequentially - to make sure we already built the previous stage image to use it in the upper
levels of the images.

## Static checks

Even simple image-management Ansible specifications needs to be linted. You can run the verification
by `./check_style.sh` script and it will execute a number of tools to validate the playbooks.

## How to create from scratch

### 1. Create ISO of MacOS installer

1. Download MacOS installer from app store:
  * [Catalina 10.15](https://itunes.apple.com/us/app/macos-catalina/id1466841314)
  * [BigSur 11](https://itunes.apple.com/us/app/macos-big-sur/id1526878132)
  * [Monterey 12](https://itunes.apple.com/by/app/macos-monterey/id1576738294)
  * [Ventura 13](https://apps.apple.com/us/app/macos-ventura/id1638787999)
2. Create dmg:
   ```
   $ hdiutil create -o /tmp/macos-installer -size 8500m -volname macosx-installer -layout SPUD -fs HFS+J
   ```
3. Mount the dmg:
   ```
   $ hdiutil attach /tmp/macos-installer.dmg -noverify -mountpoint /Volumes/macos-installer
   ```
4. Unpack the installer:
   ```
   $ sudo /Applications/Install\ macOS\ Catalina.app/Contents/Resources/createinstallmedia --volume /Volumes/macos-installer --nointeraction
   ```
5. Umount the dmg:
   ```
   $ hdiutil detach /Volumes/Install\ macOS*
   ```
   * If here is an error - check the usage by and detach/kill the apps from the inside:
      ```
      sudo lsof | grep '/Volumes/Install macOS'
      ```
6. Convert the dmg to cdr and iso:
   ```
   $ hdiutil convert /tmp/macos-installer.dmg -format UDTO -o /tmp/macos-installer.cdr
   $ mv /tmp/macos-installer.cdr /tmp/macos-installer.iso
   $ rm -f /tmp/macos-installer.dmg
   ```

### 2. Put the dependencies in place

#### ISO images

Packer will use iso images from `init/iso` directory. The iso should be named just like the packer
yml file, but with the iso extension.

* Build or Download `MacOS-Catalina-10.15.7-210125.190800.iso`
* Place it as `init/iso/MacOS-Catalina-10.15.7.iso`

#### Ansible files

Roles can download files from artifact storage, but in case it's not an option (you're remote and
can't use VPN for some reason) - you can place the files locally: Ansible playbooks use a number of
binary packages you can find on artifact-storage, check the [playbooks/files/README.md](playbooks/files/README.md)
for additional information.

Another file will help you to override the URL's to your own org storage - `override.yml`, it's
placed in the repo root directory and readed by the `run_ansible.sh` script. It can contain the
variables from the roles. For example:
```yaml
---
# Allow to use local env creds during downloading of the artifacts from storage
download_headers:
  X-JFrog-Art-Api: "{{ lookup('env', 'ARTIFACTORY_API_KEY') }}"

vmtools_vmware_lin_download_url: https://my-own-artifact-storage/archive-ubuntu-remote/pool/universe/o/open-vm-tools/open-vm-tools_11.3.0-2ubuntu0~ubuntu20.04.2_amd64.deb

xcode_version_133_mac_download_url: https://my-own-artifact-storage/aquarium/files/mac/Xcode_13.3.xip
xcode_version_133_mac_download_sum: sha256:dc5fd115b0e122427e2a82b3fbd715c3aee49ef76a64c4d1c59a787ce17a611b
xcode_version_133_cmd_mac_download_url: https://my-own-artifact-storage/aquarium/files/mac/Command_Line_Tools_for_Xcode_13.3.dmg
xcode_version_133_cmd_mac_download_sum: sha256:7eff583b5ce266cde5c1c8858e779fcb76510ec1af3d9d5408c9f864111005c3
...
```

You can grep all the variables that have `_url: http` and put them in the file and override
one-by-one. Some specs are overriding the main download variable by the template (as in example for
xcode) and you can specify the version for each one to build the different images properly.

Also there's pip index which could be overridden by placing pip.conf in the repo directory:
```
[global]
index-url = https://my-own-artifact-storage/api/pypi/pypi-remote/simple
```

**WARNING**: Make sure you using https as your artifact transport, otherwise it could interfere
with the local proxy and won't allow you to download the artifacts from the role.

For the other overrides look 

### 3. Run build

Now when all the required things are ready - you can run the image builder:
```
$ ./build_image.sh  [...]
```

This script will automatically create the not-existing images in out directory. You can specify the
packer yml files as arguments to build the specific images. Also, you can put `DEBUG=1` env var to
tell the builder to ask in case of any issue happening during the build, but debug mode created
images are not supposed to be uploaded to the artifact storage - just for debugging.


**NOTICE:** The Aquarium Bait supports local image building with corporate VPN connected through a
special local proxy which ignores the routing rules and always uses the local interfaces. For
additional info please look into [./build_image.sh](build_image.sh), [proxy_local.py](scripts/proxy_local.py)
and [./specs/vmx/.yml](specs/vmx/) specifications.

**NOTICE:** By default Aquarium Bait designed to build the images in sandbox with access from VM
only to the host. But in case it's strictly necessary you can run the http proxy during ansible
execution on the host system by setting the env variable in packer spec for ansible provisioner:

* Remote proxy script is started on the host machine in `build_image.sh` and needs a way for remote
to be able to connect to the host machine (vmx, docker...). Could be used if Socks5 is not working.
   ```yaml
   provisioners:
     - type: ansible
       ...
       extra_arguments:
         - -e
         - bait_proxy_url=http://{{ build `PackerHTTPIP`}}:{{ user `remote_proxy_port` }}
   ```
* In case your remote can't reach the host machine easily (cloud) - you can use built-in-packer ssh
tunnel like that (yep works just with SSH, so keep the base windows images simple):
   ```yaml
   variables:
     remote_proxy_port: '1080'
     ...
   builders:
     ...
     # Tunnel will transfer traffic through ssh to the http proxy for ansible
     ssh_remote_tunnels:
       - "{{ user `remote_proxy_port` }}:127.0.0.1:{{ user `remote_proxy_port` }}"

   provisioners:
     - type: ansible
       ...
       extra_arguments:
         - -e
         - bait_proxy_url=http://127.0.0.1:{{ user `remote_proxy_port` }}
   ```

The ansible variables to access this proxy passed as `bait_proxy_url` which is `http://host:port`
and could be used in playbooks/roles like:
```yaml
- name: APT task that uses proxy to escape the sandbox
  environment:
    http_proxy: "{{ bait_proxy_url | default(omit) }}"
    https_proxy: "{{ bait_proxy_url | default(omit) }}"
  apt:
    name: vim
    update_cache: true
```

**NOTICE:** During the VM build the script records the VM screen through VNC and places it into
`./records/.mp4` - so you can always check what's happened if your build accedentally
crashed during packer execution. For additional info look into [./build_image.sh](build_image.sh)
and [./scripts/vncrecord.py](scripts/vncrecord.py).

**NOTICE:** If you want to override spec file in runtime `yaml2json.py` script, which is executed
before the packer run, can override the values of the spec. For example, you want to put
`skip_create_ami` in amazon-ebs builder no matter what, you can export env variable like that:

```sh
$ export BAIT_SPEC_APPLY='{"builders":[{"skip_create_ami":true}]}'
```

... before running the `build_image.sh` and it will be added to the json spec. There are 2 more env
vars `BAIT_SPEC_CHANGE` and `BAIT_SPEC_DELETE` in case you want just to change the value of already
existing item (and do not add it if it's not here) and if you want to delete the part of the tree.
Those could be really useful to test the changes for example.

**NOTICE:** Parallel building is possible (really not recommended to build base images of VMX due
to the tight VNC boot scripts timings) and have one problem with artifacts download, described in
[#34](https://github.com/adobe/aquarium-bait/issues/34). One solution is use pre-cached artifacts,
which is the most simple way - and you can either to use one-threaded build first or use special
[./playbooks/download_file_cache.yml](playbooks/download_file_cache.yml) playbook and run it as:

```sh
$ ./scripts/run_ansible.sh playbooks/download_file_cache.yml
```

This playbook is checking the specially formed `_download_url` / `_download_sum` variables to find
what to download, so if your role vars are using this schema (properly described in the playbook),
you can benifit from automatic download flow and build your cache easy.

### 4. Run pack of the images

Now you can run script to pack all the generated images into tight tar.xz archives:
```
$ ./pack_image.sh [out/type/image_dir] [...]
```

As with the build you can pack specific images by defining the out directories to pack.

Storing the images in directory allows us to preserve the additional metadata, checksums and build
logs for further investigations.

### 5. Upload the packed images

The last step is to upload the image to artifact storage to use it across the organization:
```
$ ./upload_image.sh  [...]
```

It will output the upload progress and info about the uploaded image when it will be completed.

We're using TAR for it's ability to pack multiple files, preserve the important metadata and
XZ for it's best compacting abilities to reduce the required bandwidth.

## Supported drivers

Aquarium Bait supports a number of drivers and can be relatively easily extended with the ones you
need. When you are navigating to `specs` directory you see the driver directory (like "vmx",
"docker", "aws"), it's just for convenience and output images separation (this dir will be used in
`out` to place the images). This way the images for drivers can be built with no conflicts.

### Amazon Web Services (AWS)

First cloud provider integration to the images build system. It needs some knowledge of the AWS
and pre-setup of the AWS project. Cloud driver also quite different from the local drivers like vmx
or docker - it will still use the specs in packer yml format, but will store the image directly in
AWS project.

**NOTE:** Since the AMI's are updated by the OS providers - it's hard to bind to specific version
in the base image (it could be removed or not supported by the AWS platform itself anymore). So for
clouds we're using the latest AMI available on the build occurance and then this built base image
could be used in the childs indefinitely.

**NOTE:** For macos image building on AWS you need to have an allocated dedicated server, as well
as plenty of money upfront (24h minimum usage) and being ready to have this server be in pending
state after the instance termination.

#### Usage

##### 1. Setup the AWS project

Your AWS project should contain the created VPC with subnets, which are tagged as `Class:
image-build`. You need to create security groups which will allow the ssh and winrm connections
tagged as `Class:image-build-ssh` and `Class:image-build-winrm`. You will need to create the IAM
role with admin access to AWS EC2 permissions and receive the `key_id` and `secret_key` for it.
Also to build child images you can use "self" or obtain the Account ID to lookup the same account
for the images - it's easy to find in AWS console top right user account menu.

The enterprise networks could be quite restricted - so please make sure you have a connection to
the instances with those security groups from internal network (prefferable) or from public
internet. To ensure you can run some instances and try to connect to them. Sometimes you will need
to talk to your network and security teams to establish a proper connection to your AWS project.

##### 2. Setup the packer env

Specs are using 2 important environment variables, so prior to running the build script you need to
set them. The values you've got in the first step during creating the IAM role:
```
$ export AWS_KEY_ID=
$ export AWS_SECRET_KEY=
$ export AWS_ACCOUNT_ID=self
```

##### 3. Run the image build

Now you're ready to run the actual build by following the common steps above. By default the packer
will generate the unique ssh key (and will use autogenerated password for winrm access) so it's
quite safe images out of the box.

### Docker

It's a good driver to run Linux envs without the VM overhead on Linux hosts - gives enough
flexibility and starts relatively quickly. The images are stored using `docker save` command and
after that in the postprocess the parent layers are removed from the child image (otherwise it
contains them all).

The Aquarium Bait is not using the docker registry to control the images and uses regular files to
store them. That could be seen as bad choice until you need to support a number of different image
receiving transports. With files it's relatively easy to control the access, store the additional
metadata and have cache in the huge organization. But it's still possible to use registry to get
the base images.

**NOTICE:** Isolation during the build is running as special container - and it's allow to only
reach `host.docker.internal`. You don't have to use it but it's really improving your control over
your containers. The proxy container is sitting in `./init/docker/bait_proxy` dir, auto running by
`build_image.sh` and could be adjusted for your needs by using the build args through env variable
`bait_proxy_build_opts`. Example usage of isolation proxy in packer spec:
```
variables:
  # Will be set to random free one by build_image.sh
  remote_proxy_port: '1080'

builders:
  - type: docker
    ...
    run_command:
      - -dit
      - --net=container:bait_proxy  # Host only access
      - --entrypoint=/bin/sh
      - --
      - "{{.Image}}"

provisioners:
  - type: ansible
    ...
    extra_arguments:
      - -e
      - bait_proxy_url=http://host.docker.internal:{{ user `remote_proxy_port` }}

  - type: shell
    env:
      http_proxy: http://host.docker.internal:{{ user `remote_proxy_port` }}
      https_proxy: http://host.docker.internal:{{ user `remote_proxy_port` }}
    inline:
      - ...
```

#### Using manually

##### 1. Download and unpack

You need to download and unpack all the images in the same directory - as the result you will see
a number of directories which contains the layers from base OS to the target image.

##### 2. Load the images

The docker images can't be used directly from disk and need to be loaded to the docker in order
from base os to the target image (order is important, because the images contains only their own
layers and will rely on registry to have the parent ones):
```
$ docker load -i out/docker/ubuntu2004-VERSION/ubuntu2004.tar
$ docker load -i out/docker/ubuntu2004-build-VERSION/ubuntu2004-build.tar
$ docker load -i out/docker/ubuntu2004-build-ci-VERSION/ubuntu2004-build-ci.tar
```

And now you will be able to see that the `docker images` contains those versioned images. All of
them will have `aquarium/` prefix so you quickly can distinguish the loaded images from your own.

##### 3. Run the container

Docker will take care about the images and will not modify them, but will allow you to run the
container like that:
```
$ docker run --rm -it aquarium/ubuntu2004-build-ci:VERSION
```

Of course you can mount volumes, limit the amount of resources for each container and so on - just
check the docker capabilities.

### VMWare VMX

Aquarium Bait can work with VMware Fusion (on MacOS) and with VMware Workstation (on Linux). Player
is not tested, but in theory could work too with some additional tuning. 30 day trial period is
offered when you run the vmware gui application.

#### Using manually

##### 1. Download and unpack

You need to download and unpack all the images in the same directory - as the result you will see
a number of directories which contains the layers from base OS to the target image.

##### 2. Clone worker VM

Please ensure the image is never running as VM, it will make much simpler to update the images and
leave the images unmodified for quick recreating of the worker VM. All the images contains the
`original` snapshot (which is used for proper cloning) so don't forget to use it.
```
$ vmrun clone images/macos1106-xcode131-ci-220112.204142_cb486d76/macos1106-xcode131-ci.vmx worker/worker.vmx linked -snapshot original
```

##### 3. Change worker `.vmx` file:

When you just manually cloned the new VM to run it on the target system - you need to use the most
of the resources you have - so don't forget this part otherwise you will struggle of the performance
penalty.

During actual run on the target system you can change CPU & MEM values to the required values, but
please leave some for the HOST system.

* CPU (if you have more than one CPU socket - use `CPUS_PER_SOCKET = TOTAL_CPUS / `
to preserve the NUMA config). Tests showed that it's better to leave 2 vCPU for the host system.
   ```
   # CPU
   numvcpus = ""
   cpuid.coresPerSocket = ""
   ```
* RAM. Leave ~1/3 of the total RAM to the host system for VM disk caching.
   ```
   # Mem
   memsize = ""
   ```

##### 4. Run the worker VM

I always recommend to run the VM's headless and access them via CLI/SSH as much as possible. Only
the exceptional reasons where no CLI/SSH can help it's allowed to use GUI to debug the VM. So we
running the VM as nogui - and you always have a way to run the VMWare UI interface for GUI access.

```
$ vmrun start worker/worker.vmx nogui
```

## Overlay repository

For sure your organization don't want to share the specific overrides, specs, playbooks and roles.
You can actually make this happen by creating the new repository with the next layout:

* bait/
   > git submodule with aquarium-bait repo content, you still be able to use `bait/specs` to build images
* out/
   > Symlink to `bait/out`. VMX don't like symlinks, so to store images `bait/out` dir is always used

* specs/
   > dir with specs of your images - they are able to use `bait/specs` images as child/parents
* playbooks/
   > dir with your playbooks, roles & files cache
   * files
      > symlink to `../bait/playbooks/files`

* .ansible-lint
   > Symlink to `bait/.ansible-lint` or your own lint rules for ansible playbooks
* .gitignore
   > file copy of `bait/.gitignore_prop`
* .yamllint.yml
   > Symlink to `bait/.yamllint.yml` or your own style rules for yaml files
* ansible.cfg
   > Symlink to `bait/ansible.cfg` or your own overrides for ansible configs
* override.yml
   > your overrides for url's and other ansible variables. Check **Ansible files** section details

* build_image.sh
   > shell script to run `bait/build_image.sh`, symlink will not work here
   ```sh
   #!/bin/sh -e

   # Check if the spec exists in the directory, otherwise will run bait version of it
   spec_file="$@"
   [ -e "$@" ] || spec_file="bait/$@"

   # Set the bait_proxy image variables to properly build in the restricted environment
   # Example is here, please replace if necessary and uncomment
   #bait_proxy_build_opts="--build-arg BASE_IMAGE=docker-hub-remote.example.com/ubuntu:20.04"
   #bait_proxy_build_opts="--build-arg APT_URL=https://artifact-storage/archive-ubuntu-remote/ $bait_proxy_build_opts"
   #bait_proxy_build_opts="--build-arg APT_SEC_URL=https://artifact-storage/security-ubuntu-remote/ $bait_proxy_build_opts"
   #export bait_proxy_build_opts

   ./bait/build_image.sh "$@"
   ```
* check_style.sh
   > shell script to run `bait/check_style.sh`, symlink will not work here
   ```sh
   #!/bin/sh -e
   ./bait/check_style.sh "$@"
   ```
* pack_image.sh
   > symlink to `bait/pack_image.sh`
* upload_image.sh
   > symlink to `bait/upload_image.sh`
* upload_iso.sh
   > symlink to `bait/upload_iso.sh`

Now, when the repository is prepared you can use it just as bait one:
```bash
$ ./build_image.sh specs/type/path/to/image.yml
$ ./build_image.sh bait/specs/type/path/to/another/image.yml
$ ./pack_image.sh out/image-version
$ ./upload_image.sh user:token out/image-version.tar.xz
```

## Advices on testing

### SSH: connecting through proxy

In order to connect to the local VM with VPN connection it's necessary to use the local proxy
started up by the `build_image.sh` script.

1. Run image build in debug mode and wait until the error happened:
   ```
   $ DEBUG=1 ./build_image.sh specs/.yml
   ```
2. Find in console line `PROXY: Started Aquarium Bait noroute proxy on` which will contain proxy
host and port.
3. Find in console line `PROXY: Connected to:` which will show you the VM IP address
4. Run the next SSH command to use `nc` as the SSH transport for socks5 proxy with located info:
   ```
   $ ssh -o ProxyCommand='nc -X 5 -x 127.0.0.1: %h %p' packer@
   ```
5. Type the default password `packer` and you good to go!

### VMWare Fusion visual debugging

Sometimes it's useful to get visual representation of the running VM and there is 2 ways to do that:

1. Modify the packer spec yml file to comment the `headless` option - but don't forget to change it
back after that. This way packer will run the build process with showing the VM GUI.

2. Just open the VMWare Fusion UI and it will show the currently active VM GUI. It's dangerous,
because if you will leave the VMWare Fusion UI - it will not remove the lock files in the VM dir,
so make sure when you complete the debugging you're properly close any signs of VMWare Fusion UI.

### Ansible: run playbook

To test the playbook you will just need a VM and inventory. For example inventory for windows
looks like that and automatically generated by packer, so the inventory path can be copied from its
output "Executing Ansible:"
```
$ cat inv.ini
default ansible_host=172.16.1.80 ansible_connection=winrm ansible_winrm_transport=basic ansible_shell_type=powershell ansible_user=packer ansible_port=5985
```

```
$ cp /var/folders/dl/lb2z806x47q7dwq_lpwcmlzc0000gq/T/packer-provisioner-ansible355942774 inv.ini
$ ./scipts/run_ansible.sh -vvv -e ansible_password=packer -i inv.ini playbooks/base_image.yml
```

### Ansible: execute module

When you need to test the module behavior on the running packer VM - just copy the inventory (it is
created when packer runs ansible) file and use it to connect to the required VM.

```
$ cp /var/folders/dl/lb2z806x47q7dwq_lpwcmlzc0000gq/T/packer-provisioner-ansible355942774 inv.ini
$ . ./.venv/bin/activate
(.venv) $ no_proxy="*" ansible default -e ansible_password=packer -i inv.ini -m shell -a "echo lol"
(.venv) $ no_proxy="*" ansible default -e ansible_password=packer -i inv.ini -m win_shell -a "dir C:\\tmp\\"
(.venv) $ no_proxy="*" ansible default -e ansible_password=packer -i inv.ini -m win_copy -a "src=$(pwd)/playbooks/files/win/PSTools-v2.48.zip dest=C:\\tmp\\PSTools-v2.48.zip"
```

### Ansible: template validation

You just need to activate the python venv and run ansible in it. Venv is created when packer
executes `./scripts/run_ansible.sh` script during preparation of any image.

```
$ . ./.venv/bin/activate
(.venv) $ ansible all -i localhost, -m debug -a "msg={{ tst_var | regex_replace('\r', '') }}" --extra-vars '{"tst_var":"test\r\n\r\n"}'
```

### CI PR verification routines

In case you want to check which changes in roles/playbooks affected the packer specs you can use
`list_affected.sh` script which will check your current git branch, collect the changes against the
main branch and will show the roles/playbooks and specs you probably want to test in dry run.
---
rewrite: true
title: Amazon Personalize Destination
id: 5c7f0c9879726100019cc56b
---
Segment makes it easy to send your data to Amazon Personalize (and lots of other destinations). Once you collect your data using Segment's [open source libraries](/docs/connections/sources/catalog/), Segment translates and routes your data to Amazon Personalize in the format it can use. [Amazon Personalize](https://aws.amazon.com/personalize/) is a machine learning service that makes it easy for developers to create individualized recommendations for customers using their applications. AWS Personalize enables:

  - Media companies to provide recommended content for viewers based on their viewing history
  - Retailers to provide personalized product recommendations based on shopping behavior
  - Any company to provide personalized search results and targeted marketing promotions based on the latest machine-learning capabilities developed at Amazon

Developing the machine-learning capabilities necessary to produce these recommendation systems has been beyond the reach of most organizations today due to the complexity of developing machine learning functionality. Amazon Personalize allows developers with no prior machine learning experience to build sophisticated personalization capabilities into their applications, using machine learning technology perfected from years of use on Amazon.com.

## Getting Started



These are the pre-requisites you need before getting started:

1. Segment data flowing into an S3 destination, a Snowflake warehouse, or Amazon Redshift warehouse.
2. You have the ability to create AWS Glue jobs (only required if using S3 to [train your model](#train-your-model))
3. You have the ability to deploy Lambda functions in Amazon Web Services
4. You have access to AWS Personalize

If you don't have S3, Redshift warehouse, or Snowflake warehouse configured, you can read more about setting up [S3](/docs/connections/storage/catalog/aws-s3/), [Redshift](/docs/connections/storage/catalog/redshift/), and [Snowflake](/docs/connections/storage/catalog/snowflake/).

***If you're a Segment business tier customer, contact your Success contact to initiate a replay to S3 or your Warehouse.***

There are three main parts to using Amazon Personalize with Segment:

1. [Train your model](/docs/connections/destinations/catalog/amazon-personalize/#train-your-model) on historical data in S3 or a Warehouse.
2. [Create a Personalize Dataset Group](/docs/connections/destinations/catalog/amazon-personalize/#create-personalize-dataset-group-solution-and-campaign) and Campaign
3. [Connect Recommendations](/docs/connections/destinations/catalog/amazon-personalize/#getting-recommendations-and-live-event-updates) and Live Event Updates to your Campaign and Segment

## Train Your Model

**S3 Bucket Permissions**

Whatever method you choose to train your model will result in placing a CSV into an S3 bucket. Be sure to update the policies of the bucket to include [these permissions](https://docs.aws.amazon.com/personalize/latest/dg/data-prep-upload-s3.html){:target="_blank"} to allow Personalize to access your CSV:

```json
{
    "Version": "2012-10-17",
    "Id": "PersonalizeS3BucketAccessPolicy",
    "Statement": [
        {
            "Sid": "PersonalizeS3BucketAccessPolicy",
            "Effect": "Allow",
            "Principal": {
                "Service": "personalize.amazonaws.com"
            },
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::bucket-name",
                "arn:aws:s3:::bucket-name/*"
            ]
        }
    ]
}
```

**Define a Schema**

To train a Personalize model, you'll need to define the event schema for the event names and properties that your model uses as features. For the examples below, Segment is using the following Personalize Dataset schema to train the model. You'll want to modify this to suit your use cases.  You can learn more about [Personalize schemas](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html){:target="_blank"}.

```json
{
    "type": "record",
    "name": "Interactions",
    "namespace": "com.amazonaws.personalize.schema",
    "fields": [
        {
            "name": "USER_ID",
            "type": "string"
        },
        {
            "name": "ITEM_ID",
            "type": "string"
        },
        {
            "name": "EVENT_TYPE",
            "type": "string"
        },
        {
            "name": "TIMESTAMP",
            "type": "long"
        }
    ],
    "version": "1.0"
}
```

The examples show how multiple Segment `track` events are mapped into this schema and used to train a Personalize solution.

### From Redshift

If you already use Redshift, training your model on the data in your warehouse is the simplest way to get up and running.

**Unload Data from Redshift to S3**

  Assuming you have a Personalize schema like that described [below](#create-personalize-dataset-group), you can use the following query to pull out all the date from your `Order Completed`, `Product Added`, and `Product Viewed` events.

```sql
unload ('
  select
    user_id as USER_ID,
    products_sku as ITEM_ID,
    event as EVENT_TYPE,
    date_part(epoch,"timestamp") as TIMESTAMP
  from prod.order_completed
  UNION
  select
    user_id as USER_ID,
    products_sku as ITEM_ID,
    event as EVENT_TYPE,
    date_part(epoch,"timestamp") as TIMESTAMP
  from prod.product_added
  UNION
  select
    user_id as USER_ID,
    products_sku as ITEM_ID,
    event as EVENT_TYPE,
    date_part(epoch,"timestamp") as TIMESTAMP
  from prod.product_viewed
')
to
's3://mybucket/my_folder'
credentials 'aws_access_key_id=AWS_ACCESS_KEY_ID;aws_secret_access_key=AWS_SECRET_ACCESS_KEY;token=AWS_SESSION_TOKEN'
HEADER
REGION AS ''
DELIMITER AS ','
PARALLEL OFF;
```

**Note:** Use `date_part(epoch,"timestamp") as TIMESTAMP` because Personalize requires timestamps to be specified in UNIX/epoch time.

**Verify the Output file**
Browse to the S3 service page in the AWS console and navigate to the bucket path specified in the `unload` command. You should see the output file.


### From Snowflake

There are a few ways to load a CSV into S3 from your [Snowflake](https://docs.snowflake.net/manuals/user-guide/data-unload-s3.html){:target="_blank"} warehouse. This example shows loading the data directly into an S3 bucket.

Assuming you have a Personalize schema like that described [below](/docs/connections/destinations/catalog/amazon-personalize/#create-personalize-dataset-group), you can use the following query to pull out all the date from your `Order Completed`, `Product Added`, and `Product Viewed` events.

**Unload Data from Snowflake to S3**

```sql
    copy into
    s3://mybucket/my_folder/my_file.csv
    from
    (
      select
      user_id as USER_ID,
      products_sku as ITEM_ID,
      event as EVENT_TYPE,
      date_part(epoch,"timestamp") as TIMESTAMP
      from prod.order_completed
      UNION
      select
      user_id as USER_ID,
      products_sku as ITEM_ID,
      event as EVENT_TYPE,
      date_part(epoch,"timestamp") as TIMESTAMP
      from prod.product_added
      UNION
      select
      user_id as USER_ID,
      products_sku as ITEM_ID,
      event as EVENT_TYPE,
      date_part(epoch,"timestamp") as TIMESTAMP
      from prod.product_viewed
    )
    file_format=(type=csv)
    single = true -- Personlize requires a single CSV file
    credentials = (aws_key_id='xxxx' aws_secret_key='xxxxx' aws_token='xxxxxx');
```

This example uses temporary S3 credentials, which are generated by AWS STS and expire after a specific period of time. Temporary credentials are [recommended](https://docs.snowflake.net/manuals/user-guide/data-unload-s3.html#unloading-data-directly-into-an-s3-bucket){:target="_blank"} to protect access to the bucket.

**Verify the Output file**
Go to the S3 service page in the AWS console and navigate to the bucket path specified in the `unload` command. You should see the output file.


### From S3

**Historical Data Preparation**

Segment's S3 destination contains a copy of all of the source data you configured to go to S3.  In your S3 bucket there's a folder called `/segment-logs`.  Under this folder is another folder for each source of data you connected to your Segment S3 destination.

Note that this step is not required unless you plan to do batch data extraction from S3.

Your Glue ETL job will need to crawl each source folder to extract the backup data that forms your training set.  Analysis of this data set is beyond the scope of this document.  It is strongly recommended you familiarize yourself with the types of events that can be sent through Segment.  Segment's event structure is described in detail [here](/docs/connections/sources/catalog/libraries/server/http/).

The following examples show how to configure an AWS Glue job to convert Segment historical data into the Apache Avro format that Personalize wants to consume for training data sets.

**Create AWS Glue ETL Job**

To create an AWS Glue ETL Job:
1. Create a new AWS service IAM role using the following execution policies. These policies give your Glue job the ability to write to your S3 bucket:
  * Policy 1:

      ```json
      {
          "Version": "2012-10-17",
          "Statement": [
              {
                  "Effect": "Allow",
                  "Action": [
                      "glue:*",
                      "s3:GetBucketLocation",
                      "s3:ListBucket",
                      "s3:ListAllMyBuckets",
                      "s3:GetBucketAcl",
                      "ec2:DescribeVpcEndpoints",
                      "ec2:DescribeRouteTables",
                      "ec2:CreateNetworkInterface",
                      "ec2:DeleteNetworkInterface",
                      "ec2:DescribeNetworkInterfaces",
                      "ec2:DescribeSecurityGroups",
                      "ec2:DescribeSubnets",
                      "ec2:DescribeVpcAttribute",
                      "iam:ListRolePolicies",
                      "iam:GetRole",
                      "iam:GetRolePolicy",
                      "cloudwatch:PutMetricData"
                  ],
                  "Resource": [
                      "*"
                  ]
              },
              {
                  "Effect": "Allow",
                  "Action": [
                      "s3:CreateBucket"
                  ],
                  "Resource": [
                      "arn:aws:s3:::aws-glue-*"
                  ]
              },
              {
                  "Effect": "Allow",
                  "Action": [
                      "s3:GetObject",
                      "s3:PutObject",
                      "s3:DeleteObject"
                  ],
                  "Resource": [
                      "arn:aws:s3:::aws-glue-*/*",
                      "arn:aws:s3:::*/*aws-glue-*/*"
                  ]
              },
              {
                  "Effect": "Allow",
                  "Action": [
                      "s3:GetObject"
                  ],
                  "Resource": [
                      "arn:aws:s3:::crawler-public*",
                      "arn:aws:s3:::aws-glue-*"
                  ]
              },
              {
                  "Effect": "Allow",
                  "Action": [
                      "logs:CreateLogGroup",
                      "logs:CreateLogStream",
                      "logs:PutLogEvents"
                  ],
                  "Resource": [
                      "arn:aws:logs:*:*:/aws-glue/*"
                  ]
              },
              {
                  "Effect": "Allow",
                  "Action": [
                      "ec2:CreateTags",
                      "ec2:DeleteTags"
                  ],
                  "Condition": {
                      "ForAllValues:StringEquals": {
                          "aws:TagKeys": [
                              "aws-glue-service-resource"
                          ]
                      }
                  },
                  "Resource": [
                      "arn:aws:ec2:*:*:network-interface/*",
                      "arn:aws:ec2:*:*:security-group/*",
                      "arn:aws:ec2:*:*:instance/*"
                  ]
              }
          ]
      }
      ```

    * Policy 2:

        ```json
        {
            "Version": "2012-10-17",
            "Statement": [
                {
                    "Effect": "Allow",
                    "Action": [
                        "s3:GetObject",
                        "s3:PutObject"
                    ],
                    "Resource": [
                        "arn:aws:s3:::{ your bucket arn }/segment-logs/*",
                        "arn:aws:s3:::{ your bucket arn }/transformed/*"
                    ]
                }
            ]
        }
      ```
1. Navigate to the Glue service in your AWS console.
2. Click **Get started** and then select **Jobs** from the left navigation on the Glue console page.
3. Select **Spark script editor** and click **Create**.
4. The following code sample is the source code for a generic Glue job. Copy the code example to your clipboard and paste it into the Glue editor window, modifying as necessary to reflect the names of the events you wish to extract from the Segment logs (see line #25).

    ```python
    import sys
    from awsglue.transforms import *
    from awsglue.utils import getResolvedOptions
    from awsglue.context import GlueContext
    from awsglue.dynamicframe import DynamicFrame
    from awsglue.job import Job
    from pyspark.context import SparkContext
    from pyspark.sql.functions import unix_timestamp

    ## @params: [JOB_NAME,S3_JSON_INPUT_PATH,S3_CSV_OUTPUT_PATH]
    args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_JSON_INPUT_PATH', 'S3_CSV_OUTPUT_PATH'])

    sc = SparkContext()
    glueContext = GlueContext(sc)
    spark = glueContext.spark_session
    job = Job(glueContext)
    job.init(args['JOB_NAME'], args)

    # Load JSON into dynamic frame
    datasource0 = glueContext.create_dynamic_frame.from_options('s3', {'paths': [args['S3_JSON_INPUT_PATH']], 'recurse': True}, 'json')
    print("Input file: ", args['S3_JSON_INPUT_PATH'])
    print("Input file total record count: ", datasource0.count())

    # Filters the JSON documents that we want included in the output CSV
    supported_events = ['Product Added', 'Order Completed', 'Product Clicked']
    def filter_function(dynamicRecord):
            if ('anonymousId' in dynamicRecord and
                            'userId' in dynamicRecord and
                            'properties' in dynamicRecord and
                            'sku' in dynamicRecord["properties"] and
                            'event' in dynamicRecord and
                            dynamicRecord['event'] in supported_events):
                    return True
            else:
                    return False

    # Apply filter function to dynamic frame
    interactions = Filter.apply(frame = datasource0, f = filter_function, transformation_ctx = "interactions")
    print("Filtered record count: ", interactions.count())

    # Map only the fields we want in the output CSV, changing names to match target schema.
    applymapping1 = ApplyMapping.apply(frame = interactions, mappings = [ \
            ("anonymousId", "string", "ANONYMOUS_ID", "string"), \
            ("userId", "string", "USER_ID", "string"), \
            ("properties.sku", "string", "ITEM_ID", "string"), \
            ("event", "string", "EVENT_TYPE", "string"), \
            ("timestamp", "string", "TIMESTAMP_ISO", "string")], \
            transformation_ctx = "applymapping1")

    # Repartition to a single file since that is what is required by Personalize
    onepartitionDF = applymapping1.toDF().repartition(1)
    # Coalesce timestamp into unix timestamp
    onepartitionDF = onepartitionDF.withColumn("TIMESTAMP", \
            unix_timestamp(onepartitionDF['TIMESTAMP_ISO'], "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"))
    # Convert back to dynamic frame
    onepartition = DynamicFrame.fromDF(onepartitionDF, glueContext, "onepartition_df")

    # Write output back to S3 as a CSV
    glueContext.write_dynamic_frame.from_options(frame = onepartition, connection_type = "s3", \
            connection_options = {"path": args['S3_CSV_OUTPUT_PATH']}, \
            format = "csv", transformation_ctx = "datasink2")

    job.commit()
    ```
5. Select the **Job details** tab.
6. Enter a name for your Glue job. 
6. Leave Type as **Spark**.
7. Make any optional changes on the Job details page, and click **Save** to save the job script.

To review key parts of the Python script in more detail:
1. The script is initialized with a few job parameters. You'll see how to specify these parameter values when the job below runs. For now, see that Segment is passing in the location of the raw JSON files using `S3_JSON_INPUT_PATH` and the location where the output CSV should be written through `S3_CSV_OUTPUT_PATH`.

    ```python
        args = getResolvedOptions(sys.argv, ['JOB_NAME', 'S3_JSON_INPUT_PATH', 'S3_CSV_OUTPUT_PATH'])
    ```

2. The Spark and Glue contexts are created and associated. A Glue Job is also created and initialized.

    ```python
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
    ```

3. The first step in Segment's Job is to load the raw JSON file as a Glue DynamicFrame.

    ```python
       datasource0 = glueContext.create_dynamic_frame.from_options('s3', {'paths': [args['S3_JSON_INPUT_PATH']], 'recurse': True}, 'json')
    ```

4. Since not all events that are written to S3 by Segment are relevant to training a Personalize model, Segment uses Glue's `Filter` transformation to keep the records needed.
5. The `datasource0` DynamicFrame created above is passed to `Filter.apply(...)` function along with the `filter_function` function. It's in `filter_function` where Segment keeps events that have a product SKU and `userId` specified. The resulting DynamicFrame is captured as `interactions`.

    ```python
    def filter_function(dynamicRecord):
        if dynamicRecord["properties"]["sku"] and dynamicRecord["userId"]:
            return True
        else:
            return False

    interactions = Filter.apply(frame = datasource0, f = filter_function, transformation_ctx = "interactions")
    ```

6. Segment calls Glue's `ApplyMapping` transformation, passing the `interactions` DynamicFrame from above and field mapping specification that indicates the fields Segment wants to retain and their new names. These mapped field names become the column names in Segment's output CSV. You'll notice that Segment is using the product SKU as the `ITEM_ID` and `event` as the `EVENT_TYPE`. Segment also renames the `timestamp` field to `TIMESTAMP_ISO` since the format of this field value in the JSON file is an ISO 8601 date and Personalize requires timestamps to be specified in UNIX time (number seconds since Epoc).

    ```python
    applymapping1 = ApplyMapping.apply(frame = interactions, mappings = [ \
        ("anonymousId", "string", "ANONYMOUS_ID", "string"), \
        ("userId", "string", "USER_ID", "string"), \
        ("properties.sku", "string", "ITEM_ID", "string"), \
        ("event", "string", "EVENT_TYPE", "string"), \
        ("timestamp", "string", "TIMESTAMP_ISO", "string")], \
        transformation_ctx = "applymapping1")
    ```

7. To convert the ISO 8601 date format to UNIX time for each record, Segment uses Spark's `withColumn(...)` to create a new column called `TIMESTAMP` that is the converted value of the `TIMESTAMP_ISO` field. Before Segment can call `withColumn`, Segment needs to convert the Glue DynamicFrame into a Spark DataFrame. That is accomplished by calling `toDF()` on the output of ApplyMapping transformation above. Since Personalize requires Segment's uploaded CSV to be a single file, Segment calls `repartition(1)` on the DataFrame to force all data to be written in a single partition. After creating the `TIMESTAMP` in the expected format, `DyanmicFrame.fromDF()` is called to convert the DataFrame back into a DyanmicFrame.

    ```python
        # Repartition to a single file
        onepartitionDF = applymapping1.toDF().repartition(1)
        # Coalesce timestamp into unix timestamp
        onepartitionDF = onepartitionDF.withColumn("TIMESTAMP", \
            unix_timestamp(onepartitionDF['TIMESTAMP_ISO'], "yyyy-MM-dd'T'HH:mm:ss.SSS'Z'"))
        # Convert back to dynamic frame
        onepartition = DynamicFrame.fromDF(onepartitionDF, glueContext, "onepartition_df")
    ```

8. Segment's CSV is written back to S3 at the path specified by the `S3_CSV_OUTPUT_PATH` job property and commits the job.

    ```python
        glueContext.write_dynamic_frame.from_options(frame = onepartition, connection_type = "s3", \
            connection_options = {"path": args['S3_CSV_OUTPUT_PATH']}, \
            format = "csv", transformation_ctx = "datasink2")

        job.commit()
    ```

**Run Your AWS Glue ETL Job**

With Segment's ETL Job script created and saved, it's time to run the job to create the CSV needed to train a Personalize Solution. To do this:

1. Open another AWS console browser tab/window by right-clicking on the AWS logo in the upper left corner of the page and select **Open Link in New Tab** (or Window).

2. While still in the Glue service console and the job listed, click **Run job**. This will cause the Parameters panel to display.

3. Click the **Security configuration, script libraries, and job parameters** section header to cause the job parameters fields to be displayed.

    ![A screenshot of the Parameters panel.](images/GlueRunJobDialog.png)


4. Scroll down to the **Job parameters** section. This is where Segment will specify the job parameters that Segment's script expects for the path to the input data and the path to the output file.
5. Create two job parameters with the following key and value.
  * Be sure to prefix each key with `--` as shown. Substitute your account ID for `[ACCOUNT_ID]` in the values below. You copy the bucket name to your clipboard from the S3 service page in the tab/window you opened above. The order they are specified does not matter.

    | **Key**              | **Value**                                      |
    | -------------------- | ---------------------------------------------- |
    | --S3_JSON_INPUT_PATH | s3://personalize-data-[ACCOUNT_ID]/raw-events/ |
    | --S3_CSV_OUTPUT_PATH | s3://personalize-data-[ACCOUNT_ID]/transformed |

    ![A screenshot of the job parameters section, showing two key/value pairs as outlined in the preceding table.](images/GlueRunJobParams.png)


6. Click **Run job** to start the job.  Note that this dialog scrolls.

7. Once the job has started running, you'll see log output in the **Logs** tab at the bottom of the page. It may take a few minutes to complete.

8. When the job completes, click the **X** in the upper right corner of the page to exit the job script editor.

**Verify Output File**

To verify the output file:

1. Go to the S3 service page in the AWS console and find the bucket with a name starting with `personalize-data-...`.
2. Click on the bucket name. If the job completed successfully, you'll see a folder named **transformed**.
3. Click on **transformed** and you'll see the output file created by the ETL job.

    ![A screenshot of the overview for the personalize data bucket created in step 1.](images/GlueJobOutputFile.png)


## Create Personalize Dataset Group, Solution and Campaign

### Create Personalize Dataset Group

To create a personalize dataset group:

1. Browse to the Amazon Personalize service landing page in the AWS console.

2. Click **View dataset groups** to get started.

    ![A screenshot of the Amazon Personalize page, with boxes around the data center (N. Virginia) and the View dataset groups button.](images/PersonalizeIntroPage.png)

3. On the Dataset Groups page, click **Create dataset group**.

    ![A screenshot of the dataset groups page, with a box around the Create dataset group button.](images/PersonalizeDatasetGroups.png)

4. On the **Create dataset group** page, give your dataset group a name.

5. Select **Upload user-item interaction data** since Segment will be uploading the CSV prepared in the previous steps.

6. Click **Next** to continue.

    ![A screenshot of the create dataset group settings page, with a dataset group name, MyEventData, entered and the Upload user-item interaction data option selected.](images/PersonalizeCreateGroup.png)


7. On the **Create user-item interaction data** page, select **Create new schema** and give your schema a name.

    ![A screenshot of the Create user-item interaction data page in AWS, with the Create new schema setting selected.](images/PersonalizeSchema.png)


8. Scroll down to the **Schema definition** editor. Dataset schemas in Personalize are represented in [Avro](https://avro.apache.org/docs/current/spec.html){:target="_blank"}. Learn more about For detailed [Personalize schema definitions](https://docs.aws.amazon.com/personalize/latest/dg/how-it-works-dataset-schema.html){:target="_blank"}.
  * Avro is a remote procedure call and data serialization framework developed within Apache's Hadoop project. It uses JSON for defining data types and protocols, and serializes data in a compact binary format.
  * This example uses the following example schema:

        ```json
        {
            "type": "record",
            "name": "Interactions",
            "namespace": "com.amazonaws.personalize.schema",
            "fields": [
                {
                    "name": "USER_ID",
                    "type": "string"
                },
                {
                    "name": "ITEM_ID",
                    "type": "string"
                },
                {
                    "name": "EVENT_TYPE",
                    "type": "string"
                },
                {
                    "name": "TIMESTAMP",
                    "type": "long"
                }
            ],
            "version": "1.0"
        }
        ```

        * The required fields for the user-item interaction dataset schema are `USER_ID`, `ITEM_ID`, and `TIMESTAMP`. There's also an optional field `EVENT_TYPE`.

9. Copy the contents of Avro schema to your clipboard and paste it into the "Schema definition" editor (replacing the proposed schema).

10. Click **Next** to save the schema and move to the next step.

11. The **Import user-item interaction data** step is displayed next. To complete this form Segment needs to get two pieces of information from IAM and S3. Give your import job a name and set the automatic import to **Off**.

12. For the **IAM service role**, select **Create a new role** from the dropdown.
13. In the next pop-up, Segment recommends listing your bucket name in the **Specific S3 buckets** option, but you're free to choose the option that best suits your needs.

14. Find the location of the CSV file you generated in the earlier steps. This needs to be configured in the **Data Location** field on this screen.

    ![A screenshot of the Import user-item interaction data, with the Data Location field present.](images/PersonalizeImportJob.png)

15. After clicking the **Finish** button at the bottom of the page, you'll return to the Personalize Dashboard where you can monitor the progress of your interaction dataset as it is being created.

Be patient as this process can take a long time to complete.

![A screenshot of the Dataset groups dashboard, with a box around the status of the dataset upload (Create in progress).](images/PersonalizeInteractionDatasetCreating.png)

### Create Personalize Solution

Once Segment's event CSV is finished importing into a user-item interaction dataset, Segment can create a Personalize Solution. To do this:

1. From the Dashboard page for the dataset group created above, click **Start** in the **Create solutions** column.

    ![A screenshot of the Dataset groups dashboard, with a box around the Start button in the Create solutions column.](images/PersonalizeCreateSolution.png)

2. On the **Create solution** page, enter a **Solution name**.
  * For a discussion on the different recipes you can use with Personalize, see [here](https://docs.aws.amazon.com/personalize/latest/dg/working-with-predefined-recipes.html){:target="_blank"}.

    ![A screenshot of the Create solution page, with a solution name entered in the Solution name field.](images/PersonalizeSolutionConfig.png)


3. Click **Finish** to create your Solution. This process can take several minutes to several hours to complete.

    ![A screenshot of the Dataset groups dashboard, with a box around the status of the solution creation (Create in progress).](images/PersonalizeSolutionInProgress.png)

### Create Personalize Campaign

A deployed solution is known as a campaign, and is able to make recommendations for your users. To deploy a solution, you create a campaign in the console or by calling the CreateCampaign API. You can choose which version of the solution to use. By default, a campaign uses the latest version of a solution.

To create a Personalize campaign:

1. From the Dataset Group Dashboard, click **Create new campaign**.

    ![A screenshot of the Dataset Group Dashboard, with a box around the Create new campaign button in the Launch Campaigns column.](images/PersonalizeCreateCampaignDash.png)

2. Enter the name for your campaign.
3. Select the solution you created above and click **Create campaign**.

    ![A screenshot of the Create new campaign page, with a campaign name entered and a solution selected (the solution created above).](images/PersonalizeCreateCampaign.png)

4. Personalize will start creating your new campaign. This process can take several minutes.

    ![A screenshot of the overview page for the campaign created in the previous step, with a banner reading Campaign creation in progress.](images/PersonalizeCampaignCreating.png)


In the next section, Segment will build a real-time clickstream ingestion pipeline that accepts events from Segment and can query the solution you just deployed.

## Getting Recommendations and Live Event Updates

Once you deploy your Personalize solution and enable a Campaign, your Lambda instance consumes event notifications from Segment and uses the Solution and Campaign to react to events which drive your business cases.

The example code Segment provides below shows how to forward events to the Personalize Solution you deployed to keep your model updated.  It then forwards an `identify` event back to Segment with the recommendations from your Solution.


### Set up Segment IAM policy & role for invoking your Lambda

Segment will need to be able to call ("invoke") your Lambda in order to process events.  This requires you to configure an IAM role for your Lambda which allows the Segment account to invoke your function.

#### Create an IAM policy
To create an IAM policy:
1. Sign in to the [Identity and Access Management (IAM) console](https://console.aws.amazon.com/iam/){:target="_blank"}  and follow these instructions to [Create an IAM policy](http://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html){:target="_blank"} to allow Segment permission to invoke your Lambda function.

2. Select **Create Policy from JSON** and use the following template policy in the `Policy Document` field. Be sure to change the `{region}`, `{account-id}` and `{function-names}` with the applicable values. Here's example of a Lambda ARN `arn:aws:lambda:us-west-2:355207333203:function:``my-example-function`.

> note ""
> **NOTE:** You can put in a placeholder ARN for now, as you will need to come back to this step to update with the ARN of your Lambda once that's been created.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
        "Effect": "Allow",
        "Action": [
            "lambda:InvokeFunction"
        ],
        "Resource": [
            "lambda ARN 1",
            "lambda ARN 2",
            ...
            "lambda ARN n"
        ]
    }
  ]
}
```

#### Create an IAM role
To create an IAM role:
1. Sign in to the [Identity and Access Management (IAM) console](https://console.aws.amazon.com/iam/){:target="_blank"} and follow these instructions to [Create an IAM role](http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_create_for-user.html#roles-creatingrole-user-console){:target="_blank"} to allow Segment permission to invoke your Lambda function.
2. While setting up the new role, add the policy you created in the [previous step](/docs/connections/destinations/catalog/amazon-personalize/#create-an-iam-policy).
3. Finish with any other set up items you may want (like `tags`).
4. Search for and click on your new roles from the [IAM home](https://console.aws.amazon.com/iam/home#/home){:target="_blank"}.
5. Select the **Trust Relationships** tab, then click **Edit trust relationship**.

    ![A screenshot of the Trust Relationships tab, with the Edit trust relationship button visible.](images/LambdaTrustRelationship.png)

6. Copy and paste the following into your trust relationship. You should replace `` with either the Source ID of the attached Segment source (the default) or the custom external ID you set in your Amazon Lambda destination settings.

> note ""
> **NOTE:** Your Source ID can be found by navigating to **Settings > API Keys** from your Segment source homepage.
>
> For security purposes, Segment will set your Workspace ID as your External ID. If you are currently using an External ID different from your Workspace ID, reach out to Segment support so they can change it and make your account more secure.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::595280932656:role/customer-personalize-prod-destination-access"
      },
      "Action": "sts:AssumeRole",
      "Condition": {
        "StringEquals": {
          "sts:ExternalId": "YOUR_SEGMENT_SOURCE_ID"
        }
      }
    }
  ]
}
```

If you have multiple Sources using this Role, or require the use of multiple externalIds, replace the `sts:ExternalId` setting above with:

```
    "sts:ExternalId": ["YOUR_SEGMENT_SOURCE_ID", "ANOTHER_SOURCE_ID", "AN_EXTERNAL_ID", "ANOTHER_EXTERNAL_ID"]
```

### Build a Lambda Function to Process Segment Events

In order to process events from Segment, you will need to provide a Lambda function that can handle your event flow.  This function can be used to forward events to a Tracker for your Personalize solution, or to post process events, get recommendations from your Solution, or to push these results back to Segment for later use in the destinations you have configured.

Segment allows you to send each call type (`track`,`identify`,etc) to a different Lambda function. The example below shows one generic function that could be used to handle any call.

Segment provides an example Lambda function, written in Python, for you to get up and running.

To build a Lambda function to process Segment events:
1. Go to the Lambda service page in your AWS account.
2. Ensure that you are in AWS Region 'us-west-2'. You must be in us-west-2 so that Segment's Lambdas can  communicate with your resources.
3. Click **Create a function** to create a new function.

    ![A screenshot of the Lambda service page in AWS, with a box around the Create a function button.](images/LambdaDashboard.png)

4. Select **Author from scratch** since Segment will be providing the source code for the function.

5. Enter a name for your function and select **Python 3.7** for the runtime.

6. For the **Role** field, select **Create a new role from AWS policy templates** from the dropdown.
7. Create a **Role name** that makes sense for you, and leave **Policy templates** empty. You will come back to modify this role shortly.

8. Click **Create function**.

    ![A screenshot of the Create a function settings page, with a function name, runtime, permissions, and role name entered.](images/LambdaCreateFunction.png)


**Lambda Function Source Code**

1. Download the `.zip` file at https://github.com/segmentio/segment-lambda-recipes/blob/master/segment-personalize/support/segment_personalize_boilerplate.zip.

2. Within the Lambda function screen, scroll down to the **Function code** panel.
3. For **Code entry type** choose **Upload a .zip file**, and click **Upload**, then load the `.zip` you downloaded in the first step.

You should now be able to see the code (and associate folders) in the code editor.

![A screenshot of the Lambda code editor.](images/LambdaFunctionCode.png)

Segment will call your lambda once per event.  The provided code maps Segment event fields from the Segment event it gets, and sends them to your Personalize Tracker. It then calls Personalize to get a recommendation for the userId in the event, and pushes that recommendation back as a user trait into Segment, using the `identify` call.

Make sure you are clicking **Save** frequently during the next steps!

**Wire up Personalize API using Lambda Layer (Preview only)**

You will notice in the function source the following `import` and function call.

```python
import of import init_personalize_api as api_helper
...
api_helper.init()
```

This `import` and function call uses some boilerplate code, packaged as a [Lambda Layer](https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html){:target="_blank} needed to configure the Personalize API with the AWS Python SDK. This is only necessary while Personalize is in Preview. Once Personalize is GA and the API is bundled with the Python SDK, as well as other language SDKs, this supporting Layer will no longer be needed.

To install Segment's Layer:

1. Open the Lambda navigation panel and click **Layers**.

    ![Lambda Nav Panel.](images/LambdaNav.png)

    ![Lambda Layers Nav.](images/LambdaLayersNav.png)

2. From the Lambda Layers view, click **Create layer**.

    ![Lambda Create Layer.](images/LambdaCreateLayer.png)

3. Create the layer by specifying a name such as "PersonalizeApiInstaller", browsing to the pre-made zip in https://github.com/segmentio/segment-lambda-recipes/blob/master/segment-personalize/support/python_personalize_init.zip, and select **Python 3.7** as the compatible runtime.
4. Click **Create** to upload the zip file and create the layer.

    ![Lambda Create Layer Config.](images/LambdaCreateLayerConfig.png)

5. Add the layer just created to Segment's function.
6. Return to the Lambda function by opening the Lambda navigation panel and clicking **Functions**.

    ![Lambda Nav Panel.](images/LambdaNav.png)

    ![Lambda Function Layer Add.](images/LambdaFunctionsNav.png)

7. Click on your function name to access the configuration page again.
8. In the Lambda Designer, click the **Layers** panel below the function name and then **Add layer**  in the **Referenced layers** panel at the bottom of the page.

    ![Lambda Function Layer Add.](images/LambdaLayerAdd.png)

9. Select the layer you just added and the latest version.
10. Click **Add** to add the layer to the function.

    ![Lambda Function Layer Add.](images/LambdaLayerAddSelect.png)

**Update your IAM role for your Lambda to call Personalize**

You need to modify the IAM Role & Policy originally created with this Lambda to allow it to send and receive data from Personalize. To do this:

1. From the **Execution role** section of your Lambda function, click the **View the ** link.

    ![A screenshot of the execution role settings section on your Lambda.](images/ExecutionRoleIAM.png)

2. Click the arrow next to your policy in this role, then **Edit Policy**.

    ![A screenshot of the permissions policies settings section on your Lambda.](images/EditPolicy.png)

3. Add the code below to the existing permissions from within the JSON editor.
4. Click **Review Policy** and **Save Changes**.

```json
{
    "Effect": "Allow",
    "Action": [
        "personalize:GetRecommendations",
        "personalize:Record",
        "personalize:PutEvents"
    ],
    "Resource": [
        "*"
    ]
}
```

**Wire-up Personalize Event Tracker**

Another dependency in the function is the ability to call the Personalize [PutEvents API](https://docs.aws.amazon.com/personalize/latest/dg/API_UBS_PutEvents.html){:target="_blank} endpoint as shown in the following excerpt.

```js
    personalize_events.put_events(
      trackingId = os.environ['personalize_tracking_id'],
      userId = event['userId'],
      sessionId = event['anonymousId'],
      eventList = [
          {
              "eventId": event['messageId'],
              "sentAt": int(dp.parse(event['timestamp']).strftime('%s')),
              "eventType": event['event'],
              "properties": json.dumps(properties)
          }
      ]
      )
```

The `trackingId` function argument identifies the Personalize Event Tracker which handles the events Segment submits. This value is passed to Segment's Lambda function as an Environment variable.

You need to create a Personalize Event Tracker for the Dataset Group you created earlier. To do this:

1. In another browser tab/window, go to the Personalize service landing page in the AWS console.

2. Click on your Dataset Group and then **Event trackers** in the left navigation.

3. Click **Create event tracker** button.

    ![A screenshot of the Event trackers settings page, with a box around the Create event tracker button.](images/PersonalizeCreateTracker.png)

4. Enter a name for your Event Tracker.

5. You need to configure a role for Personalize to that allows it to execute the tracker.  This is the same as the execution role you defined earlier for Personalize. Often this is automatically included as a policy labelled "AmazonPersonalizeFullAccess"

      ```json
          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Effect": "Allow",
                      "Action": [
                          "personalize:*"
                      ],
                      "Resource": "*"
                  },
                  {
                      "Effect": "Allow",
                      "Action": [
                          "cloudwatch:PutMetricData"
                      ],
                      "Resource": "*"
                  },
                  {
                      "Effect": "Allow",
                      "Action": [
                          "s3:GetObject",
                          "s3:PutObject",
                          "s3:DeleteObject",
                          "s3:ListBucket"
                      ],
                      "Resource": [
                          "arn:aws:s3:::*Personalize*",
                          "arn:aws:s3:::*personalize*"
                      ]
                  },
                  {
                      "Effect": "Allow",
                      "Action": [
                          "iam:PassRole"
                      ],
                      "Resource": "*",
                      "Condition": {
                          "StringEquals": {
                              "iam:PassedToService": "personalize.amazonaws.com"
                          }
                      }
                  }
              ]
          }
      ```

    * This may be automatically included as policy "AmazonPersonalize-ExecutionPolicy-"

      ```json
          {
              "Version": "2012-10-17",
              "Statement": [
                  {
                      "Action": [
                          "s3:ListBucket"
                      ],
                      "Effect": "Allow",
                      "Resource": [
                          "arn:aws:s3:::{ your s3 bucket }"
                      ]
                  },
                  {
                      "Action": [
                          "s3:GetObject",
                          "s3:PutObject"
                      ],
                      "Effect": "Allow",
                      "Resource": [
                          "arn:aws:s3:::{ your s3 bucket }/*"
                      ]
                  }
              ]
          }
      ```

      ![A screenshot of the Configure tracker setup page.](images/PersonalizeEventTrackerConfig.png)


6. The Event Tracker's tracking ID is displayed on the following page and is also available on the Event Tracker's detail page. Copy this value to your clipboard.

    ![A screenshot of the Install the SDK page, with a box around the Tracking Id value.](images/PersonalizeEventTrackerCreating.png)

7. Returning to the Lambda function, paste the Event Tracker's tracking ID into an Environment variable for the function with the key `personalize_tracking_id`.

    ![A screenshot of the key/value pairs under the Environment variables page, with the key personalize_tracking_id present.](images/LambdaEnvVariable.png)

8. Add environment variables for Segment and for the function to tell it the Personalize Campaign to call for retrieving recommendations.

9. To obtain the Personalize Campaign ARN, go to the Personalize service landing page in the AWS console.

10. Select the Dataset Group you created earlier and then Campaigns in the left navigation.

11. Click on the campaign you created earlier and copy the **Campaign ARN** to your clipboard.

    ![A screenshot of the campaign overview page, with a box around Campaigns in the side navigation.](images/PersonalizeCampaignArn.png)


12. Return to your Lambda function and scroll down to the **Environment variables** panel.

13. Add an environment variable with the key `personalize_campaign_arn` and value of the Campaign ARN in your clipboard.
14. Scroll to the top of the page and click **Save** to save your changes.

    ![A screenshot of the environmental variables panel, with a key/value pair of personalize_campaign_arn included.](images/LambdaRecCampaignArn.png)

15. You need a key for the Segment source that will get Segment's update events. Go back to your Segment workspace tab or window, and click on the source which will receive events from your Lambda, and copy the write key from the **Overview** tab.

  ![A screenshot of a Source's overview panel in Segment, with an arrow pointing to the Write Key field.](images/SegmentWriteKey.png)


16. Go back to your Lambda tab or window, and paste the key under a property called `connections_source_api_key`.

_Make sure to click **Save**_ here or you will need to do this again.

![A screenshot of the environmental variables panel, with a key/value pair, connections_source_api_key, included.](images/LambdaRecCampaignArn.png)


Your lambda is now ready to receive events from Segment.  Next, you will need to enable your Segment Personalize Destination.


### Configure Segment Personalize Lambda Destination

Once your Lambda function is enabled, you can send it events from Segment using the Personalize Destination.

1. In the Segment source that you want to connect to your Personalize destination to, click **Add Destination**.
2. Search and select the **Personalize** destination and enter details for [these settings options](/docs/connections/destinations/catalog/amazon-personalize/#settings)

Segment allows you to send each call type to a different Lambda. If you leave the Lambda field blank for a given call type, Segment won't attempt to send any of those calls.


**Track**

There are two settings relevant for track calls:

1. Lambda for track calls - the Lambda where the Segment app should route track calls.
2. Events - a list of specific events to send. You may send *all* track events (see setting details for instructions on how), but use caution with this option, as it may significantly increase your Lambda costs.


## FAQ

**What is the Log Type Setting?**

This setting controls the [Log Type](https://docs.aws.amazon.com/lambda/latest/dg/API_Invoke.html#API_Invoke_RequestSyntax){:target="_blank} for your Lambda function using Cloud Watch. Select option `Tail` if you would like to see [detailed logs](https://docs.aws.amazon.com/lambda/latest/dg/monitoring-functions.html){:target="_blank}in Cloud Watch.

**My Lambda <> Segment connection is timing out, what do I do?**

Due to how Segment's event delivery system, [Centrifuge](https://segment.com/blog/introducing-centrifuge/){:target="_blank}, works, your Lambda can't take more than five seconds to run per message. If you're consistently running into timeout issues, you should consult the [AWS Lambda docs](https://docs.aws.amazon.com/lambda/index.html#lang/en_us){:target="_blank}, as well as docs for your language of choice, for tips on optimizing performance.
snippet rollbar_deployment
	rollbar_deployment:
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		revision=${3:# REQUIRED}
		comment=${4}
		rollbar_user=${5}
		url=${6:https://api.rollbar.com/api/1/deploy/}
		user=${7}
		validate_certs=${8:#yes|no}

snippet logentries
	logentries: path=${1:# REQUIRED} state=${2:#present|absent}

snippet nagios
	nagios:
		action=${1:#downtime|enable_alerts|disable_alerts|silence|unsilence|silence_nagios|unsilence_nagios|command}
		command=${2:# REQUIRED}
		services=${3:# REQUIRED}
		host=${4}
		author=${5:ansible}
		minutes=${6:30}
		cmdfile=${7:auto-detected}

snippet newrelic_deployment
	newrelic_deployment:
		token=${1:# REQUIRED}
		application_id=${2}
		description=${3}
		changelog=${4}
		appname=${5}
		environment=${6}
		user=${7}
		revision=${8}
		validate_certs=${9:#yes|no}
		app_name=${10}

snippet librato_annotation
	librato_annotation:
		links=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		user=${4:# REQUIRED}
		description=${5}
		start_time=${6}
		name=${7}
		source=${8}
		end_time=${9}

snippet stackdriver
	stackdriver:
		key=${1:# REQUIRED}
		repository=${2}
		level=${3:#INFO|WARN|ERROR}
		annotated_by=${4:ansible}
		deployed_to=${5}
		deployed_by=${6:ansible}
		instance_id=${7}
		msg=${8}
		event_epoch=${9}
		revision_id=${10}
		event=${11:#annotation|deploy}

snippet pagerduty
	pagerduty:
		name=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|started|ongoing}
		user=${4:# REQUIRED}
		service=${5}
		hours=${6:1}
		validate_certs=${7:#yes|no}
		desc=${8:created by ansible}

snippet boundary_meter
	boundary_meter:
		apikey=${1:# REQUIRED}
		apiid=${2:# REQUIRED}
		name=${3:# REQUIRED}
		state=${4:#present|absent}
		validate_certs=${5:#yes|no}

snippet airbrake_deployment
	airbrake_deployment:
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		repo=${3}
		user=${4}
		url=${5:https://airbrake.io/deploys}
		validate_certs=${6:#yes|no}
		revision=${7}

snippet pingdom
	pingdom:
		checkid=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|paused}
		uid=${4:# REQUIRED}
		key=${5:# REQUIRED}

snippet datadog_event
	datadog_event:
		text=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		date_happened=${4:now}
		alert_type=${5:#error|warning|info|success}
		tags=${6}
		priority=${7:#normal|low}
		aggregation_key=${8}
		validate_certs=${9:#yes|no}

snippet monit
	monit: state=${1:#present|started|stopped|restarted|monitored|unmonitored|reloaded} name=${2:# REQUIRED}

snippet redhat_subscription
	redhat_subscription:
		username=${1}
		server_hostname=${2:current value from c(/etc/rhsm/rhsm.conf) is the default}
		state=${3:#present|absent}
		autosubscribe=${4:false}
		activationkey=${5}
		server_insecure=${6:current value from c(/etc/rhsm/rhsm.conf) is the default}
		password=${7}
		rhsm_baseurl=${8:current value from c(/etc/rhsm/rhsm.conf) is the default}
		pool=${9:^$}

snippet zypper
	zypper: name=${1:# REQUIRED} state=${2:#present|latest|absent} disable_gpg_check=${3:#yes|no}

snippet homebrew_cask
	homebrew_cask: name=${1:# REQUIRED} state=${2:#installed|uninstalled}

snippet yum
	yum:
		name=${1:# REQUIRED}
		state=${2:#present|latest|absent}
		disablerepo=${3}
		enablerepo=${4}
		list=${5}
		disable_gpg_check=${6:#yes|no}
		conf_file=${7}

snippet apt_repository
	apt_repository:
		repo=${1:# REQUIRED}
		update_cache=${2:#yes|no}
		state=${3:#absent|present}
		validate_certs=${4:#yes|no}
		mode=${5:420}

snippet pkgutil
	pkgutil: state=${1:#present|absent|latest} name=${2:# REQUIRED} site=${3}

snippet apt
	apt:
		dpkg_options=${1:force-confdef,force-confold}
		upgrade=${2:#yes|safe|full|dist}
		force=${3:#yes|no}
		name=${4}
		purge=${5:#yes|no}
		state=${6:#latest|absent|present}
		update_cache=${7:#yes|no}
		default_release=${8}
		cache_valid_time=${9:false}
		deb=${10}
		install_recommends=${11:#yes|no}

snippet svr4pkg
	svr4pkg:
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		category=${3:#true|false}
		src=${4}
		zone=${5:#current|all}
		response_file=${6}
		proxy=${7}

snippet npm
	npm:
		executable=${1}
		name=${2}
		global=${3:#yes|no}
		state=${4:#present|absent|latest}
		production=${5:#yes|no}
		registry=${6}
		version=${7}
		path=${8}

snippet pkgng
	pkgng:
		name=${1:# REQUIRED}
		cached=${2:#yes|no}
		state=${3:#present|absent}
		pkgsite=${4}
		annotation=${5}

snippet openbsd_pkg
	openbsd_pkg: state=${1:#present|latest|absent} name=${2:# REQUIRED}

snippet gem
	gem:
		name=${1:# REQUIRED}
		include_dependencies=${2:#yes|no}
		executable=${3}
		repository=${4}
		user_install=${5:yes}
		pre_release=${6:no}
		state=${7:#present|absent|latest}
		version=${8}
		gem_source=${9}

snippet layman
	layman: name=${1:# REQUIRED} list_url=${2} state=${3:#present|absent|updated}

snippet composer
	composer:
		working_dir=${1:# REQUIRED}
		prefer_dist=${2:#yes|no}
		prefer_source=${3:#yes|no}
		no_scripts=${4:#yes|no}
		no_dev=${5:#yes|no}
		no_plugins=${6:#yes|no}
		optimize_autoloader=${7:#yes|no}

snippet pkgin
	pkgin: name=${1:# REQUIRED} state=${2:#present|absent}

snippet zypper_repository
	zypper_repository:
		repo=${1}
		state=${2:#absent|present}
		description=${3}
		disable_gpg_check=${4:#yes|no}
		name=${5}

snippet pip
	pip:
		virtualenv=${1}
		virtualenv_site_packages=${2:#yes|no}
		virtualenv_command=${3:virtualenv}
		chdir=${4}
		requirements=${5}
		name=${6}
		executable=${7}
		extra_args=${8}
		state=${9:#present|absent|latest}
		version=${10}

snippet cpanm
	cpanm:
		notest=${1:false}
		from_path=${2}
		name=${3}
		locallib=${4:false}
		mirror=${5:false}

snippet homebrew_tap
	homebrew_tap: tap=${1:# REQUIRED} state=${2:#present|absent}

snippet portinstall
	portinstall: name=${1:# REQUIRED} state=${2:#present|absent} use_packages=${3:#yes|no}

snippet homebrew
	homebrew:
		name=${1:# REQUIRED}
		update_homebrew=${2:#yes|no}
		install_options=${3}
		state=${4:#head|latest|present|absent|linked|unlinked}
		upgrade_all=${5:#yes|no}

snippet rhn_channel
	rhn_channel:
		sysname=${1:# REQUIRED}
		name=${2:# REQUIRED}
		url=${3:# REQUIRED}
		password=${4:# REQUIRED}
		user=${5:# REQUIRED}
		state=${6:present}

snippet apt_key
	apt_key:
		keyserver=${1}
		url=${2}
		data=${3}
		keyring=${4}
		state=${5:#absent|present}
		file=${6}
		validate_certs=${7:#yes|no}
		id=${8}

snippet opkg
	opkg: name=${1:# REQUIRED} state=${2:#present|absent} update_cache=${3:#yes|no}

snippet rhn_register
	rhn_register:
		username=${1}
		channels=${2:[]}
		state=${3:#present|absent}
		activationkey=${4}
		password=${5}
		server_url=${6:current value of i(serverurl) from c(/etc/sysconfig/rhn/up2date) is the default}

snippet easy_install
	easy_install:
		name=${1:# REQUIRED}
		virtualenv=${2}
		virtualenv_site_packages=${3:#yes|no}
		virtualenv_command=${4:virtualenv}
		executable=${5}

snippet swdepot
	swdepot: state=${1:#present|latest|absent} name=${2:# REQUIRED} depot=${3}

snippet rpm_key
	rpm_key: key=${1:# REQUIRED} state=${2:#present|absent} validate_certs=${3:#yes|no}

snippet portage
	portage:
		nodeps=${1:#yes}
		onlydeps=${2:#yes}
		newuse=${3:#yes}
		package=${4}
		oneshot=${5:#yes}
		update=${6:#yes}
		deep=${7:#yes}
		quiet=${8:#yes}
		sync=${9:#yes|web}
		state=${10:#present|installed|emerged|absent|removed|unmerged}
		depclean=${11:#yes}
		noreplace=${12:#yes}
		verbose=${13:#yes}

snippet macports
	macports: name=${1:# REQUIRED} state=${2:#present|absent|active|inactive} update_cache=${3:#yes|no}

snippet pacman
	pacman: recurse=${1:#yes|no} state=${2:#present|absent} update_cache=${3:#yes|no} name=${4}

snippet apt_rpm
	apt_rpm: pkg=${1:# REQUIRED} state=${2:#absent|present} update_cache=${3:#yes|no}

snippet urpmi
	urpmi:
		pkg=${1:# REQUIRED}
		force=${2:#yes|no}
		state=${3:#absent|present}
		no-suggests=${4:#yes|no}
		update_cache=${5:#yes|no}

snippet add_host
	add_host: name=${1:# REQUIRED} groups=${2}

snippet group_by
	group_by: key=${1:# REQUIRED}

snippet win_feature
	win_feature:
		name=${1:# REQUIRED}
		include_management_tools=${2:#True|False}
		include_sub_features=${3:#True|False}
		state=${4:#present|absent}
		restart=${5:#True|False}

snippet win_stat
	win_stat: path=${1:# REQUIRED} get_md5=${2:true}

snippet win_service
	win_service: name=${1:# REQUIRED} start_mode=${2:#auto|manual|disabled} state=${3:#started|stopped|restarted}

snippet win_get_url
	win_get_url: url=${1:# REQUIRED} dest=${2:true}

snippet win_group
	win_group: name=${1:# REQUIRED} state=${2:#present|absent} description=${3}

snippet slurp
	slurp: src=${1:# REQUIRED}

snippet win_user
	win_user: password=${1:# REQUIRED} name=${2:# REQUIRED} state=${3:#present|absent}

snippet win_ping
	win_ping: data=${1:pong}

snippet setup
	setup: filter=${1:*} fact_path=${2:/etc/ansible/facts.d}

snippet win_msi
	win_msi: path=${1:# REQUIRED} creates=${2} state=${3:#present|absent}

snippet mail
	mail:
		subject=${1:# REQUIRED}
		body=${2:$subject}
		from=${3:root}
		to=${4:root}
		headers=${5}
		cc=${6}
		charset=${7:us-ascii}
		bcc=${8}
		attach=${9}
		host=${10:localhost}
		port=${11:25}

snippet sns
	sns:
		topic=${1:# REQUIRED}
		msg=${2:# REQUIRED}
		aws_secret_key=${3}
		aws_access_key=${4}
		http=${5}
		sqs=${6}
		region=${7}
		sms=${8}
		https=${9}
		email=${10}
		subject=${11}

snippet twilio
	twilio:
		msg=${1:# REQUIRED}
		auth_token=${2:# REQUIRED}
		from_number=${3:# REQUIRED}
		to_number=${4:# REQUIRED}
		account_sid=${5:# REQUIRED}

snippet osx_say
	osx_say: msg=${1:# REQUIRED} voice=${2}

snippet grove
	grove:
		message=${1:# REQUIRED}
		channel_token=${2:# REQUIRED}
		service=${3:ansible}
		url=${4}
		icon_url=${5}
		validate_certs=${6:#yes|no}

snippet hipchat
	hipchat:
		room=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from=${4:ansible}
		color=${5:#yellow|red|green|purple|gray|random}
		msg_format=${6:#text|html}
		api=${7:https://api.hipchat.com/v1/rooms/message}
		notify=${8:#yes|no}
		validate_certs=${9:#yes|no}

snippet jabber
	jabber:
		to=${1:# REQUIRED}
		user=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		password=${4:# REQUIRED}
		host=${5}
		encoding=${6}
		port=${7:5222}

snippet slack
	slack:
		domain=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		username=${4:ansible}
		icon_url=${5}
		parse=${6:#full|none}
		icon_emoji=${7}
		link_names=${8:#1|0}
		validate_certs=${9:#yes|no}
		channel=${10}

snippet flowdock
	flowdock:
		type=${1:#inbox|chat}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from_name=${4}
		from_address=${5}
		tags=${6}
		external_user_name=${7}
		project=${8}
		source=${9}
		link=${10}
		reply_to=${11}
		subject=${12}
		validate_certs=${13:#yes|no}

snippet typetalk
	typetalk: topic=${1:# REQUIRED} client_secret=${2:# REQUIRED} client_id=${3:# REQUIRED} msg=${4:# REQUIRED}

snippet irc
	irc:
		msg=${1:# REQUIRED}
		channel=${2:# REQUIRED}
		key=${3}
		color=${4:#none|yellow|red|green|blue|black}
		server=${5:localhost}
		nick=${6:ansible}
		passwd=${7}
		timeout=${8:30}
		port=${9:6667}

snippet campfire
	campfire:
		msg=${1:# REQUIRED}
		token=${2:# REQUIRED}
		subscription=${3:# REQUIRED}
		room=${4:# REQUIRED}
		notify=${5:#56k|bell|bezos|bueller|clowntown|cottoneyejoe|crickets|dadgummit|dangerzone|danielsan|deeper|drama|greatjob|greyjoy|guarantee|heygirl|horn|horror|inconceivable|live|loggins|makeitso|noooo|nyan|ohmy|ohyeah|pushit|rimshot|rollout|rumble|sax|secret|sexyback|story|tada|tmyk|trololo|trombone|unix|vuvuzela|what|whoomp|yeah|yodel}

snippet nexmo
	nexmo:
		src=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		api_secret=${3:# REQUIRED}
		api_key=${4:# REQUIRED}
		msg=${5:# REQUIRED}
		validate_certs=${6:#yes|no}

snippet mqtt
	mqtt:
		topic=${1:# REQUIRED}
		payload=${2:# REQUIRED}
		username=${3}
		qos=${4:#0|1|2}
		port=${5:1883}
		server=${6:localhost}
		client_id=${7:hostname + pid}
		retain=${8:false}
		password=${9}

snippet async_status
	async_status: jid=${1:# REQUIRED} mode=${2:#status|cleanup}

snippet apache2_module
	apache2_module: name=${1:# REQUIRED} state=${2:#present|absent}

snippet jira
	jira:
		username=${1:# REQUIRED}
		uri=${2:# REQUIRED}
		operation=${3:#create|comment|edit|fetch|transition}
		password=${4:# REQUIRED}
		comment=${5}
		description=${6}
		fields=${7}
		summary=${8}
		project=${9}
		assignee=${10}
		status=${11}
		issuetype=${12}
		issue=${13}

snippet ejabberd_user
	ejabberd_user:
		username=${1:# REQUIRED}
		host=${2:# REQUIRED}
		password=${3}
		logging=${4:#true|false|yes|no}
		state=${5:#present|absent}

snippet jboss
	jboss: deployment=${1:# REQUIRED} src=${2} deploy_path=${3:/var/lib/jbossas/standalone/deployments} state=${4:#present|absent}

snippet django_manage
	django_manage:
		app_path=${1:# REQUIRED}
		command=${2:#cleanup|collectstatic|flush|loaddata|migrate|runfcgi|syncdb|test|validate}
		virtualenv=${3}
		settings=${4}
		pythonpath=${5}
		database=${6}
		apps=${7}
		cache_table=${8}
		merge=${9}
		skip=${10}
		link=${11}
		fixtures=${12}
		failfast=${13:#yes|no}

snippet supervisorctl
	supervisorctl:
		state=${1:#present|started|stopped|restarted}
		name=${2:# REQUIRED}
		username=${3}
		supervisorctl_path=${4}
		password=${5}
		config=${6}
		server_url=${7}

snippet htpasswd
	htpasswd:
		name=${1:# REQUIRED}
		path=${2:# REQUIRED}
		state=${3:#present|absent}
		create=${4:#yes|no}
		password=${5}
		crypt_scheme=${6:#apr_md5_crypt|des_crypt|ldap_sha1|plaintext}

snippet rabbitmq_parameter
	rabbitmq_parameter:
		name=${1:# REQUIRED}
		component=${2:# REQUIRED}
		node=${3:rabbit}
		vhost=${4:/}
		state=${5:#present|absent}
		value=${6}

snippet rabbitmq_policy
	rabbitmq_policy:
		name=${1:# REQUIRED}
		tags=${2:# REQUIRED}
		pattern=${3:# REQUIRED}
		node=${4:rabbit}
		priority=${5:0}
		state=${6:#present|absent}
		vhost=${7:/}

snippet rabbitmq_plugin
	rabbitmq_plugin: names=${1:# REQUIRED} state=${2:#enabled|disabled} new_only=${3:#yes|no} prefix=${4}

snippet rabbitmq_user
	rabbitmq_user:
		user=${1:# REQUIRED}
		node=${2:rabbit}
		force=${3:#yes|no}
		tags=${4}
		read_priv=${5:^$}
		write_priv=${6:^$}
		state=${7:#present|absent}
		configure_priv=${8:^$}
		vhost=${9:/}
		password=${10}

snippet rabbitmq_vhost
	rabbitmq_vhost: name=${1:# REQUIRED} node=${2:rabbit} tracing=${3:#yes|no} state=${4:#present|absent}

snippet raw
	raw: ${1} executable=${2}

snippet script
	script: ${1} creates=${2} removes=${3}

snippet command
	command: ${1}
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet shell
	shell: ${1}
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet stat
	stat: path=${1:# REQUIRED} get_md5=${2:true} follow=${3:false}

snippet acl
	acl:
		name=${1:# REQUIRED}
		default=${2:#yes|no}
		entity=${3}
		state=${4:#query|present|absent}
		follow=${5:#yes|no}
		etype=${6:#user|group|mask|other}
		entry=${7}
		permissions=${8}

snippet unarchive
	unarchive: dest=${1:# REQUIRED} src=${2:# REQUIRED} copy=${3:#yes|no} creates=${4}

snippet fetch
	fetch:
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		validate_md5=${3:#yes|no}
		fail_on_missing=${4:#yes|no}
		flat=${5}

snippet file
	file:
		path=${1:[]}
		src=${2}
		serole=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:#yes|no}
		setype=${8}
		group=${9}
		state=${10:#file|link|directory|hard|touch|absent}
		mode=${11}
		owner=${12}

snippet xattr
	xattr:
		name=${1:# REQUIRED}
		key=${2}
		follow=${3:#yes|no}
		state=${4:#read|present|all|keys|absent}
		value=${5}

snippet copy
	copy:
		dest=${1:# REQUIRED}
		src=${2}
		directory_mode=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:false}
		serole=${8}
		content=${9}
		setype=${10}
		mode=${11}
		owner=${12}
		group=${13}
		validate=${14}
		backup=${15:#yes|no}

snippet synchronize
	synchronize:
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		dirs=${3:#yes|no}
		links=${4:#yes|no}
		copy_links=${5:#yes|no}
		compress=${6:#yes|no}
		rsync_timeout=${7:0}
		rsync_opts=${8}
		owner=${9:#yes|no}
		set_remote_user=${10:true}
		rsync_path=${11}
		recursive=${12:#yes|no}
		group=${13:#yes|no}
		existing_only=${14:#yes|no}
		archive=${15:#yes|no}
		checksum=${16:#yes|no}
		times=${17:#yes|no}
		perms=${18:#yes|no}
		mode=${19:#push|pull}
		dest_port=${20:22}
		delete=${21:#yes|no}

snippet template
	template: dest=${1:# REQUIRED} src=${2:# REQUIRED} validate=${3} backup=${4:#yes|no}

snippet bigip_node
	bigip_node:
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		name=${6}
		partition=${7:common}
		description=${8}

snippet bigip_monitor_http
	bigip_monitor_http:
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		receive_disable=${3:# REQUIRED}
		name=${4:# REQUIRED}
		receive=${5:# REQUIRED}
		send=${6:# REQUIRED}
		server=${7:# REQUIRED}
		interval=${8}
		parent=${9:http}
		ip=${10}
		port=${11}
		partition=${12:common}
		state=${13:#present|absent}
		time_until_up=${14}
		timeout=${15}
		parent_partition=${16:common}

snippet arista_vlan
	arista_vlan: vlan_id=${1:# REQUIRED} state=${2:#present|absent} logging=${3:#true|false|yes|no} name=${4}

snippet bigip_monitor_tcp
	bigip_monitor_tcp:
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		name=${3:# REQUIRED}
		receive=${4:# REQUIRED}
		send=${5:# REQUIRED}
		server=${6:# REQUIRED}
		interval=${7}
		parent=${8:#tcp|tcp_echo|tcp_half_open}
		ip=${9}
		port=${10}
		partition=${11:common}
		state=${12:#present|absent}
		time_until_up=${13}
		timeout=${14}
		parent_partition=${15:common}
		type=${16:#TTYPE_TCP|TTYPE_TCP_ECHO|TTYPE_TCP_HALF_OPEN}

snippet openvswitch_bridge
	openvswitch_bridge: bridge=${1:# REQUIRED} state=${2:#present|absent} timeout=${3:5}

snippet dnsimple
	dnsimple:
		solo=${1}
		domain=${2}
		account_email=${3}
		record_ids=${4}
		value=${5}
		priority=${6}
		record=${7}
		state=${8:#present|absent}
		ttl=${9:3600 (one hour)}
		type=${10:#A|ALIAS|CNAME|MX|SPF|URL|TXT|NS|SRV|NAPTR|PTR|AAAA|SSHFP|HINFO|POOL}
		account_api_token=${11}

snippet dnsmadeeasy
	dnsmadeeasy:
		domain=${1:# REQUIRED}
		account_secret=${2:# REQUIRED}
		account_key=${3:# REQUIRED}
		state=${4:#present|absent}
		record_name=${5}
		record_ttl=${6:1800}
		record_type=${7:#A|AAAA|CNAME|HTTPRED|MX|NS|PTR|SRV|TXT}
		record_value=${8}
		validate_certs=${9:#yes|no}

snippet openvswitch_port
	openvswitch_port: bridge=${1:# REQUIRED} port=${2:# REQUIRED} state=${3:#present|absent} timeout=${4:5}

snippet bigip_pool_member
	bigip_pool_member:
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		port=${6:# REQUIRED}
		pool=${7:# REQUIRED}
		ratio=${8}
		description=${9}
		connection_limit=${10}
		partition=${11:common}
		rate_limit=${12}

snippet arista_lag
	arista_lag:
		interface_id=${1:# REQUIRED}
		lacp=${2:#active|passive|off}
		state=${3:#present|absent}
		minimum_links=${4}
		logging=${5:#true|false|yes|no}
		links=${6}

snippet arista_interface
	arista_interface:
		interface_id=${1:# REQUIRED}
		duplex=${2:#auto|half|full}
		logging=${3:#true|false|yes|no}
		description=${4}
		admin=${5:#up|down}
		speed=${6:#auto|100m|1g|10g}
		mtu=${7:1500}

snippet bigip_facts
	bigip_facts:
		include=${1:#address_class|certificate|client_ssl_profile|device_group|interface|key|node|pool|rule|self_ip|software|system_info|traffic_group|trunk|virtual_address|virtual_server|vlan}
		user=${2:# REQUIRED}
		password=${3:# REQUIRED}
		server=${4:# REQUIRED}
		filter=${5}
		session=${6:true}

snippet arista_l2interface
	arista_l2interface:
		interface_id=${1:# REQUIRED}
		state=${2:#present|absent}
		logging=${3:#true|false|yes|no}
		tagged_vlans=${4}
		vlan_tagging=${5:#enable|disable}
		untagged_vlan=${6:default}

snippet bigip_pool
	bigip_pool:
		name=${1:# REQUIRED}
		server=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		lb_method=${5:#round_robin|ratio_member|least_connection_member|observed_member|predictive_member|ratio_node_address|least_connection_node_address|fastest_node_address|observed_node_address|predictive_node_address|dynamic_ratio|fastest_app_response|least_sessions|dynamic_ratio_member|l3_addr|unknown|weighted_least_connection_member|weighted_least_connection_node_address|ratio_session|ratio_least_connection_member|ratio_least_connection_node_address}
		quorum=${6}
		partition=${7:common}
		slow_ramp_time=${8}
		state=${9:#present|absent}
		service_down_action=${10:#none|reset|drop|reselect}
		port=${11}
		host=${12}
		monitors=${13}
		monitor_type=${14:#and_list|m_of_n}

snippet netscaler
	netscaler:
		name=${1:hostname}
		nsc_host=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		type=${5:#server|service}
		nsc_protocol=${6:https}
		action=${7:#enable|disable}
		validate_certs=${8:#yes|no}

snippet accelerate
	accelerate:
		timeout=${1:300}
		minutes=${2:30}
		port=${3:5099}
		multi_key=${4:false}
		ipv6=${5:false}

snippet debug
	debug: msg=${1:hello world!} var=${2}

snippet wait_for
	wait_for:
		delay=${1:0}
		state=${2:#present|started|stopped|absent}
		timeout=${3:300}
		search_regex=${4}
		path=${5}
		host=${6:127.0.0.1}
		port=${7}

snippet assert
	assert: that=${1:# REQUIRED}

snippet set_fact
	set_fact: key_value=${1:# REQUIRED}

snippet pause
	pause: seconds=${1} minutes=${2} prompt=${3}

snippet include_vars
	include_vars: ${1}

snippet fail
	fail: msg=${1:'failed as requested from task'}

snippet fireball
	fireball: minutes=${1:30} port=${2:5099}

snippet mount
	mount:
		src=${1:# REQUIRED}
		name=${2:# REQUIRED}
		fstype=${3:# REQUIRED}
		state=${4:#present|absent|mounted|unmounted}
		dump=${5}
		fstab=${6:/etc/fstab}
		passno=${7}
		opts=${8}

snippet seboolean
	seboolean: state=${1:#yes|no} name=${2:# REQUIRED} persistent=${3:#yes|no}

snippet at
	at:
		count=${1:# REQUIRED}
		units=${2:#minutes|hours|days|weeks}
		state=${3:#present|absent}
		command=${4}
		unique=${5:false}
		script_file=${6}

snippet authorized_key
	authorized_key:
		user=${1:# REQUIRED}
		key=${2:# REQUIRED}
		key_options=${3}
		state=${4:#present|absent}
		path=${5:(homedir)+/.ssh/authorized_keys}
		manage_dir=${6:#yes|no}

snippet locale_gen
	locale_gen: name=${1:# REQUIRED} state=${2:#present|absent}

snippet user
	user:
		name=${1:# REQUIRED}
		comment=${2}
		ssh_key_bits=${3:2048}
		update_password=${4:#always|on_create}
		non_unique=${5:#yes|no}
		force=${6:#yes|no}
		ssh_key_type=${7:rsa}
		ssh_key_passphrase=${8}
		groups=${9}
		home=${10}
		move_home=${11:#yes|no}
		password=${12}
		generate_ssh_key=${13:#yes|no}
		append=${14:#yes|no}
		uid=${15}
		ssh_key_comment=${16:ansible-generated}
		group=${17}
		createhome=${18:#yes|no}
		system=${19:#yes|no}
		remove=${20:#yes|no}
		state=${21:#present|absent}
		ssh_key_file=${22:$home/.ssh/id_rsa}
		login_class=${23}
		shell=${24}

snippet cron
	cron:
		name=${1}
		hour=${2:*}
		job=${3}
		cron_file=${4}
		reboot=${5:#yes|no}
		month=${6:*}
		state=${7:#present|absent}
		special_time=${8:#reboot|yearly|annually|monthly|weekly|daily|hourly}
		user=${9:root}
		backup=${10:false}
		day=${11:*}
		minute=${12:*}
		weekday=${13:*}

snippet lvol
	lvol:
		lv=${1:# REQUIRED}
		vg=${2:# REQUIRED}
		state=${3:#present|absent}
		force=${4:#yes|no}
		size=${5}

snippet debconf
	debconf:
		name=${1:# REQUIRED}
		value=${2}
		vtype=${3:#string|boolean|select|multiselect|note|text|password|title}
		question=${4}
		unseen=${5:false}

snippet firewalld
	firewalld:
		state=${1:enabled}
		permanent=${2:true}
		zone=${3:#work|drop|internal|external|trusted|home|dmz|public|block}
		service=${4}
		timeout=${5:0}
		rich_rule=${6}
		port=${7}

snippet capabilities
	capabilities: capability=${1:# REQUIRED} path=${2:# REQUIRED} state=${3:#present|absent}

snippet group
	group: name=${1:# REQUIRED} state=${2:#present|absent} gid=${3} system=${4:#yes|no}

snippet modprobe
	modprobe: name=${1:# REQUIRED} state=${2:#present|absent} params=${3}

snippet alternatives
	alternatives: path=${1:# REQUIRED} name=${2:# REQUIRED} link=${3}

snippet filesystem
	filesystem: dev=${1:# REQUIRED} fstype=${2:# REQUIRED} force=${3:#yes|no} opts=${4}

snippet sysctl
	sysctl:
		name=${1:# REQUIRED}
		reload=${2:#yes|no}
		state=${3:#present|absent}
		sysctl_set=${4:#yes|no}
		ignoreerrors=${5:#yes|no}
		sysctl_file=${6:/etc/sysctl.conf}
		value=${7}

snippet hostname
	hostname: name=${1:# REQUIRED}

snippet kernel_blacklist
	kernel_blacklist: name=${1:# REQUIRED} blacklist_file=${2} state=${3:#present|absent}

snippet lvg
	lvg:
		vg=${1:# REQUIRED}
		vg_options=${2}
		pvs=${3}
		force=${4:#yes|no}
		pesize=${5:4}
		state=${6:#present|absent}

snippet ufw
	ufw:
		insert=${1}
		direction=${2:#in|out|incoming|outgoing}
		from_port=${3}
		logging=${4:#on|off|low|medium|high|full}
		log=${5:#yes|no}
		proto=${6:#any|tcp|udp|ipv6|esp|ah}
		to_port=${7}
		from_ip=${8:any}
		rule=${9:#allow|deny|reject|limit}
		name=${10}
		policy=${11:#allow|deny|reject}
		state=${12:#enabled|disabled|reloaded|reset}
		interface=${13}
		to_ip=${14:any}
		delete=${15:#yes|no}

snippet service
	service:
		name=${1:# REQUIRED}
		state=${2:#started|stopped|restarted|reloaded}
		sleep=${3}
		runlevel=${4:default}
		pattern=${5}
		enabled=${6:#yes|no}
		arguments=${7}

snippet zfs
	zfs:
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		setuid=${3:#on|off}
		zoned=${4:#on|off}
		primarycache=${5:#all|none|metadata}
		logbias=${6:#latency|throughput}
		sync=${7:#on|off}
		copies=${8:#1|2|3}
		sharenfs=${9}
		sharesmb=${10}
		canmount=${11:#on|off|noauto}
		mountpoint=${12}
		casesensitivity=${13:#sensitive|insensitive|mixed}
		utf8only=${14:#on|off}
		xattr=${15:#on|off}
		compression=${16:#on|off|lzjb|gzip|gzip-1|gzip-2|gzip-3|gzip-4|gzip-5|gzip-6|gzip-7|gzip-8|gzip-9|lz4|zle}
		shareiscsi=${17:#on|off}
		aclmode=${18:#discard|groupmask|passthrough}
		exec=${19:#on|off}
		dedup=${20:#on|off}
		aclinherit=${21:#discard|noallow|restricted|passthrough|passthrough-x}
		readonly=${22:#on|off}
		recordsize=${23}
		jailed=${24:#on|off}
		secondarycache=${25:#all|none|metadata}
		refquota=${26}
		quota=${27}
		volsize=${28}
		vscan=${29:#on|off}
		reservation=${30}
		atime=${31:#on|off}
		normalization=${32:#none|formC|formD|formKC|formKD}
		volblocksize=${33}
		checksum=${34:#on|off|fletcher2|fletcher4|sha256}
		devices=${35:#on|off}
		nbmand=${36:#on|off}
		refreservation=${37}
		snapdir=${38:#hidden|visible}

snippet open_iscsi
	open_iscsi:
		auto_node_startup=${1:#True|False}
		target=${2}
		show_nodes=${3:#True|False}
		node_auth=${4:chap}
		node_pass=${5}
		discover=${6:#True|False}
		portal=${7}
		login=${8:#True|False}
		node_user=${9}
		port=${10:3260}

snippet selinux
	selinux: state=${1:#enforcing|permissive|disabled} policy=${2} conf=${3:/etc/selinux/config}

snippet hg
	hg:
		repo=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		purge=${3:#yes|no}
		executable=${4}
		force=${5:#yes|no}
		revision=${6:default}

snippet git
	git:
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		executable=${3}
		remote=${4:origin}
		recursive=${5:#yes|no}
		reference=${6}
		accept_hostkey=${7:#yes|no}
		update=${8:#yes|no}
		ssh_opts=${9}
		depth=${10}
		version=${11:head}
		bare=${12:#yes|no}
		force=${13:#yes|no}
		key_file=${14}

snippet bzr
	bzr:
		dest=${1:# REQUIRED}
		name=${2:# REQUIRED}
		executable=${3}
		version=${4:head}
		force=${5:#yes|no}

snippet subversion
	subversion:
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		username=${3}
		executable=${4}
		force=${5:#yes|no}
		export=${6:#yes|no}
		password=${7}
		revision=${8:head}

snippet github_hooks
	github_hooks:
		repo=${1:# REQUIRED}
		oauthkey=${2:# REQUIRED}
		user=${3:# REQUIRED}
		action=${4:#create|cleanall}
		validate_certs=${5:#yes|no}
		hookurl=${6}

snippet digital_ocean_sshkey
	digital_ocean_sshkey:
		state=${1:#present|absent}
		name=${2}
		client_id=${3}
		api_key=${4}
		id=${5}
		ssh_pub_key=${6}

snippet ovirt
	ovirt:
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		url=${3:# REQUIRED}
		instance_name=${4:# REQUIRED}
		instance_mem=${5}
		instance_cores=${6:1}
		instance_cpus=${7:1}
		image=${8}
		instance_disksize=${9}
		instance_nic=${10}
		instance_network=${11:rhevm}
		sdomain=${12}
		instance_os=${13}
		zone=${14}
		disk_alloc=${15:#thin|preallocated}
		region=${16}
		instance_type=${17:#server|desktop}
		state=${18:#present|absent|shutdown|started|restarted}
		resource_type=${19:#new|template}
		disk_int=${20:#virtio|ide}

snippet ec2_ami
	ec2_ami:
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		security_token=${5}
		delete_snapshot=${6}
		region=${7}
		state=${8:present}
		instance_id=${9}
		image_id=${10}
		no_reboot=${11:#yes|no}
		wait_timeout=${12:300}
		ec2_url=${13}
		wait=${14:#yes|no}
		validate_certs=${15:#yes|no}
		description=${16}

snippet ec2_metric_alarm
	ec2_metric_alarm:
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		comparison=${4}
		alarm_actions=${5}
		ok_actions=${6}
		security_token=${7}
		evaluation_periods=${8}
		metric=${9}
		description=${10}
		namespace=${11}
		period=${12}
		ec2_url=${13}
		profile=${14}
		insufficient_data_actions=${15}
		statistic=${16}
		threshold=${17}
		aws_access_key=${18}
		validate_certs=${19:#yes|no}
		unit=${20}
		dimensions=${21}

snippet elasticache
	elasticache:
		name=${1:# REQUIRED}
		state=${2:#present|absent|rebooted}
		engine=${3:memcached}
		aws_secret_key=${4}
		cache_port=${5:11211}
		security_group_ids=${6:['default']}
		cache_engine_version=${7:1.4.14}
		region=${8}
		num_nodes=${9}
		node_type=${10:cache.m1.small}
		cache_security_groups=${11:['default']}
		hard_modify=${12:#yes|no}
		aws_access_key=${13}
		zone=${14}
		wait=${15:#yes|no}

snippet ec2_lc
	ec2_lc:
		name=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		spot_price=${7}
		security_token=${8}
		key_name=${9}
		region=${10}
		user_data=${11}
		image_id=${12}
		volumes=${13}
		ec2_url=${14}
		instance_monitoring=${15:false}
		validate_certs=${16:#yes|no}
		security_groups=${17}

snippet quantum_router_gateway
	quantum_router_gateway:
		router_name=${1:# REQUIRED}
		login_tenant_name=${2:yes}
		login_password=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet quantum_floating_ip_associate
	quantum_floating_ip_associate:
		instance_name=${1:# REQUIRED}
		login_tenant_name=${2:true}
		login_password=${3:yes}
		login_username=${4:admin}
		ip_address=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2_key
	ec2_key:
		name=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		key_material=${7}
		state=${8:present}
		wait_timeout=${9:300}
		ec2_url=${10}
		validate_certs=${11:#yes|no}
		wait=${12:false}

snippet ec2_ami_search
	ec2_ami_search:
		release=${1:# REQUIRED}
		distro=${2:#ubuntu}
		stream=${3:#server|desktop}
		virt=${4:#paravirtual|hvm}
		region=${5:#ap-northeast-1|ap-southeast-1|ap-southeast-2|eu-west-1|sa-east-1|us-east-1|us-west-1|us-west-2}
		arch=${6:#i386|amd64}
		store=${7:#ebs|instance-store}

snippet gce_lb
	gce_lb:
		httphealthcheck_host=${1}
		protocol=${2:#tcp|udp}
		pem_file=${3}
		members=${4}
		httphealthcheck_port=${5:80}
		httphealthcheck_name=${6}
		name=${7}
		external_ip=${8}
		service_account_email=${9}
		region=${10}
		httphealthcheck_unhealthy_count=${11:2}
		httphealthcheck_healthy_count=${12:2}
		httphealthcheck_path=${13:/}
		port_range=${14}
		state=${15:#active|present|absent|deleted}
		httphealthcheck_timeout=${16:5}
		project_id=${17}
		httphealthcheck_interval=${18:5}

snippet rax_files_objects
	rax_files_objects:
		container=${1:# REQUIRED}
		username=${2}
		src=${3}
		dest=${4}
		region=${5:dfw}
		expires=${6}
		verify_ssl=${7}
		state=${8:#present|absent}
		clear_meta=${9:#yes|no}
		meta=${10}
		env=${11}
		credentials=${12}
		api_key=${13}
		type=${14:#file|meta}
		method=${15:#get|put|delete}
		structure=${16:#True|no}

snippet quantum_router
	quantum_router:
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		admin_state_up=${6:true}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet azure
	azure:
		image=${1:# REQUIRED}
		storage_account=${2:# REQUIRED}
		name=${3:# REQUIRED}
		location=${4:# REQUIRED}
		role_size=${5:small}
		virtual_network_name=${6}
		wait_timeout_redirects=${7:300}
		wait_timeout=${8:600}
		user=${9}
		password=${10}
		wait=${11:#yes|no}
		management_cert_path=${12}
		hostname=${13}
		ssh_cert_path=${14}
		state=${15:present}
		subscription_id=${16}
		endpoints=${17:22}

snippet gce_net
	gce_net:
		fwname=${1}
		name=${2}
		src_range=${3}
		allowed=${4}
		src_tags=${5}
		pem_file=${6}
		state=${7:#active|present|absent|deleted}
		service_account_email=${8}
		ipv4_range=${9}
		project_id=${10}

snippet rds_subnet_group
	rds_subnet_group:
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		subnets=${5}
		aws_access_key=${6}
		description=${7}

snippet rax_clb_nodes
	rax_clb_nodes:
		load_balancer_id=${1:# REQUIRED}
		username=${2}
		weight=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		wait_timeout=${7:30}
		condition=${8:#enabled|disabled|draining}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		type=${13:#primary|secondary}
		port=${14}
		node_id=${15}
		wait=${16:#yes|no}

snippet docker_image
	docker_image:
		name=${1:# REQUIRED}
		state=${2:#present|absent|build}
		tag=${3:latest}
		nocache=${4:false}
		path=${5}
		docker_url=${6:unix://var/run/docker.sock}
		timeout=${7:600}

snippet rax_dns
	rax_dns:
		comment=${1}
		username=${2}
		name=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		ttl=${8:3600}
		credentials=${9}
		api_key=${10}
		email=${11}

snippet ec2_elb
	ec2_elb:
		instance_id=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		security_token=${6}
		region=${7}
		wait_timeout=${8:0}
		ec2_url=${9}
		wait=${10:#yes|no}
		validate_certs=${11:#yes|no}
		enable_availability_zone=${12:#yes|no}
		ec2_elbs=${13}

snippet digital_ocean
	digital_ocean:
		unique_name=${1:#yes|no}
		virtio=${2:#yes|no}
		region_id=${3}
		backups_enabled=${4:#yes|no}
		image_id=${5}
		wait_timeout=${6:300}
		client_id=${7}
		ssh_pub_key=${8}
		wait=${9:#yes|no}
		name=${10}
		size_id=${11}
		id=${12}
		state=${13:#present|active|absent|deleted}
		command=${14:#droplet|ssh}
		ssh_key_ids=${15}
		private_networking=${16:#yes|no}
		api_key=${17}

snippet keystone_user
	keystone_user:
		endpoint=${1:http://127.0.0.1:35357/v2.0/}
		description=${2}
		login_user=${3:admin}
		token=${4}
		login_tenant_name=${5}
		state=${6:#present|absent}
		role=${7}
		user=${8}
		login_password=${9:yes}
		password=${10}
		email=${11}
		tenant=${12}

snippet rax_scaling_policy
	rax_scaling_policy:
		name=${1:# REQUIRED}
		scaling_group=${2:# REQUIRED}
		policy_type=${3:#webhook|schedule}
		username=${4}
		is_percent=${5:false}
		env=${6}
		region=${7:dfw}
		verify_ssl=${8}
		cron=${9}
		desired_capacity=${10}
		state=${11:#present|absent}
		cooldown=${12}
		at=${13}
		credentials=${14}
		api_key=${15}
		change=${16}

snippet rax_meta
	rax_meta:
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		meta=${8}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		id=${13}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}

snippet quantum_subnet
	quantum_subnet:
		login_password=${1:true}
		login_username=${2:admin}
		cidr=${3:# REQUIRED}
		network_name=${4:# REQUIRED}
		name=${5:# REQUIRED}
		login_tenant_name=${6:true}
		region_name=${7}
		tenant_name=${8}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}
		allocation_pool_end=${10}
		enable_dhcp=${11:true}
		dns_nameservers=${12}
		state=${13:#present|absent}
		allocation_pool_start=${14}
		gateway_ip=${15}
		ip_version=${16:4}

snippet vsphere_guest
	vsphere_guest:
		password=${1:# REQUIRED}
		guest=${2:# REQUIRED}
		user=${3:# REQUIRED}
		vcenter_hostname=${4:# REQUIRED}
		resource_pool=${5}
		vm_hw_version=${6}
		force=${7:#yes|no}
		vm_disk=${8}
		esxi=${9}
		vm_nic=${10}
		vm_hardware=${11}
		cluster=${12}
		state=${13:#present|powered_on|absent|powered_on|restarted|reconfigured}
		vmware_guest_facts=${14}
		vm_extra_config=${15}

snippet rax_facts
	rax_facts:
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		env=${8}
		address=${9}
		credentials=${10}
		api_key=${11}
		id=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet rax_dns_record
	rax_dns_record:
		name=${1:# REQUIRED}
		data=${2:# REQUIRED}
		type=${3:#A|AAAA|CNAME|MX|NS|SRV|TXT|PTR}
		comment=${4}
		username=${5}
		domain=${6}
		region=${7:dfw}
		verify_ssl=${8}
		server=${9}
		priority=${10}
		state=${11:#present|absent}
		env=${12}
		ttl=${13:3600}
		credentials=${14}
		api_key=${15}
		loadbalancer=${16}

snippet rax_network
	rax_network:
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		label=${6}
		state=${7:#present|absent}
		env=${8}
		tenant_name=${9}
		credentials=${10}
		cidr=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_tag
	ec2_tag:
		resource=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		state=${7:#present|absent|list}
		ec2_url=${8}
		validate_certs=${9:#yes|no}

snippet nova_keypair
	nova_keypair:
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		public_key=${5}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2
	ec2:
		image=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		ramdisk=${3}
		kernel=${4}
		volumes=${5}
		count_tag=${6}
		monitoring=${7}
		vpc_subnet_id=${8}
		user_data=${9}
		instance_ids=${10}
		wait_timeout=${11:300}
		profile=${12}
		private_ip=${13}
		assign_public_ip=${14}
		spot_price=${15}
		id=${16}
		source_dest_check=${17:true}
		wait=${18:#yes|no}
		count=${19:1}
		spot_wait_timeout=${20:600}
		aws_access_key=${21}
		group=${22}
		instance_profile_name=${23}
		zone=${24}
		exact_count=${25}
		ebs_optimized=${26:false}
		security_token=${27}
		state=${28:#present|absent|running|stopped}
		aws_secret_key=${29}
		ec2_url=${30}
		placement_group=${31}
		key_name=${32}
		instance_tags=${33}
		group_id=${34}
		validate_certs=${35:#yes|no}
		region=${36}

snippet quantum_network
	quantum_network:
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		provider_network_type=${6}
		admin_state_up=${7:true}
		router_external=${8:false}
		tenant_name=${9}
		provider_physical_network=${10}
		state=${11:#present|absent}
		auth_url=${12:http://127.0.0.1:35357/v2.0/}
		shared=${13:false}
		provider_segmentation_id=${14}

snippet rax_cbs
	rax_cbs:
		size=${1:100}
		volume_type=${2:#SATA|SSD}
		state=${3:#present|absent}
		name=${4:# REQUIRED}
		username=${5}
		api_key=${6}
		tenant_name=${7}
		description=${8}
		identity_type=${9:rackspace}
		tenant_id=${10}
		region=${11:dfw}
		auth_endpoint=${12:https://identity.api.rackspacecloud.com/v2.0/}
		verify_ssl=${13}
		wait_timeout=${14:300}
		meta=${15}
		env=${16}
		snapshot_id=${17}
		credentials=${18}
		wait=${19:#yes|no}

snippet rax_queue
	rax_queue:
		username=${1}
		name=${2}
		region=${3:dfw}
		verify_ssl=${4}
		state=${5:#present|absent}
		env=${6}
		credentials=${7}
		api_key=${8}

snippet cloudformation
	cloudformation:
		stack_name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		template=${3:# REQUIRED}
		aws_secret_key=${4}
		aws_access_key=${5}
		disable_rollback=${6:#true|false}
		tags=${7}
		region=${8}
		template_parameters=${9:{}}

snippet rax_identity
	rax_identity:
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		tenant_name=${8}
		credentials=${9}
		api_key=${10}
		auth_endpoint=${11:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_eip
	ec2_eip:
		aws_secret_key=${1}
		instance_id=${2}
		aws_access_key=${3}
		security_token=${4}
		reuse_existing_ip_allowed=${5:false}
		region=${6}
		public_ip=${7}
		state=${8:#present|absent}
		in_vpc=${9:false}
		profile=${10}
		ec2_url=${11}
		validate_certs=${12:#yes|no}
		wait_timeout=${13:300}

snippet gc_storage
	gc_storage:
		gcs_secret_key=${1:# REQUIRED}
		bucket=${2:# REQUIRED}
		gcs_access_key=${3:# REQUIRED}
		mode=${4:#get|put|get_url|get_str|delete|create}
		src=${5}
		force=${6:true}
		permission=${7:private}
		dest=${8}
		object=${9}
		expiration=${10}

snippet rax_scaling_group
	rax_scaling_group:
		max_entities=${1:# REQUIRED}
		name=${2:# REQUIRED}
		server_name=${3:# REQUIRED}
		image=${4:# REQUIRED}
		min_entities=${5:# REQUIRED}
		flavor=${6:# REQUIRED}
		files=${7}
		username=${8}
		api_key=${9}
		loadbalancers=${10}
		key_name=${11}
		disk_config=${12:#auto|manual}
		verify_ssl=${13}
		state=${14:#present|absent}
		cooldown=${15}
		meta=${16}
		env=${17}
		credentials=${18}
		region=${19:dfw}
		networks=${20:['public', 'private']}

snippet ec2_group
	ec2_group:
		name=${1:# REQUIRED}
		description=${2:# REQUIRED}
		aws_secret_key=${3}
		rules_egress=${4}
		aws_access_key=${5}
		security_token=${6}
		rules=${7}
		region=${8}
		state=${9:present}
		profile=${10}
		ec2_url=${11}
		vpc_id=${12}
		validate_certs=${13:#yes|no}

snippet quantum_floating_ip
	quantum_floating_ip:
		login_password=${1:yes}
		instance_name=${2:# REQUIRED}
		login_tenant_name=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}
		internal_network_name=${9}

snippet quantum_router_interface
	quantum_router_interface:
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		subnet_name=${4:# REQUIRED}
		router_name=${5:# REQUIRED}
		region_name=${6}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet rax_files
	rax_files:
		container=${1:# REQUIRED}
		username=${2}
		web_index=${3}
		region=${4:dfw}
		verify_ssl=${5}
		private=${6}
		state=${7:#present|absent}
		clear_meta=${8:#yes|no}
		meta=${9}
		env=${10}
		ttl=${11}
		web_error=${12}
		credentials=${13}
		api_key=${14}
		type=${15:#file|meta}
		public=${16}

snippet ec2_vol
	ec2_vol:
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		zone=${5}
		instance=${6}
		region=${7}
		device_name=${8}
		volume_size=${9}
		state=${10:#absent|present}
		iops=${11:100}
		snapshot=${12}
		ec2_url=${13}
		security_token=${14}
		validate_certs=${15:#yes|no}
		id=${16}

snippet virt
	virt:
		name=${1:# REQUIRED}
		xml=${2}
		state=${3:#running|shutdown|destroyed|paused}
		command=${4:#create|status|start|stop|pause|unpause|shutdown|undefine|destroy|get_xml|autostart|freemem|list_vms|info|nodeinfo|virttype|define}
		uri=${5}

snippet rax_keypair
	rax_keypair:
		name=${1:# REQUIRED}
		username=${2}
		public_key=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		state=${8:#present|absent}
		env=${9}
		tenant_name=${10}
		credentials=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_elb_lb
	ec2_elb_lb:
		name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		aws_secret_key=${3}
		subnets=${4}
		aws_access_key=${5}
		health_check=${6}
		security_token=${7}
		region=${8}
		purge_subnets=${9:false}
		ec2_url=${10}
		listeners=${11}
		security_group_ids=${12}
		zones=${13}
		purge_listeners=${14:true}
		profile=${15}
		scheme=${16:internet-facing}
		validate_certs=${17:#yes|no}
		purge_zones=${18:false}

snippet nova_compute
	nova_compute:
		image_id=${1:# REQUIRED}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		login_tenant_name=${5:yes}
		region_name=${6}
		key_name=${7}
		user_data=${8}
		meta=${9}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		wait_for=${11:180}
		security_groups=${12}
		wait=${13:yes}
		nics=${14}
		state=${15:#present|absent}
		flavor_id=${16:1}

snippet linode
	linode:
		datacenter=${1}
		swap=${2:512}
		api_key=${3}
		name=${4}
		payment_term=${5:#1|12|24}
		linode_id=${6}
		state=${7:#present|active|started|absent|deleted|stopped|restarted}
		wait_timeout=${8:300}
		plan=${9}
		distribution=${10}
		password=${11}
		ssh_pub_key=${12}
		wait=${13:#yes|no}

snippet ec2_facts
	ec2_facts: validate_certs=${1:#yes|no}

snippet rax_cbs_attachments
	rax_cbs_attachments:
		volume=${1:# REQUIRED}
		device=${2:# REQUIRED}
		server=${3:# REQUIRED}
		state=${4:#present|absent}
		username=${5}
		tenant_name=${6}
		verify_ssl=${7}
		wait_timeout=${8:300}
		credentials=${9}
		wait=${10:#yes|no}
		identity_type=${11:rackspace}
		tenant_id=${12}
		region=${13:dfw}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}
		env=${15}
		api_key=${16}

snippet docker
	docker:
		image=${1:# REQUIRED}
		username=${2}
		publish_all_ports=${3:false}
		tty=${4:false}
		env=${5}
		links=${6}
		memory_limit=${7:256mb}
		lxc_conf=${8}
		stdin_open=${9:false}
		volumes=${10}
		password=${11}
		count=${12:1}
		detach=${13:true}
		name=${14}
		hostname=${15}
		docker_url=${16:unix://var/run/docker.sock}
		ports=${17}
		state=${18:#present|running|stopped|absent|killed|restarted}
		command=${19}
		dns=${20}
		volumes_from=${21}
		expose=${22}
		privileged=${23:false}

snippet s3
	s3:
		bucket=${1:# REQUIRED}
		mode=${2:# REQUIRED}
		aws_secret_key=${3}
		src=${4}
		aws_access_key=${5}
		expiration=${6:600}
		dest=${7}
		object=${8}
		s3_url=${9}
		overwrite=${10:true}
		metadata=${11}

snippet digital_ocean_domain
	digital_ocean_domain:
		state=${1:#present|active|absent|deleted}
		name=${2}
		client_id=${3}
		ip=${4}
		api_key=${5}
		id=${6}

snippet ec2_snapshot
	ec2_snapshot:
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		description=${4}
		security_token=${5}
		snapshot_tags=${6}
		region=${7}
		ec2_url=${8}
		device_name=${9}
		instance_id=${10}
		volume_id=${11}
		validate_certs=${12:#yes|no}

snippet rds_param_group
	rds_param_group:
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		engine=${4:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		aws_secret_key=${5}
		aws_access_key=${6}
		immediate=${7}
		params=${8:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		description=${9}

snippet gce_pd
	gce_pd:
		name=${1:# REQUIRED}
		size_gb=${2:10}
		zone=${3:us-central1-b}
		service_account_email=${4}
		image=${5}
		pem_file=${6}
		instance_name=${7}
		state=${8:#active|present|absent|deleted}
		snapshot=${9}
		detach_only=${10:#yes|no}
		project_id=${11}
		mode=${12:#READ_WRITE|READ_ONLY}

snippet gce
	gce:
		zone=${1:us-central1-a}
		name=${2}
		tags=${3}
		service_account_email=${4}
		image=${5:debian-7}
		disks=${6}
		metadata=${7}
		persistent_boot_disk=${8:false}
		pem_file=${9}
		state=${10:#active|present|absent|deleted}
		machine_type=${11:n1-standard-1}
		project_id=${12}
		instance_names=${13}
		network=${14:default}

snippet rax
	rax:
		files=${1}
		username=${2}
		tenant_name=${3}
		auto_increment=${4:#yes|no}
		image=${5}
		count_offset=${6:1}
		instance_ids=${7}
		user_data=${8}
		verify_ssl=${9}
		wait_timeout=${10:300}
		tenant_id=${11}
		credentials=${12}
		region=${13:dfw}
		flavor=${14}
		networks=${15:['public', 'private']}
		wait=${16:#yes|no}
		count=${17:1}
		group=${18}
		name=${19}
		identity_type=${20:rackspace}
		extra_client_args=${21}
		exact_count=${22:#yes|no}
		disk_config=${23:#auto|manual}
		auth_endpoint=${24:https://identity.api.rackspacecloud.com/v2.0/}
		state=${25:#present|absent}
		meta=${26}
		env=${27}
		key_name=${28}
		api_key=${29}
		extra_create_args=${30}
		config_drive=${31:#yes|no}

snippet ec2_vpc
	ec2_vpc:
		resource_tags=${1:# REQUIRED}
		cidr_block=${2:# REQUIRED}
		state=${3:present}
		subnets=${4}
		internet_gateway=${5:#yes|no}
		wait_timeout=${6:300}
		dns_hostnames=${7:#yes|no}
		wait=${8:#yes|no}
		aws_secret_key=${9}
		aws_access_key=${10}
		route_tables=${11}
		dns_support=${12:#yes|no}
		region=${13}
		instance_tenancy=${14:#default|dedicated}
		vpc_id=${15}
		validate_certs=${16:#yes|no}

snippet glance_image
	glance_image:
		login_password=${1:yes}
		login_username=${2:admin}
		name=${3:# REQUIRED}
		login_tenant_name=${4:yes}
		region_name=${5}
		container_format=${6:bare}
		min_ram=${7}
		owner=${8}
		endpoint_type=${9:#publicURL|internalURL}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		file=${11}
		min_disk=${12}
		is_public=${13:yes}
		disk_format=${14:qcow2}
		copy_from=${15}
		state=${16:#present|absent}
		timeout=${17:180}

snippet rax_clb
	rax_clb:
		username=${1}
		protocol=${2:#DNS_TCP|DNS_UDP|FTP|HTTP|HTTPS|IMAPS|IMAPv4|LDAP|LDAPS|MYSQL|POP3|POP3S|SMTP|TCP|TCP_CLIENT_FIRST|UDP|UDP_STREAM|SFTP}
		name=${3}
		algorithm=${4:#RANDOM|LEAST_CONNECTIONS|ROUND_ROBIN|WEIGHTED_LEAST_CONNECTIONS|WEIGHTED_ROUND_ROBIN}
		env=${5}
		region=${6:dfw}
		verify_ssl=${7}
		vip_id=${8}
		state=${9:#present|absent}
		wait_timeout=${10:300}
		meta=${11}
		timeout=${12:30}
		credentials=${13}
		api_key=${14}
		type=${15:#PUBLIC|SERVICENET}
		port=${16:80}
		wait=${17:#yes|no}

snippet rds
	rds:
		command=${1:#create|replicate|delete|facts|modify|promote|snapshot|restore}
		region=${2:# REQUIRED}
		instance_name=${3:# REQUIRED}
		db_engine=${4:#MySQL|oracle-se1|oracle-se|oracle-ee|sqlserver-ee|sqlserver-se|sqlserver-ex|sqlserver-web|postgres}
		iops=${5}
		backup_window=${6}
		backup_retention=${7}
		port=${8}
		security_groups=${9}
		size=${10}
		aws_secret_key=${11}
		subnet=${12}
		vpc_security_groups=${13}
		upgrade=${14:#yes|no}
		zone=${15}
		source_instance=${16}
		parameter_group=${17}
		multi_zone=${18:#yes|no}
		new_instance_name=${19}
		username=${20}
		db_name=${21}
		license_model=${22:#license-included|bring-your-own-license|general-public-license}
		password=${23}
		apply_immediately=${24:#yes|no}
		wait=${25:#yes|no}
		aws_access_key=${26}
		option_group=${27}
		engine_version=${28}
		instance_type=${29}
		wait_timeout=${30:300}
		snapshot=${31}
		maint_window=${32}

snippet route53
	route53:
		zone=${1:# REQUIRED}
		record=${2:# REQUIRED}
		command=${3:#get|create|delete}
		type=${4:#A|CNAME|MX|AAAA|TXT|PTR|SRV|SPF|NS}
		aws_secret_key=${5}
		aws_access_key=${6}
		retry_interval=${7:500}
		value=${8}
		ttl=${9:3600 (one hour)}
		overwrite=${10}

snippet ec2_asg
	ec2_asg:
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		availability_zones=${6}
		security_token=${7}
		tags=${8}
		region=${9}
		min_size=${10}
		desired_capacity=${11}
		vpc_zone_identifier=${12}
		launch_config_name=${13}
		health_check_period=${14:500 seconds}
		ec2_url=${15}
		load_balancers=${16}
		validate_certs=${17:#yes|no}
		max_size=${18}
		health_check_type=${19:#EC2|ELB}

snippet ec2_scaling_policy
	ec2_scaling_policy:
		name=${1:# REQUIRED}
		asg_name=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		security_token=${7}
		adjustment_type=${8:#ChangeInCapacity|ExactCapacity|PercentChangeInCapacity}
		min_adjustment_step=${9}
		scaling_adjustment=${10}
		cooldown=${11}
		ec2_url=${12}
		validate_certs=${13:#yes|no}

snippet riak
	riak:
		target_node=${1:riak@127.0.0.1}
		config_dir=${2:/etc/riak}
		wait_for_service=${3:#kv}
		http_conn=${4:127.0.0.1:8098}
		wait_for_ring=${5}
		wait_for_handoffs=${6}
		command=${7:#ping|kv_test|join|plan|commit}
		validate_certs=${8:#yes|no}

snippet mysql_user
	mysql_user:
		name=${1:# REQUIRED}
		login_port=${2:3306}
		login_user=${3}
		login_host=${4:localhost}
		append_privs=${5:#yes|no}
		host=${6:localhost}
		login_unix_socket=${7}
		state=${8:#present|absent}
		login_password=${9}
		check_implicit_admin=${10:false}
		password=${11}
		priv=${12}

snippet mysql_replication
	mysql_replication:
		master_ssl_cert=${1}
		master_password=${2}
		login_user=${3}
		login_host=${4}
		login_password=${5}
		master_host=${6}
		master_ssl_ca=${7}
		login_unix_socket=${8}
		master_connect_retry=${9}
		master_user=${10}
		master_port=${11}
		master_log_file=${12}
		master_ssl_cipher=${13}
		relay_log_file=${14}
		master_ssl=${15}
		master_ssl_key=${16}
		master_ssl_capath=${17}
		mode=${18:#getslave|getmaster|changemaster|stopslave|startslave}
		master_log_pos=${19}
		relay_log_pos=${20}

snippet postgresql_user
	postgresql_user:
		name=${1:# REQUIRED}
		login_password=${2}
		login_user=${3:postgres}
		login_host=${4:localhost}
		expires=${5}
		db=${6}
		port=${7:5432}
		state=${8:#present|absent}
		encrypted=${9:false}
		password=${10}
		role_attr_flags=${11:#[NO]SUPERUSER|[NO]CREATEROLE|[NO]CREATEUSER|[NO]CREATEDB|[NO]INHERIT|[NO]LOGIN|[NO]REPLICATION}
		fail_on_user=${12:#yes|no}
		priv=${13}

snippet postgresql_privs
	postgresql_privs:
		roles=${1:# REQUIRED}
		database=${2:# REQUIRED}
		objs=${3}
		privs=${4}
		state=${5:#present|absent}
		host=${6}
		login=${7:postgres}
		password=${8}
		type=${9:#table|sequence|function|database|schema|language|tablespace|group}
		port=${10:5432}
		grant_option=${11:#yes|no}
		schema=${12}

snippet redis
	redis:
		command=${1:#slave|flush|config}
		login_port=${2:6379}
		name=${3}
		flush_mode=${4:#all|db}
		master_host=${5}
		login_host=${6:localhost}
		master_port=${7}
		db=${8}
		value=${9}
		login_password=${10}
		slave_mode=${11:#master|slave}

snippet postgresql_db
	postgresql_db:
		name=${1:# REQUIRED}
		encoding=${2}
		login_user=${3}
		lc_collate=${4}
		lc_ctype=${5}
		port=${6:5432}
		state=${7:#present|absent}
		template=${8}
		login_password=${9}
		owner=${10}
		login_host=${11:localhost}

snippet mysql_variables
	mysql_variables:
		variable=${1:# REQUIRED}
		login_unix_socket=${2}
		login_password=${3}
		login_user=${4}
		login_host=${5}
		value=${6}

snippet mongodb_user
	mongodb_user:
		database=${1:# REQUIRED}
		user=${2:# REQUIRED}
		login_port=${3:27017}
		roles=${4:readwrite}
		login_user=${5}
		login_host=${6:localhost}
		state=${7:#present|absent}
		login_password=${8}
		password=${9}
		replica_set=${10}

snippet mysql_db
	mysql_db:
		name=${1:# REQUIRED}
		login_port=${2:3306}
		encoding=${3}
		login_user=${4}
		login_host=${5:localhost}
		login_unix_socket=${6}
		state=${7:#present|absent|dump|import}
		login_password=${8}
		collation=${9}
		target=${10}

snippet lineinfile
	lineinfile:
		dest=${1:# REQUIRED}
		path=${2:[]}
		src=${3}
		force=${4:#yes|no}
		insertbefore=${5:#BOF|*regex*}
		selevel=${6:s0}
		create=${7:#yes|no}
		seuser=${8}
		recurse=${9:#yes|no}
		serole=${10}
		backrefs=${11:#yes|no}
		owner=${12}
		state=${13:#file|link|directory|hard|touch|absent}
		mode=${14}
		insertafter=${15:#EOF|*regex*}
		regexp=${16}
		line=${17}
		backup=${18:#yes|no}
		validate=${19}
		group=${20}
		setype=${21}

snippet get_url
	get_url:
		url=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		path=${3:[]}
		url_password=${4}
		force=${5:#yes|no}
		use_proxy=${6:#yes|no}
		src=${7}
		selevel=${8:s0}
		seuser=${9}
		recurse=${10:#yes|no}
		setype=${11}
		sha256sum=${12}
		serole=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		mode=${15}
		url_username=${16}
		owner=${17}
		group=${18}
		validate_certs=${19:#yes|no}

snippet ini_file
	ini_file:
		dest=${1:# REQUIRED}
		path=${2:[]}
		section=${3:# REQUIRED}
		force=${4:#yes|no}
		option=${5}
		state=${6:#file|link|directory|hard|touch|absent}
		selevel=${7:s0}
		owner=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		setype=${13}
		value=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet uri
	uri:
		path=${1:[]}
		url=${2:# REQUIRED}
		force=${3:#yes|no}
		follow_redirects=${4:#all|safe|none}
		owner=${5}
		HEADER_=${6}
		group=${7}
		serole=${8}
		setype=${9}
		status_code=${10:200}
		return_content=${11:#yes|no}
		method=${12:#GET|POST|PUT|HEAD|DELETE|OPTIONS|PATCH}
		body=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		dest=${15}
		selevel=${16:s0}
		force_basic_auth=${17:#yes|no}
		removes=${18}
		user=${19}
		password=${20}
		src=${21}
		seuser=${22}
		recurse=${23:#yes|no}
		creates=${24}
		mode=${25}
		timeout=${26:30}

snippet replace
	replace:
		dest=${1:# REQUIRED}
		path=${2:[]}
		regexp=${3:# REQUIRED}
		force=${4:#yes|no}
		state=${5:#file|link|directory|hard|touch|absent}
		selevel=${6:s0}
		replace=${7}
		owner=${8}
		validate=${9}
		src=${10}
		group=${11}
		seuser=${12}
		recurse=${13:#yes|no}
		setype=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet assemble
	assemble:
		dest=${1:# REQUIRED}
		path=${2:[]}
		force=${3:#yes|no}
		remote_src=${4:#True|False}
		selevel=${5:s0}
		state=${6:#file|link|directory|hard|touch|absent}
		owner=${7}
		regexp=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		serole=${13}
		delimiter=${14}
		mode=${15}
		backup=${16:#yes|no}
		setype=${17}


mod_authn_dbd_mysql Cookbook
=================

Requirements
------------

### Platform:

*List supported platforms here*

### Cookbooks:

*List cookbook dependencies here*

Attributes
----------

*List attributes here*

Recipes
-------

### mod_authn_dbd_mysql::default

*Explain what the recipe does here*

### Packer Build

In order to build an Amazon AMI, DigitalOcean Droplet, Google Compute or Rackspace OpenStack image, you will need accounts for each. Export these values to get Packer to honor them automatically:

    # Rackspace
    export SDK_USERNAME="username"  # Same as here: https://mycloud.rackspace.com/
    export SDK_PASSWORD="password-to-login" # Not the API key.
    export SDK_PROVIDER="rackspace-us" # Or rackspace-uk

    # EC2 - can be found here: https://portal.aws.amazon.com/gp/aws/securityCredentials?
    export AWS_ACCESS_KEY="long-random-string"
    export AWS_SECRET_KEY="another-even-longer-long-random-string"

    # Digital Ocean - get these here: https://cloud.digitalocean.com/api_access
    export DIGITALOCEAN_CLIENT_ID="long-random-string"
    export DIGITALOCEAN_API_KEY="another-long-random-string"

Building images with Google Compute requires more setup than just a few ENV vars. Once you setup your GCE account and enable billing, take a look [here](http://www.packer.io/docs/builders/googlecompute.html) and [here](https://github.com/mitchellh/packer/issues/809) for additional Packer specific information.

Testing
-------

[![Build Status](https://travis-ci.org/darron/mod_authn_dbd_mysql-cookbook.png?branch=master)](https://travis-ci.org/darron/mod_authn_dbd_mysql-cookbook)

The cookbook provides the following Rake tasks:

    rake build                        # Syntax check and build all Packer targets
    rake build_ami                    # Syntax check and build AMI
    rake build_droplet                # Syntax check and build Droplet
    rake build_gce                    # Syntax check and build Google Compute Image
    rake build_openstack              # Syntax check and build Openstack Image
    rake build_vagrant                # Syntax check and build Vagrant box
    rake cleanup_vendor               # Cleanup Vendor directory
    rake convert_gce                  # Convert GCE key to pem format
    rake food_extra                   # Run extra Foodcritic rulesets
    rake integration                  # Alias for kitchen:all
    rake kitchen:all                  # Run all test instances
    rake kitchen:default-ubuntu-1204  # Run default-ubuntu-1204 test instance
    rake kitchen:default-ubuntu-1404  # Run default-ubuntu-1404 test instance
    rake knife_solo                   # Usage: rake knife_solo user={user} ip={ip.address.goes.here}
    rake lint                         # Lint Chef cookbooks
    rake rubocop                      # Run rubocop tests
    rake spec                         # Run ChefSpec examples
    rake tailor                       # Run tailor tests
    rake taste                        # Run taste tests
    rake test                         # Run all tests

License and Author
------------------

Author:: Darron Froese (darron@froese.org)

Copyright:: 2014, Darron Froese

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

Contributing
------------

We welcome contributed improvements and bug fixes via the usual workflow:

1. Fork this repository
2. Create your feature branch (`git checkout -b my-new-feature`)
3. Commit your changes (`git commit -am 'Add some feature'`)
4. Push to the branch (`git push origin my-new-feature`)
5. Create a new pull request
from fabric.contrib.files import exists
try: 
    input = raw_input
except NameError: 
    pass
from datetime import datetime
import os
import sys
import time

import boto.ec2
from fabric.api import env, run, sudo, cd, put
from fabric.exceptions import NetworkError
import requests
from transitfeed.gtfsfactory import GetGtfsFactory
from transitfeed.problems import ProblemReporter, TYPE_WARNING
import transitfeed


from deployer import REPORTS_DIR, CONFIG_TEMPLATE_DIR, DL_DIR
from deployer.config import get_aws_config, get_gtfs_config,\
    get_ontransit_config
from deployer.fab_crontab import crontab_update
from deployer.feedvalidator import HTMLCountingProblemAccumulator
from deployer.util import FabLogger, write_template, unix_path_join


GTFS_FILE_NAME_RAW = 'google_transit_{0}.zip'.format(datetime.now().strftime('%Y-%m-%d'))
GTFS_FILE_NAME = os.path.join(DL_DIR, GTFS_FILE_NAME_RAW)


class Fab:
    
    aws_conf = get_aws_config()
    gtfs_conf = get_gtfs_config()
    ontransit_conf = get_ontransit_config()
    ontransit_base_folder = ontransit_conf.get('ontransit_base_folder')
    ontransit_server_folder = unix_path_join(ontransit_base_folder, 'server')
    ontransit_web_folder = unix_path_join(ontransit_base_folder, 'web')
    
    def __init__(self, user_type, host_name, ssh_sleep=10):
        '''Constructor for Class.  Sets up fabric environment.
        
        Args:
            user_type (string): the linux user type
            host_name (string): ec2 public dns name
            ssh_sleep (int, default=10): number of seconds to sleep between ssh tries
        '''
        
        if user_type == 'root':
            self.user = self.aws_conf.get('root_user')
            self.user_home = unix_path_join('/root')
            self.server_config_dir = unix_path_join(self.user_home, 'conf')
            self.script_dir = unix_path_join(self.user_home, 'scripts')
            self.data_dir = unix_path_join(self.user_home, 'data')
        else:
            env.password = self.aws_conf.get('regular_user_password')
            self.user = self.aws_conf.get('regular_user_name')
            self.user_home = unix_path_join('/home', self.user)
            self.server_config_dir = unix_path_join(self.user_home, 'conf')
            self.script_dir = unix_path_join(self.user_home, 'scripts')
            self.data_dir = unix_path_join(self.user_home, 'data')
        
        env.host_string = '{0}@{1}'.format(self.user, host_name)
        env.key_filename = [self.aws_conf.get('key_filename')]
        sys.stdout = FabLogger(os.path.join(REPORTS_DIR, 'aws_fab.log'))
        
        max_retries = 6
        num_retries = 0
        
        retry = True
        while retry:
            try:
                # SSH into the box here.
                self.test_cmd()
                retry = False
            except NetworkError as e:
                print(e)
                if num_retries > max_retries:
                    raise Exception('Maximum Number of SSH Retries Hit.  Did EC2 instance get configured with ssh correctly?')
                num_retries += 1 
                print('SSH failed (the system may still be starting up), waiting {0} seconds...'.format(ssh_sleep))
                time.sleep(ssh_sleep)
        
    def test_cmd(self):
        '''A test command to see if everything is running ok.
        '''
        
        run('uname')
        
    def new_user(self):
        '''Setup a non-root user.
        '''
        
        regular_user_name = self.aws_conf.get('regular_user_name')
        
        run('adduser {0}'.format(regular_user_name))
        run('echo {0} | passwd {1} --stdin'.
            format(self.aws_conf.get('regular_user_password'),
                   regular_user_name))
        
        # grant sudo access to regular user
        put(write_template(dict(regular_user_name=regular_user_name),
                           'user-init'),
            '/etc/sudoers.d',
            True)
        
    def update_system(self):
        '''Updates the instance with the latest patches and upgrades.
        '''
        
        sudo('yum -y update')
        
    def set_timezone(self):
        '''Changes the machine's localtime to the desired timezone.
        '''
        
        with cd('/etc'):
            sudo('rm -rf localtime')
            sudo('ln -s {0} localtime'.format(self.aws_conf.get('timezone')))        
        
    def install_custom_monitoring(self):
        '''Installs a custom monitoring script to monitor memory and disk utilization.
        '''
        
        # install helpers
        sudo('yum -y install perl-DateTime perl-Sys-Syslog perl-LWP-Protocol-https')
        
        # dl scripts
        run('wget http://aws-cloudwatch.s3.amazonaws.com/downloads/CloudWatchMonitoringScripts-1.2.1.zip')
        sudo('unzip CloudWatchMonitoringScripts-1.2.1.zip -d /usr/local')
        run('rm CloudWatchMonitoringScripts-1.2.1.zip')
        
        # prepare the monitoring crontab        
        with open(os.path.join(CONFIG_TEMPLATE_DIR, 'monitoring_crontab')) as f:
            cron = f.read()
        
        cron_settings = dict(aws_access_key_id=self.aws_conf.get('aws_access_key_id'),
                             aws_secret_key=self.aws_conf.get('aws_secret_access_key'),
                             cron_email=self.aws_conf.get('cron_email'))  
        aws_logging_cron = cron.format(**cron_settings)
            
        # start crontab for aws monitoring
        crontab_update(aws_logging_cron, 'aws_monitoring')
        
    def install_helpers(self):
        '''Installs various utilities (typically not included with CentOS).
        '''
        
        sudo('yum -y install wget')
        sudo('yum -y install unzip')
    
    def install_git(self):
        '''Installs git.
        '''
        
        sudo('yum -y install git')
        
    def upload_pg_hba_conf(self, local_method):
        '''Overwrites pg_hba.conf with specified local method.
        '''
        
        remote_data_folder = '/var/lib/pgsql/9.3/data'
        remote_pg_hba_conf = '/var/lib/pgsql/9.3/data/pg_hba.conf'
        
        sudo('rm -rf {0}'.format(remote_pg_hba_conf))
        
        put(write_template(dict(local_method=local_method), 'pg_hba.conf'),
            remote_data_folder,
            True)
        
        with cd(remote_data_folder):
            sudo('chmod 600 pg_hba.conf')
            sudo('chgrp postgres pg_hba.conf')
            sudo('chown postgres pg_hba.conf')
        
    def install_pg(self):
        '''Downloads and configures PostgreSQL.
        '''
        
        # get yum squared away
        sudo('rpm -ivh http://yum.postgresql.org/9.3/redhat/rhel-6-x86_64/pgdg-centos93-9.3-1.noarch.rpm')
        
        # install postgresql
        sudo('yum -y install postgresql93 postgresql93-server postgresql93-libs postgresql93-contrib postgresql93-devel')
        
        # make sure yum knows about postgis dependencies
        sudo('rpm -ivh http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm')
        
        # install postgis
        sudo('yum -y install postgis2_93')
        
        # initialize db
        sudo('service postgresql-9.3 initdb')
        
        # upload insecure pg_hba.conf
        self.upload_pg_hba_conf('trust')
        
        db_setup_dict = dict(pg_worker_username=self.ontransit_conf.get('pg_worker_username'),
                             pg_worker_password=self.ontransit_conf.get('pg_worker_password'),
                             pg_web_username=self.ontransit_conf.get('pg_web_username'),
                             pg_web_password=self.ontransit_conf.get('pg_web_password'),
                             pg_worker_groupname=self.ontransit_conf.get('pg_worker_groupname'),
                             database_name=self.ontransit_conf.get('database_name'))
        
        if not exists(self.server_config_dir):
            run('mkdir {0}'.format(self.server_config_dir))
        
        put(write_template(db_setup_dict, 'db_setup.sql'),
            self.server_config_dir)
        
        # start postgres
        sudo('service postgresql-9.3 start')
        
        # execute db setup sql
        sudo('psql -U postgres -f {0}'.format(unix_path_join(self.server_config_dir, 'db_setup.sql')))
        sudo('psql -U postgres -d {0} -c "CREATE EXTENSION postgis;"'.
             format(self.ontransit_conf.get('database_name')))
        
        # switch to more secure pg_hba.conf
        self.upload_pg_hba_conf('md5')
        
        # restart postgres
        sudo('service postgresql-9.3 restart')
        
        # start postgresql on boot
        sudo('chkconfig postgresql-9.3 on')
        
    def install_node(self):
        '''Installs node and npm.
        '''
        
        # download and install from website instead of yum
        run('wget https://nodejs.org/dist/v4.0.0/node-v4.0.0-linux-x64.tar.gz')
        run('tar xzf node-v4.0.0-linux-x64.tar.gz -C /usr/local')
        run('rm -rf node-v4.0.0-linux-x64.tar.gz')
        run('mv /usr/local/node-v4.0.0-linux-x64 /usr/local/node')
        run('ln -s /usr/local/node/bin/node /usr/bin/node')
        run('ln -s /usr/local/node/bin/npm /usr/bin/npm')
        
        # install forever to run server continuously
        sudo('npm install forever -g')
        sudo('ln -s /usr/local/node/bin/forever /usr/bin/forever') 
        
    def setup_iptables(self):
        '''Adds in iptables rules to forward 80 to 3000.
        
        Got help from: http://serverfault.com/questions/722270/configuring-centos-6-iptables-to-use-port-80-for-nodejs-app-running-on-port-3000/722282#722282
        '''
        
        sudo('iptables -I INPUT 1 -p tcp --dport 80 -j ACCEPT')
        sudo('iptables -t nat -A PREROUTING -i eth0 -p tcp --dport 80 -j REDIRECT --to-port 3000')
        sudo('iptables -I INPUT 1 -p tcp --dport 3000 -j ACCEPT')
        sudo('sudo service iptables save')
        
    def install_ontransit(self):
        '''Clones and then installs OnTransit.
        '''
        
        run('git clone {0}'.format(self.ontransit_conf.get('ontransit_git_repo')))
        
        with cd(self.ontransit_base_folder):
            run('git checkout {0}'.format(self.ontransit_conf.get('ontransit_git_branch')))
        
        # upload server config
        server_data = dict(gtfs_static_url=self.gtfs_conf.get('gtfs_static_url'),
                           block_delay_delete_threshold=self.ontransit_conf.get('block_delay_delete_threshold'),
                           database_name=self.ontransit_conf.get('database_name'),
                           nearby_stop_future_padding=self.ontransit_conf.get('nearby_stop_future_padding'),
                           nearby_stop_past_padding=self.ontransit_conf.get('nearby_stop_past_padding'),
                           pg_worker_username=self.ontransit_conf.get('pg_worker_username'),
                           pg_worker_password=self.ontransit_conf.get('pg_worker_password'),
                           pg_web_username=self.ontransit_conf.get('pg_web_username'),
                           pg_web_password=self.ontransit_conf.get('pg_web_password'),
                           trip_end_padding=self.ontransit_conf.get('trip_end_padding'),
                           trip_start_padding=self.ontransit_conf.get('trip_start_padding'))
        
        with(cd(self.ontransit_server_folder)):

            # create config dir and upload file
            run('mkdir config')

            put(write_template(server_data, 
                               'server_config_index.js', 
                               'index.js'),
                'config')

            # install npm stuff
            run('npm install')
            
        # upload web config
        web_data = dict(agency_timezone=self.ontransit_conf.get('agency_timezone'),
                        google_analytics_tracking_key=self.ontransit_conf.get('google_analytics_tracking_key'),
                        google_maps_api_key=self.ontransit_conf.get('google_maps_api_key'))
        put(write_template(web_data, 
                           'web_config.js', 
                           'config.js'), 
            unix_path_join(self.ontransit_web_folder, 'js'))
                
        with(cd(self.ontransit_web_folder)):
            # install npm stuff
            run('npm install')
            
            if not exists('dist'):
                run('mkdir dist')
            
            # build static files            
            run('npm run build')
            
    def update_gtfs(self):
        '''Instructs OnTransit server to immediately reload the transit data.
        '''
        
        with(cd(self.ontransit_server_folder)):
            run('npm run load-gtfs')

    def start_server(self):
        '''Starts the OnTransit server
        '''
        
        with cd(self.ontransit_server_folder):
            run('forever start --uid "ontransit" index.js')
            
    def install_ontransit_cron_scripts(self):
        '''Install cron scripts to make sure ontransit data is up-to-date.
        '''
        
        # check if script folder exists
        if not exists(self.script_dir):
            run('mkdir {0}'.format(self.script_dir))
            
        # check if data folder exists
        if not exists(self.data_dir):
            run('mkdir {0}'.format(self.data_dir))
            
        # prepare nightly update cron script
        with open(os.path.join(CONFIG_TEMPLATE_DIR, 'gtfs_refresh_crontab')) as f:
            cron_template = f.read()
            
        cron_settings = dict(cron_email=self.aws_conf.get('cron_email'),
                             logfile=unix_path_join(self.data_dir, 'nightly_refresh.out'),
                             user=self.user,
                             ontransit_base_folder=self.ontransit_base_folder)
        cron = cron_template.format(**cron_settings)
            
        crontab_update(cron, 'gtfs_refresh_cron')
        
        # prepare block delay purge cron script
        with open(os.path.join(CONFIG_TEMPLATE_DIR, 'block_delay_purge_crontab')) as f:
            cron_template = f.read()
            
        cron_settings = dict(cron_email=self.aws_conf.get('cron_email'),
                             logfile=unix_path_join(self.data_dir, 'blockPurge.out'),
                             user=self.user,
                             ontransit_base_folder=self.ontransit_base_folder)
        cron = cron_template.format(**cron_settings)
            
        crontab_update(cron, 'block_delay_purge_cron')
        
    def stop_server(self):
        '''Stops the OnTransit server.
        '''
        
        # call to stop listener port
        run('forever stop ontransit')    
        

def get_aws_connection():
    '''Connect to AWS.
    
    Returns:
        boto.ec2.connection.EC2Connection: Connection to region.
    '''
    
    print('Connecting to AWS')
    aws_conf = get_aws_config()
    return boto.ec2.connect_to_region(aws_conf.get('region'),
                                      aws_access_key_id=aws_conf.get('aws_access_key_id'),
                                      aws_secret_access_key=aws_conf.get('aws_secret_access_key'))
    
    
def get_fab(user_type, instance_dns_name=None, ssh_sleep=10):
    '''Get a instance of fabric deployer.
    
    Args:
        user_type (string): The user type to use when logging in.  
            Typically "root" when first logging in and then whatever is defined in config. 
        instance_dns_name (string, default=None): The EC2 instance to deploy to.
        
    Returns:
        Fab: a Fab instance
    '''
    
    if not instance_dns_name:
        instance_dns_name = input('Enter EC2 public dns name: ')
        
    return Fab(user_type, instance_dns_name, ssh_sleep)


def launch_new():
    '''Launch a new EC2 instance installing an OBA instance
    
    Retruns:
        String: the dns name of the instance that was launched.
    '''
    
    # connect to AWS and launch new instance
    aws_conf = get_aws_config()
    conn = get_aws_connection()
    
    print('Preparing volume')
    block_device = boto.ec2.blockdevicemapping.EBSBlockDeviceType()
    block_device.size = aws_conf.get('volume_size')
    block_device.delete_on_termination = True
    block_device_map = boto.ec2.blockdevicemapping.BlockDeviceMapping()
    block_device_map[aws_conf.get('block_device_map')] = block_device 
    
    print('Launching new instance')
    reservation = conn.run_instances(aws_conf.get('ami_id'),
                                     instance_type=aws_conf.get('instance_type'),
                                     key_name=aws_conf.get('key_pair_name'),
                                     security_groups=aws_conf.get('security_groups').split(','),
                                     block_device_map=block_device_map)
    
    # Get the instance
    instance = reservation.instances[0]
    
    # Check if it's up and running a specified maximum number of times
    max_retries = 10
    num_retries = 0
    
    # Check up on its status every so often
    status = instance.update()
    while status == 'pending':
        if num_retries > max_retries:
            tear_down(instance.id, conn)
            raise Exception('Maximum Number of Instance Retries Hit.  Did EC2 instance spawn correctly?')
        num_retries += 1 
        print('Instance pending, waiting 10 seconds...')
        time.sleep(10)
        status = instance.update()
    
    if status == 'running':
        instance.add_tag("Name", aws_conf.get('instance_name'))
    else:
        print('Instance status: ' + status)
        return None
    
    # setup regular user
    fab = get_fab('root', instance.public_dns_name, 60)
    fab.new_user()
    
    return instance.public_dns_name
    
    
def install_dependencies(instance_dns_name=None):
    '''Install a bunch of linux stuff and libraries essential to running OnTransit
    '''
    
    root_fab = get_fab('root', instance_dns_name)
    
    # If we've reached this point, the instance is up and running.
    print('SSH working')
    root_fab.update_system()
    root_fab.install_helpers()
    #root_fab.install_custom_monitoring()  # apparently only works with Amazon Linux
    root_fab.set_timezone()
    root_fab.install_pg()
    root_fab.install_git()
    root_fab.install_node()
    root_fab.setup_iptables()
    

def tear_down(instance_id=None, conn=None):
    '''Terminates a EC2 instance and deletes all associated volumes.
    
    Args:
        instance_id (string): The ec2 instance id to terminate.
        conn (boto.ec2.connection.EC2Connection): Connection to region.
    '''
    
    if not instance_id:
        instance_id = input('Enter instance id: ')
    
    if not conn:
        conn = get_aws_connection()
        
    volumes = conn.get_all_volumes(filters={'attachment.instance-id': [instance_id]})
        
    print('Terminating instance')
    conn.terminate_instances([instance_id])
    
    aws_conf = get_aws_config()
    if aws_conf.get('delete_volumes_on_tear_down') == 'true':
            
        max_wait_retries = 12
        
        print('Deleting volume(s) associated with instance')
        for volume in volumes:
            volume_deleted = False
            num_retries = 0
            while not volume_deleted:
                try:
                    conn.delete_volume(volume.id)
                    volume_deleted = True
                except Exception as e:
                    if num_retries >= max_wait_retries:
                        raise e
                    print('Waiting for volume to become detached from instance.  Waiting 10 seconds...')
                    time.sleep(10)


def install_ontransit(instance_dns_name=None):
    '''Installs OnTransit on the EC2 instance.
    
    Args:
        instance_dns_name (string, default=None): The EC2 instance to deploy to.
    '''
    
    fab = get_fab('regular', instance_dns_name)
    fab.install_ontransit()
    
    
def validate_gtfs():
    '''Download and validate the latest static GTFS file.
    
    Returns:
        boolean: True if no errors in GTFS.
    '''
    
    # get gtfs settings
    gtfs_conf = get_gtfs_config()
    
    # Create the `downloads` directory if it doesn't exist
    if not os.path.exists(DL_DIR):
        os.makedirs(DL_DIR)
        
    # Create the `reports` directory if it doesn't exist
    if not os.path.exists(REPORTS_DIR):
        os.makedirs(REPORTS_DIR)
    
    # download gtfs
    print('Downloading GTFS')
    r = requests.get(gtfs_conf.get('gtfs_static_url'), stream=True)
    with open(GTFS_FILE_NAME, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024): 
            if chunk:  # filter out keep-alive new chunks
                f.write(chunk)
                f.flush()
                
    # load gtfs
    print('Validating GTFS')
    gtfs_factory = GetGtfsFactory()
    accumulator = HTMLCountingProblemAccumulator(limit_per_type=50)
    problem_reporter = ProblemReporter(accumulator)
    loader = gtfs_factory.Loader(GTFS_FILE_NAME, problems=problem_reporter)
    schedule = loader.Load()
    
    # validate gtfs
    schedule.Validate()
    
    # check for trips with a null value for trip_headsign
    for trip in schedule.GetTripList():
        if trip.trip_headsign == 'null':
            problem_reporter.InvalidValue('trip_headsign', 'null', type=TYPE_WARNING)
            
    # write GTFS report to file
    report_name = 'gtfs_validation_{0}.html'.format(datetime.now().strftime('%Y-%m-%d %H.%M'))
    report_filenmae = os.path.join(REPORTS_DIR, report_name)
    with open(report_filenmae, 'w') as f:
        accumulator.WriteOutput(GTFS_FILE_NAME, f, schedule, transitfeed)
        
    print('GTFS validation report written to {0}'.format(report_filenmae))
    
    # post-validation
    gtfs_validated = True
    num_errors = accumulator.ErrorCount()
    if num_errors > 0:
        gtfs_validated = False
        print('{0} errors in GTFS data'.format(num_errors))
        
    num_warnings = accumulator.WarningCount()
    if num_warnings > 0:
        print('{0} warnings about GTFS data'.format(num_warnings))
        
    if 'ExpirationDate' in accumulator.ProblemListMap(TYPE_WARNING).keys():
        start_date, end_date = schedule.GetDateRange()
        last_service_day = datetime(*(time.strptime(end_date, "%Y%m%d")[0:6]))
        if last_service_day < datetime.now():
            print('GTFS Feed has expired.')
            gtfs_validated = False
        
    return gtfs_validated


def update_gtfs(instance_dns_name=None, validate=True):
    '''Update the gtfs file on the EC2 instance and tell OnTransit to load the latest gtfs.
    
    This assumes that OnTransit has been installed on the server.
    
    Args:
        instance_dns_name (string, default=None): The EC2 instance to update the gtfs on.
        validate (boolean, default=True): Whether or not to require a passing validation the GTFS.
    '''
    
    if validate and not validate_gtfs():
        raise Exception('GTFS static file validation Failed.')
        
    fab = get_fab('regular', instance_dns_name)
    fab.update_gtfs()
    
    
def start_server(instance_dns_name=None):
    '''Starts the OnTransit server.
    '''
    
    fab = get_fab('regular', instance_dns_name)
    fab.start_server()
    
    
def stop_server(instance_dns_name=None):
    '''Starts the OnTransit server.
    '''
    
    fab = get_fab('regular', instance_dns_name)
    fab.stop_server()
    
    
def install_cron_scripts(instance_dns_name=None):
    '''Starts the OnTransit server.
    '''
    
    fab = get_fab('regular', instance_dns_name)
    fab.install_ontransit_cron_scripts()
    
    
def master():
    '''A single script to deploy OnTransit in one command to a new EC2 instance
    '''
    
    # dl gtfs and validate it
    if not validate_gtfs():
        raise Exception('GTFS Validation Failed')
    
    # setup new EC2 instance
    public_dns_name = launch_new()
    
    install_dependencies(public_dns_name)
    
    # install OBA
    install_ontransit(public_dns_name)
    
    # update GTFS, instruct OnTransit to load in new data
    update_gtfs(public_dns_name, False)
    
    # start server
    start_server(public_dns_name)
    
    # install crons
    install_cron_scripts(public_dns_name)
snippet rollbar_deployment
	rollbar_deployment: >
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		revision=${3:# REQUIRED}
		comment=${4}
		rollbar_user=${5}
		url=${6:https://api.rollbar.com/api/1/deploy/}
		user=${7}
		validate_certs=${8:#yes|no}

snippet logentries
	logentries: path=${1:# REQUIRED} state=${2:#present|absent}

snippet nagios
	nagios: >
		action=${1:#downtime|enable_alerts|disable_alerts|silence|unsilence|silence_nagios|unsilence_nagios|command}
		command=${2:# REQUIRED}
		services=${3:# REQUIRED}
		host=${4}
		author=${5:ansible}
		minutes=${6:30}
		cmdfile=${7:auto-detected}

snippet newrelic_deployment
	newrelic_deployment: >
		token=${1:# REQUIRED}
		application_id=${2}
		description=${3}
		changelog=${4}
		appname=${5}
		environment=${6}
		user=${7}
		revision=${8}
		validate_certs=${9:#yes|no}
		app_name=${10}

snippet librato_annotation
	librato_annotation: >
		links=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		user=${4:# REQUIRED}
		description=${5}
		start_time=${6}
		name=${7}
		source=${8}
		end_time=${9}

snippet stackdriver
	stackdriver: >
		key=${1:# REQUIRED}
		repository=${2}
		level=${3:#INFO|WARN|ERROR}
		annotated_by=${4:ansible}
		deployed_to=${5}
		deployed_by=${6:ansible}
		instance_id=${7}
		msg=${8}
		event_epoch=${9}
		revision_id=${10}
		event=${11:#annotation|deploy}

snippet pagerduty
	pagerduty: >
		name=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|started|ongoing}
		user=${4:# REQUIRED}
		service=${5}
		hours=${6:1}
		validate_certs=${7:#yes|no}
		desc=${8:created by ansible}

snippet boundary_meter
	boundary_meter: >
		apikey=${1:# REQUIRED}
		apiid=${2:# REQUIRED}
		name=${3:# REQUIRED}
		state=${4:#present|absent}
		validate_certs=${5:#yes|no}

snippet airbrake_deployment
	airbrake_deployment: >
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		repo=${3}
		user=${4}
		url=${5:https://airbrake.io/deploys}
		validate_certs=${6:#yes|no}
		revision=${7}

snippet pingdom
	pingdom: >
		checkid=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|paused}
		uid=${4:# REQUIRED}
		key=${5:# REQUIRED}

snippet datadog_event
	datadog_event: >
		text=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		date_happened=${4:now}
		alert_type=${5:#error|warning|info|success}
		tags=${6}
		priority=${7:#normal|low}
		aggregation_key=${8}
		validate_certs=${9:#yes|no}

snippet monit
	monit: state=${1:#present|started|stopped|restarted|monitored|unmonitored|reloaded} name=${2:# REQUIRED}

snippet redhat_subscription
	redhat_subscription: >
		username=${1}
		server_hostname=${2:current value from c(/etc/rhsm/rhsm.conf) is the default}
		state=${3:#present|absent}
		autosubscribe=${4:false}
		activationkey=${5}
		server_insecure=${6:current value from c(/etc/rhsm/rhsm.conf) is the default}
		password=${7}
		rhsm_baseurl=${8:current value from c(/etc/rhsm/rhsm.conf) is the default}
		pool=${9:^$}

snippet zypper
	zypper: name=${1:# REQUIRED} state=${2:#present|latest|absent} disable_gpg_check=${3:#yes|no}

snippet homebrew_cask
	homebrew_cask: name=${1:# REQUIRED} state=${2:#installed|uninstalled}

snippet yum
	yum: >
		name=${1:# REQUIRED}
		state=${2:#present|latest|absent}
		disablerepo=${3}
		enablerepo=${4}
		list=${5}
		disable_gpg_check=${6:#yes|no}
		conf_file=${7}

snippet apt_repository
	apt_repository: >
		repo=${1:# REQUIRED}
		update_cache=${2:#yes|no}
		state=${3:#absent|present}
		validate_certs=${4:#yes|no}
		mode=${5:420}

snippet pkgutil
	pkgutil: state=${1:#present|absent|latest} name=${2:# REQUIRED} site=${3}

snippet apt
	apt: >
		dpkg_options=${1:force-confdef,force-confold}
		upgrade=${2:#yes|safe|full|dist}
		force=${3:#yes|no}
		name=${4}
		purge=${5:#yes|no}
		state=${6:#latest|absent|present}
		update_cache=${7:#yes|no}
		default_release=${8}
		cache_valid_time=${9:false}
		deb=${10}
		install_recommends=${11:#yes|no}

snippet svr4pkg
	svr4pkg: >
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		category=${3:#true|false}
		src=${4}
		zone=${5:#current|all}
		response_file=${6}
		proxy=${7}

snippet npm
	npm: >
		executable=${1}
		name=${2}
		global=${3:#yes|no}
		state=${4:#present|absent|latest}
		production=${5:#yes|no}
		registry=${6}
		version=${7}
		path=${8}

snippet pkgng
	pkgng: >
		name=${1:# REQUIRED}
		cached=${2:#yes|no}
		state=${3:#present|absent}
		pkgsite=${4}
		annotation=${5}

snippet openbsd_pkg
	openbsd_pkg: state=${1:#present|latest|absent} name=${2:# REQUIRED}

snippet gem
	gem: >
		name=${1:# REQUIRED}
		include_dependencies=${2:#yes|no}
		executable=${3}
		repository=${4}
		user_install=${5:yes}
		pre_release=${6:no}
		state=${7:#present|absent|latest}
		version=${8}
		gem_source=${9}

snippet layman
	layman: name=${1:# REQUIRED} list_url=${2} state=${3:#present|absent|updated}

snippet composer
	composer: >
		working_dir=${1:# REQUIRED}
		prefer_dist=${2:#yes|no}
		prefer_source=${3:#yes|no}
		no_scripts=${4:#yes|no}
		no_dev=${5:#yes|no}
		no_plugins=${6:#yes|no}
		optimize_autoloader=${7:#yes|no}

snippet pkgin
	pkgin: name=${1:# REQUIRED} state=${2:#present|absent}

snippet zypper_repository
	zypper_repository: >
		repo=${1}
		state=${2:#absent|present}
		description=${3}
		disable_gpg_check=${4:#yes|no}
		name=${5}

snippet pip
	pip: >
		virtualenv=${1}
		virtualenv_site_packages=${2:#yes|no}
		virtualenv_command=${3:virtualenv}
		chdir=${4}
		requirements=${5}
		name=${6}
		executable=${7}
		extra_args=${8}
		state=${9:#present|absent|latest}
		version=${10}

snippet cpanm
	cpanm: >
		notest=${1:false}
		from_path=${2}
		name=${3}
		locallib=${4:false}
		mirror=${5:false}

snippet homebrew_tap
	homebrew_tap: tap=${1:# REQUIRED} state=${2:#present|absent}

snippet portinstall
	portinstall: name=${1:# REQUIRED} state=${2:#present|absent} use_packages=${3:#yes|no}

snippet homebrew
	homebrew: >
		name=${1:# REQUIRED}
		update_homebrew=${2:#yes|no}
		install_options=${3}
		state=${4:#head|latest|present|absent|linked|unlinked}
		upgrade_all=${5:#yes|no}

snippet rhn_channel
	rhn_channel: >
		sysname=${1:# REQUIRED}
		name=${2:# REQUIRED}
		url=${3:# REQUIRED}
		password=${4:# REQUIRED}
		user=${5:# REQUIRED}
		state=${6:present}

snippet apt_key
	apt_key: >
		keyserver=${1}
		url=${2}
		data=${3}
		keyring=${4}
		state=${5:#absent|present}
		file=${6}
		validate_certs=${7:#yes|no}
		id=${8}

snippet opkg
	opkg: name=${1:# REQUIRED} state=${2:#present|absent} update_cache=${3:#yes|no}

snippet rhn_register
	rhn_register: >
		username=${1}
		channels=${2:[]}
		state=${3:#present|absent}
		activationkey=${4}
		password=${5}
		server_url=${6:current value of i(serverurl) from c(/etc/sysconfig/rhn/up2date) is the default}

snippet easy_install
	easy_install: >
		name=${1:# REQUIRED}
		virtualenv=${2}
		virtualenv_site_packages=${3:#yes|no}
		virtualenv_command=${4:virtualenv}
		executable=${5}

snippet swdepot
	swdepot: state=${1:#present|latest|absent} name=${2:# REQUIRED} depot=${3}

snippet rpm_key
	rpm_key: key=${1:# REQUIRED} state=${2:#present|absent} validate_certs=${3:#yes|no}

snippet portage
	portage: >
		nodeps=${1:#yes}
		onlydeps=${2:#yes}
		newuse=${3:#yes}
		package=${4}
		oneshot=${5:#yes}
		update=${6:#yes}
		deep=${7:#yes}
		quiet=${8:#yes}
		sync=${9:#yes|web}
		state=${10:#present|installed|emerged|absent|removed|unmerged}
		depclean=${11:#yes}
		noreplace=${12:#yes}
		verbose=${13:#yes}

snippet macports
	macports: name=${1:# REQUIRED} state=${2:#present|absent|active|inactive} update_cache=${3:#yes|no}

snippet pacman
	pacman: recurse=${1:#yes|no} state=${2:#present|absent} update_cache=${3:#yes|no} name=${4}

snippet apt_rpm
	apt_rpm: pkg=${1:# REQUIRED} state=${2:#absent|present} update_cache=${3:#yes|no}

snippet urpmi
	urpmi: >
		pkg=${1:# REQUIRED}
		force=${2:#yes|no}
		state=${3:#absent|present}
		no-suggests=${4:#yes|no}
		update_cache=${5:#yes|no}

snippet add_host
	add_host: name=${1:# REQUIRED} groups=${2}

snippet group_by
	group_by: key=${1:# REQUIRED}

snippet win_feature
	win_feature: >
		name=${1:# REQUIRED}
		include_management_tools=${2:#True|False}
		include_sub_features=${3:#True|False}
		state=${4:#present|absent}
		restart=${5:#True|False}

snippet win_stat
	win_stat: path=${1:# REQUIRED} get_md5=${2:true}

snippet win_service
	win_service: name=${1:# REQUIRED} start_mode=${2:#auto|manual|disabled} state=${3:#started|stopped|restarted}

snippet win_get_url
	win_get_url: url=${1:# REQUIRED} dest=${2:true}

snippet win_group
	win_group: name=${1:# REQUIRED} state=${2:#present|absent} description=${3}

snippet slurp
	slurp: src=${1:# REQUIRED}

snippet win_user
	win_user: password=${1:# REQUIRED} name=${2:# REQUIRED} state=${3:#present|absent}

snippet win_ping
	win_ping: data=${1:pong}

snippet setup
	setup: filter=${1:*} fact_path=${2:/etc/ansible/facts.d}

snippet win_msi
	win_msi: path=${1:# REQUIRED} creates=${2} state=${3:#present|absent}

snippet mail
	mail: >
		subject=${1:# REQUIRED}
		body=${2:$subject}
		from=${3:root}
		to=${4:root}
		headers=${5}
		cc=${6}
		charset=${7:us-ascii}
		bcc=${8}
		attach=${9}
		host=${10:localhost}
		port=${11:25}

snippet sns
	sns: >
		topic=${1:# REQUIRED}
		msg=${2:# REQUIRED}
		aws_secret_key=${3}
		aws_access_key=${4}
		http=${5}
		sqs=${6}
		region=${7}
		sms=${8}
		https=${9}
		email=${10}
		subject=${11}

snippet twilio
	twilio: >
		msg=${1:# REQUIRED}
		auth_token=${2:# REQUIRED}
		from_number=${3:# REQUIRED}
		to_number=${4:# REQUIRED}
		account_sid=${5:# REQUIRED}

snippet osx_say
	osx_say: msg=${1:# REQUIRED} voice=${2}

snippet grove
	grove: >
		message=${1:# REQUIRED}
		channel_token=${2:# REQUIRED}
		service=${3:ansible}
		url=${4}
		icon_url=${5}
		validate_certs=${6:#yes|no}

snippet hipchat
	hipchat: >
		room=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from=${4:ansible}
		color=${5:#yellow|red|green|purple|gray|random}
		msg_format=${6:#text|html}
		api=${7:https://api.hipchat.com/v1/rooms/message}
		notify=${8:#yes|no}
		validate_certs=${9:#yes|no}

snippet jabber
	jabber: >
		to=${1:# REQUIRED}
		user=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		password=${4:# REQUIRED}
		host=${5}
		encoding=${6}
		port=${7:5222}

snippet slack
	slack: >
		domain=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		username=${4:ansible}
		icon_url=${5}
		parse=${6:#full|none}
		icon_emoji=${7}
		link_names=${8:#1|0}
		validate_certs=${9:#yes|no}
		channel=${10}

snippet flowdock
	flowdock: >
		type=${1:#inbox|chat}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from_name=${4}
		from_address=${5}
		tags=${6}
		external_user_name=${7}
		project=${8}
		source=${9}
		link=${10}
		reply_to=${11}
		subject=${12}
		validate_certs=${13:#yes|no}

snippet typetalk
	typetalk: topic=${1:# REQUIRED} client_secret=${2:# REQUIRED} client_id=${3:# REQUIRED} msg=${4:# REQUIRED}

snippet irc
	irc: >
		msg=${1:# REQUIRED}
		channel=${2:# REQUIRED}
		key=${3}
		color=${4:#none|yellow|red|green|blue|black}
		server=${5:localhost}
		nick=${6:ansible}
		passwd=${7}
		timeout=${8:30}
		port=${9:6667}

snippet campfire
	campfire: >
		msg=${1:# REQUIRED}
		token=${2:# REQUIRED}
		subscription=${3:# REQUIRED}
		room=${4:# REQUIRED}
		notify=${5:#56k|bell|bezos|bueller|clowntown|cottoneyejoe|crickets|dadgummit|dangerzone|danielsan|deeper|drama|greatjob|greyjoy|guarantee|heygirl|horn|horror|inconceivable|live|loggins|makeitso|noooo|nyan|ohmy|ohyeah|pushit|rimshot|rollout|rumble|sax|secret|sexyback|story|tada|tmyk|trololo|trombone|unix|vuvuzela|what|whoomp|yeah|yodel}

snippet nexmo
	nexmo: >
		src=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		api_secret=${3:# REQUIRED}
		api_key=${4:# REQUIRED}
		msg=${5:# REQUIRED}
		validate_certs=${6:#yes|no}

snippet mqtt
	mqtt: >
		topic=${1:# REQUIRED}
		payload=${2:# REQUIRED}
		username=${3}
		qos=${4:#0|1|2}
		port=${5:1883}
		server=${6:localhost}
		client_id=${7:hostname + pid}
		retain=${8:false}
		password=${9}

snippet async_status
	async_status: jid=${1:# REQUIRED} mode=${2:#status|cleanup}

snippet apache2_module
	apache2_module: name=${1:# REQUIRED} state=${2:#present|absent}

snippet jira
	jira: >
		username=${1:# REQUIRED}
		uri=${2:# REQUIRED}
		operation=${3:#create|comment|edit|fetch|transition}
		password=${4:# REQUIRED}
		comment=${5}
		description=${6}
		fields=${7}
		summary=${8}
		project=${9}
		assignee=${10}
		status=${11}
		issuetype=${12}
		issue=${13}

snippet ejabberd_user
	ejabberd_user: >
		username=${1:# REQUIRED}
		host=${2:# REQUIRED}
		password=${3}
		logging=${4:#true|false|yes|no}
		state=${5:#present|absent}

snippet jboss
	jboss: deployment=${1:# REQUIRED} src=${2} deploy_path=${3:/var/lib/jbossas/standalone/deployments} state=${4:#present|absent}

snippet django_manage
	django_manage: >
		app_path=${1:# REQUIRED}
		command=${2:#cleanup|collectstatic|flush|loaddata|migrate|runfcgi|syncdb|test|validate}
		virtualenv=${3}
		settings=${4}
		pythonpath=${5}
		database=${6}
		apps=${7}
		cache_table=${8}
		merge=${9}
		skip=${10}
		link=${11}
		fixtures=${12}
		failfast=${13:#yes|no}

snippet supervisorctl
	supervisorctl: >
		state=${1:#present|started|stopped|restarted}
		name=${2:# REQUIRED}
		username=${3}
		supervisorctl_path=${4}
		password=${5}
		config=${6}
		server_url=${7}

snippet htpasswd
	htpasswd: >
		name=${1:# REQUIRED}
		path=${2:# REQUIRED}
		state=${3:#present|absent}
		create=${4:#yes|no}
		password=${5}
		crypt_scheme=${6:#apr_md5_crypt|des_crypt|ldap_sha1|plaintext}

snippet rabbitmq_parameter
	rabbitmq_parameter: >
		name=${1:# REQUIRED}
		component=${2:# REQUIRED}
		node=${3:rabbit}
		vhost=${4:/}
		state=${5:#present|absent}
		value=${6}

snippet rabbitmq_policy
	rabbitmq_policy: >
		name=${1:# REQUIRED}
		tags=${2:# REQUIRED}
		pattern=${3:# REQUIRED}
		node=${4:rabbit}
		priority=${5:0}
		state=${6:#present|absent}
		vhost=${7:/}

snippet rabbitmq_plugin
	rabbitmq_plugin: names=${1:# REQUIRED} state=${2:#enabled|disabled} new_only=${3:#yes|no} prefix=${4}

snippet rabbitmq_user
	rabbitmq_user: >
		user=${1:# REQUIRED}
		node=${2:rabbit}
		force=${3:#yes|no}
		tags=${4}
		read_priv=${5:^$}
		write_priv=${6:^$}
		state=${7:#present|absent}
		configure_priv=${8:^$}
		vhost=${9:/}
		password=${10}

snippet rabbitmq_vhost
	rabbitmq_vhost: name=${1:# REQUIRED} node=${2:rabbit} tracing=${3:#yes|no} state=${4:#present|absent}

snippet raw
	raw: ${1} executable=${2}

snippet script
	script: ${1} creates=${2} removes=${3}

snippet command
	command: ${1} >
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet shell
	shell: ${1} >
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet stat
	stat: path=${1:# REQUIRED} get_md5=${2:true} follow=${3:false}

snippet acl
	acl: >
		name=${1:# REQUIRED}
		default=${2:#yes|no}
		entity=${3}
		state=${4:#query|present|absent}
		follow=${5:#yes|no}
		etype=${6:#user|group|mask|other}
		entry=${7}
		permissions=${8}

snippet unarchive
	unarchive: dest=${1:# REQUIRED} src=${2:# REQUIRED} copy=${3:#yes|no} creates=${4}

snippet fetch
	fetch: >
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		validate_md5=${3:#yes|no}
		fail_on_missing=${4:#yes|no}
		flat=${5}

snippet file
	file: >
		path=${1:[]}
		src=${2}
		serole=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:#yes|no}
		setype=${8}
		group=${9}
		state=${10:#file|link|directory|hard|touch|absent}
		mode=${11}
		owner=${12}

snippet xattr
	xattr: >
		name=${1:# REQUIRED}
		key=${2}
		follow=${3:#yes|no}
		state=${4:#read|present|all|keys|absent}
		value=${5}

snippet copy
	copy: >
		dest=${1:# REQUIRED}
		src=${2}
		directory_mode=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:false}
		serole=${8}
		content=${9}
		setype=${10}
		mode=${11}
		owner=${12}
		group=${13}
		validate=${14}
		backup=${15:#yes|no}

snippet synchronize
	synchronize: >
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		dirs=${3:#yes|no}
		links=${4:#yes|no}
		copy_links=${5:#yes|no}
		compress=${6:#yes|no}
		rsync_timeout=${7:0}
		rsync_opts=${8}
		owner=${9:#yes|no}
		set_remote_user=${10:true}
		rsync_path=${11}
		recursive=${12:#yes|no}
		group=${13:#yes|no}
		existing_only=${14:#yes|no}
		archive=${15:#yes|no}
		checksum=${16:#yes|no}
		times=${17:#yes|no}
		perms=${18:#yes|no}
		mode=${19:#push|pull}
		dest_port=${20:22}
		delete=${21:#yes|no}

snippet template
	template: dest=${1:# REQUIRED} src=${2:# REQUIRED} validate=${3} backup=${4:#yes|no}

snippet bigip_node
	bigip_node: >
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		name=${6}
		partition=${7:common}
		description=${8}

snippet bigip_monitor_http
	bigip_monitor_http: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		receive_disable=${3:# REQUIRED}
		name=${4:# REQUIRED}
		receive=${5:# REQUIRED}
		send=${6:# REQUIRED}
		server=${7:# REQUIRED}
		interval=${8}
		parent=${9:http}
		ip=${10}
		port=${11}
		partition=${12:common}
		state=${13:#present|absent}
		time_until_up=${14}
		timeout=${15}
		parent_partition=${16:common}

snippet arista_vlan
	arista_vlan: vlan_id=${1:# REQUIRED} state=${2:#present|absent} logging=${3:#true|false|yes|no} name=${4}

snippet bigip_monitor_tcp
	bigip_monitor_tcp: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		name=${3:# REQUIRED}
		receive=${4:# REQUIRED}
		send=${5:# REQUIRED}
		server=${6:# REQUIRED}
		interval=${7}
		parent=${8:#tcp|tcp_echo|tcp_half_open}
		ip=${9}
		port=${10}
		partition=${11:common}
		state=${12:#present|absent}
		time_until_up=${13}
		timeout=${14}
		parent_partition=${15:common}
		type=${16:#TTYPE_TCP|TTYPE_TCP_ECHO|TTYPE_TCP_HALF_OPEN}

snippet openvswitch_bridge
	openvswitch_bridge: bridge=${1:# REQUIRED} state=${2:#present|absent} timeout=${3:5}

snippet dnsimple
	dnsimple: >
		solo=${1}
		domain=${2}
		account_email=${3}
		record_ids=${4}
		value=${5}
		priority=${6}
		record=${7}
		state=${8:#present|absent}
		ttl=${9:3600 (one hour)}
		type=${10:#A|ALIAS|CNAME|MX|SPF|URL|TXT|NS|SRV|NAPTR|PTR|AAAA|SSHFP|HINFO|POOL}
		account_api_token=${11}

snippet dnsmadeeasy
	dnsmadeeasy: >
		domain=${1:# REQUIRED}
		account_secret=${2:# REQUIRED}
		account_key=${3:# REQUIRED}
		state=${4:#present|absent}
		record_name=${5}
		record_ttl=${6:1800}
		record_type=${7:#A|AAAA|CNAME|HTTPRED|MX|NS|PTR|SRV|TXT}
		record_value=${8}
		validate_certs=${9:#yes|no}

snippet openvswitch_port
	openvswitch_port: bridge=${1:# REQUIRED} port=${2:# REQUIRED} state=${3:#present|absent} timeout=${4:5}

snippet bigip_pool_member
	bigip_pool_member: >
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		port=${6:# REQUIRED}
		pool=${7:# REQUIRED}
		ratio=${8}
		description=${9}
		connection_limit=${10}
		partition=${11:common}
		rate_limit=${12}

snippet arista_lag
	arista_lag: >
		interface_id=${1:# REQUIRED}
		lacp=${2:#active|passive|off}
		state=${3:#present|absent}
		minimum_links=${4}
		logging=${5:#true|false|yes|no}
		links=${6}

snippet arista_interface
	arista_interface: >
		interface_id=${1:# REQUIRED}
		duplex=${2:#auto|half|full}
		logging=${3:#true|false|yes|no}
		description=${4}
		admin=${5:#up|down}
		speed=${6:#auto|100m|1g|10g}
		mtu=${7:1500}

snippet bigip_facts
	bigip_facts: >
		include=${1:#address_class|certificate|client_ssl_profile|device_group|interface|key|node|pool|rule|self_ip|software|system_info|traffic_group|trunk|virtual_address|virtual_server|vlan}
		user=${2:# REQUIRED}
		password=${3:# REQUIRED}
		server=${4:# REQUIRED}
		filter=${5}
		session=${6:true}

snippet arista_l2interface
	arista_l2interface: >
		interface_id=${1:# REQUIRED}
		state=${2:#present|absent}
		logging=${3:#true|false|yes|no}
		tagged_vlans=${4}
		vlan_tagging=${5:#enable|disable}
		untagged_vlan=${6:default}

snippet bigip_pool
	bigip_pool: >
		name=${1:# REQUIRED}
		server=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		lb_method=${5:#round_robin|ratio_member|least_connection_member|observed_member|predictive_member|ratio_node_address|least_connection_node_address|fastest_node_address|observed_node_address|predictive_node_address|dynamic_ratio|fastest_app_response|least_sessions|dynamic_ratio_member|l3_addr|unknown|weighted_least_connection_member|weighted_least_connection_node_address|ratio_session|ratio_least_connection_member|ratio_least_connection_node_address}
		quorum=${6}
		partition=${7:common}
		slow_ramp_time=${8}
		state=${9:#present|absent}
		service_down_action=${10:#none|reset|drop|reselect}
		port=${11}
		host=${12}
		monitors=${13}
		monitor_type=${14:#and_list|m_of_n}

snippet netscaler
	netscaler: >
		name=${1:hostname}
		nsc_host=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		type=${5:#server|service}
		nsc_protocol=${6:https}
		action=${7:#enable|disable}
		validate_certs=${8:#yes|no}

snippet accelerate
	accelerate: >
		timeout=${1:300}
		minutes=${2:30}
		port=${3:5099}
		multi_key=${4:false}
		ipv6=${5:false}

snippet debug
	debug: msg=${1:hello world!} var=${2}

snippet wait_for
	wait_for: >
		delay=${1:0}
		state=${2:#present|started|stopped|absent}
		timeout=${3:300}
		search_regex=${4}
		path=${5}
		host=${6:127.0.0.1}
		port=${7}

snippet assert
	assert: that=${1:# REQUIRED}

snippet set_fact
	set_fact: key_value=${1:# REQUIRED}

snippet pause
	pause: seconds=${1} minutes=${2} prompt=${3}

snippet include_vars
	include_vars: ${1}

snippet fail
	fail: msg=${1:'failed as requested from task'}

snippet fireball
	fireball: minutes=${1:30} port=${2:5099}

snippet mount
	mount: >
		src=${1:# REQUIRED}
		name=${2:# REQUIRED}
		fstype=${3:# REQUIRED}
		state=${4:#present|absent|mounted|unmounted}
		dump=${5}
		fstab=${6:/etc/fstab}
		passno=${7}
		opts=${8}

snippet seboolean
	seboolean: state=${1:#yes|no} name=${2:# REQUIRED} persistent=${3:#yes|no}

snippet at
	at: >
		count=${1:# REQUIRED}
		units=${2:#minutes|hours|days|weeks}
		state=${3:#present|absent}
		command=${4}
		unique=${5:false}
		script_file=${6}

snippet authorized_key
	authorized_key: >
		user=${1:# REQUIRED}
		key=${2:# REQUIRED}
		key_options=${3}
		state=${4:#present|absent}
		path=${5:(homedir)+/.ssh/authorized_keys}
		manage_dir=${6:#yes|no}

snippet locale_gen
	locale_gen: name=${1:# REQUIRED} state=${2:#present|absent}

snippet user
	user: >
		name=${1:# REQUIRED}
		comment=${2}
		ssh_key_bits=${3:2048}
		update_password=${4:#always|on_create}
		non_unique=${5:#yes|no}
		force=${6:#yes|no}
		ssh_key_type=${7:rsa}
		ssh_key_passphrase=${8}
		groups=${9}
		home=${10}
		move_home=${11:#yes|no}
		password=${12}
		generate_ssh_key=${13:#yes|no}
		append=${14:#yes|no}
		uid=${15}
		ssh_key_comment=${16:ansible-generated}
		group=${17}
		createhome=${18:#yes|no}
		system=${19:#yes|no}
		remove=${20:#yes|no}
		state=${21:#present|absent}
		ssh_key_file=${22:$home/.ssh/id_rsa}
		login_class=${23}
		shell=${24}

snippet cron
	cron: >
		name=${1}
		hour=${2:*}
		job=${3}
		cron_file=${4}
		reboot=${5:#yes|no}
		month=${6:*}
		state=${7:#present|absent}
		special_time=${8:#reboot|yearly|annually|monthly|weekly|daily|hourly}
		user=${9:root}
		backup=${10:false}
		day=${11:*}
		minute=${12:*}
		weekday=${13:*}

snippet lvol
	lvol: >
		lv=${1:# REQUIRED}
		vg=${2:# REQUIRED}
		state=${3:#present|absent}
		force=${4:#yes|no}
		size=${5}

snippet debconf
	debconf: >
		name=${1:# REQUIRED}
		value=${2}
		vtype=${3:#string|boolean|select|multiselect|note|text|password|title}
		question=${4}
		unseen=${5:false}

snippet firewalld
	firewalld: >
		state=${1:enabled}
		permanent=${2:true}
		zone=${3:#work|drop|internal|external|trusted|home|dmz|public|block}
		service=${4}
		timeout=${5:0}
		rich_rule=${6}
		port=${7}

snippet capabilities
	capabilities: capability=${1:# REQUIRED} path=${2:# REQUIRED} state=${3:#present|absent}

snippet group
	group: name=${1:# REQUIRED} state=${2:#present|absent} gid=${3} system=${4:#yes|no}

snippet modprobe
	modprobe: name=${1:# REQUIRED} state=${2:#present|absent} params=${3}

snippet alternatives
	alternatives: path=${1:# REQUIRED} name=${2:# REQUIRED} link=${3}

snippet filesystem
	filesystem: dev=${1:# REQUIRED} fstype=${2:# REQUIRED} force=${3:#yes|no} opts=${4}

snippet sysctl
	sysctl: >
		name=${1:# REQUIRED}
		reload=${2:#yes|no}
		state=${3:#present|absent}
		sysctl_set=${4:#yes|no}
		ignoreerrors=${5:#yes|no}
		sysctl_file=${6:/etc/sysctl.conf}
		value=${7}

snippet hostname
	hostname: name=${1:# REQUIRED}

snippet kernel_blacklist
	kernel_blacklist: name=${1:# REQUIRED} blacklist_file=${2} state=${3:#present|absent}

snippet lvg
	lvg: >
		vg=${1:# REQUIRED}
		vg_options=${2}
		pvs=${3}
		force=${4:#yes|no}
		pesize=${5:4}
		state=${6:#present|absent}

snippet ufw
	ufw: >
		insert=${1}
		direction=${2:#in|out|incoming|outgoing}
		from_port=${3}
		logging=${4:#on|off|low|medium|high|full}
		log=${5:#yes|no}
		proto=${6:#any|tcp|udp|ipv6|esp|ah}
		to_port=${7}
		from_ip=${8:any}
		rule=${9:#allow|deny|reject|limit}
		name=${10}
		policy=${11:#allow|deny|reject}
		state=${12:#enabled|disabled|reloaded|reset}
		interface=${13}
		to_ip=${14:any}
		delete=${15:#yes|no}

snippet service
	service: >
		name=${1:# REQUIRED}
		state=${2:#started|stopped|restarted|reloaded}
		sleep=${3}
		runlevel=${4:default}
		pattern=${5}
		enabled=${6:#yes|no}
		arguments=${7}

snippet zfs
	zfs: >
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		setuid=${3:#on|off}
		zoned=${4:#on|off}
		primarycache=${5:#all|none|metadata}
		logbias=${6:#latency|throughput}
		sync=${7:#on|off}
		copies=${8:#1|2|3}
		sharenfs=${9}
		sharesmb=${10}
		canmount=${11:#on|off|noauto}
		mountpoint=${12}
		casesensitivity=${13:#sensitive|insensitive|mixed}
		utf8only=${14:#on|off}
		xattr=${15:#on|off}
		compression=${16:#on|off|lzjb|gzip|gzip-1|gzip-2|gzip-3|gzip-4|gzip-5|gzip-6|gzip-7|gzip-8|gzip-9|lz4|zle}
		shareiscsi=${17:#on|off}
		aclmode=${18:#discard|groupmask|passthrough}
		exec=${19:#on|off}
		dedup=${20:#on|off}
		aclinherit=${21:#discard|noallow|restricted|passthrough|passthrough-x}
		readonly=${22:#on|off}
		recordsize=${23}
		jailed=${24:#on|off}
		secondarycache=${25:#all|none|metadata}
		refquota=${26}
		quota=${27}
		volsize=${28}
		vscan=${29:#on|off}
		reservation=${30}
		atime=${31:#on|off}
		normalization=${32:#none|formC|formD|formKC|formKD}
		volblocksize=${33}
		checksum=${34:#on|off|fletcher2|fletcher4|sha256}
		devices=${35:#on|off}
		nbmand=${36:#on|off}
		refreservation=${37}
		snapdir=${38:#hidden|visible}

snippet open_iscsi
	open_iscsi: >
		auto_node_startup=${1:#True|False}
		target=${2}
		show_nodes=${3:#True|False}
		node_auth=${4:chap}
		node_pass=${5}
		discover=${6:#True|False}
		portal=${7}
		login=${8:#True|False}
		node_user=${9}
		port=${10:3260}

snippet selinux
	selinux: state=${1:#enforcing|permissive|disabled} policy=${2} conf=${3:/etc/selinux/config}

snippet hg
	hg: >
		repo=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		purge=${3:#yes|no}
		executable=${4}
		force=${5:#yes|no}
		revision=${6:default}

snippet git
	git: >
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		executable=${3}
		remote=${4:origin}
		recursive=${5:#yes|no}
		reference=${6}
		accept_hostkey=${7:#yes|no}
		update=${8:#yes|no}
		ssh_opts=${9}
		depth=${10}
		version=${11:head}
		bare=${12:#yes|no}
		force=${13:#yes|no}
		key_file=${14}

snippet bzr
	bzr: >
		dest=${1:# REQUIRED}
		name=${2:# REQUIRED}
		executable=${3}
		version=${4:head}
		force=${5:#yes|no}

snippet subversion
	subversion: >
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		username=${3}
		executable=${4}
		force=${5:#yes|no}
		export=${6:#yes|no}
		password=${7}
		revision=${8:head}

snippet github_hooks
	github_hooks: >
		repo=${1:# REQUIRED}
		oauthkey=${2:# REQUIRED}
		user=${3:# REQUIRED}
		action=${4:#create|cleanall}
		validate_certs=${5:#yes|no}
		hookurl=${6}

snippet digital_ocean_sshkey
	digital_ocean_sshkey: >
		state=${1:#present|absent}
		name=${2}
		client_id=${3}
		api_key=${4}
		id=${5}
		ssh_pub_key=${6}

snippet ovirt
	ovirt: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		url=${3:# REQUIRED}
		instance_name=${4:# REQUIRED}
		instance_mem=${5}
		instance_cores=${6:1}
		instance_cpus=${7:1}
		image=${8}
		instance_disksize=${9}
		instance_nic=${10}
		instance_network=${11:rhevm}
		sdomain=${12}
		instance_os=${13}
		zone=${14}
		disk_alloc=${15:#thin|preallocated}
		region=${16}
		instance_type=${17:#server|desktop}
		state=${18:#present|absent|shutdown|started|restarted}
		resource_type=${19:#new|template}
		disk_int=${20:#virtio|ide}

snippet ec2_ami
	ec2_ami: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		security_token=${5}
		delete_snapshot=${6}
		region=${7}
		state=${8:present}
		instance_id=${9}
		image_id=${10}
		no_reboot=${11:#yes|no}
		wait_timeout=${12:300}
		ec2_url=${13}
		wait=${14:#yes|no}
		validate_certs=${15:#yes|no}
		description=${16}

snippet ec2_metric_alarm
	ec2_metric_alarm: >
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		comparison=${4}
		alarm_actions=${5}
		ok_actions=${6}
		security_token=${7}
		evaluation_periods=${8}
		metric=${9}
		description=${10}
		namespace=${11}
		period=${12}
		ec2_url=${13}
		profile=${14}
		insufficient_data_actions=${15}
		statistic=${16}
		threshold=${17}
		aws_access_key=${18}
		validate_certs=${19:#yes|no}
		unit=${20}
		dimensions=${21}

snippet elasticache
	elasticache: >
		name=${1:# REQUIRED}
		state=${2:#present|absent|rebooted}
		engine=${3:memcached}
		aws_secret_key=${4}
		cache_port=${5:11211}
		security_group_ids=${6:['default']}
		cache_engine_version=${7:1.4.14}
		region=${8}
		num_nodes=${9}
		node_type=${10:cache.m1.small}
		cache_security_groups=${11:['default']}
		hard_modify=${12:#yes|no}
		aws_access_key=${13}
		zone=${14}
		wait=${15:#yes|no}

snippet ec2_lc
	ec2_lc: >
		name=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		spot_price=${7}
		security_token=${8}
		key_name=${9}
		region=${10}
		user_data=${11}
		image_id=${12}
		volumes=${13}
		ec2_url=${14}
		instance_monitoring=${15:false}
		validate_certs=${16:#yes|no}
		security_groups=${17}

snippet quantum_router_gateway
	quantum_router_gateway: >
		router_name=${1:# REQUIRED}
		login_tenant_name=${2:yes}
		login_password=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet quantum_floating_ip_associate
	quantum_floating_ip_associate: >
		instance_name=${1:# REQUIRED}
		login_tenant_name=${2:true}
		login_password=${3:yes}
		login_username=${4:admin}
		ip_address=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2_key
	ec2_key: >
		name=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		key_material=${7}
		state=${8:present}
		wait_timeout=${9:300}
		ec2_url=${10}
		validate_certs=${11:#yes|no}
		wait=${12:false}

snippet ec2_ami_search
	ec2_ami_search: >
		release=${1:# REQUIRED}
		distro=${2:#ubuntu}
		stream=${3:#server|desktop}
		virt=${4:#paravirtual|hvm}
		region=${5:#ap-northeast-1|ap-southeast-1|ap-southeast-2|eu-west-1|sa-east-1|us-east-1|us-west-1|us-west-2}
		arch=${6:#i386|amd64}
		store=${7:#ebs|instance-store}

snippet gce_lb
	gce_lb: >
		httphealthcheck_host=${1}
		protocol=${2:#tcp|udp}
		pem_file=${3}
		members=${4}
		httphealthcheck_port=${5:80}
		httphealthcheck_name=${6}
		name=${7}
		external_ip=${8}
		service_account_email=${9}
		region=${10}
		httphealthcheck_unhealthy_count=${11:2}
		httphealthcheck_healthy_count=${12:2}
		httphealthcheck_path=${13:/}
		port_range=${14}
		state=${15:#active|present|absent|deleted}
		httphealthcheck_timeout=${16:5}
		project_id=${17}
		httphealthcheck_interval=${18:5}

snippet rax_files_objects
	rax_files_objects: >
		container=${1:# REQUIRED}
		username=${2}
		src=${3}
		dest=${4}
		region=${5:dfw}
		expires=${6}
		verify_ssl=${7}
		state=${8:#present|absent}
		clear_meta=${9:#yes|no}
		meta=${10}
		env=${11}
		credentials=${12}
		api_key=${13}
		type=${14:#file|meta}
		method=${15:#get|put|delete}
		structure=${16:#True|no}

snippet quantum_router
	quantum_router: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		admin_state_up=${6:true}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet azure
	azure: >
		image=${1:# REQUIRED}
		storage_account=${2:# REQUIRED}
		name=${3:# REQUIRED}
		location=${4:# REQUIRED}
		role_size=${5:small}
		virtual_network_name=${6}
		wait_timeout_redirects=${7:300}
		wait_timeout=${8:600}
		user=${9}
		password=${10}
		wait=${11:#yes|no}
		management_cert_path=${12}
		hostname=${13}
		ssh_cert_path=${14}
		state=${15:present}
		subscription_id=${16}
		endpoints=${17:22}

snippet gce_net
	gce_net: >
		fwname=${1}
		name=${2}
		src_range=${3}
		allowed=${4}
		src_tags=${5}
		pem_file=${6}
		state=${7:#active|present|absent|deleted}
		service_account_email=${8}
		ipv4_range=${9}
		project_id=${10}

snippet rds_subnet_group
	rds_subnet_group: >
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		subnets=${5}
		aws_access_key=${6}
		description=${7}

snippet rax_clb_nodes
	rax_clb_nodes: >
		load_balancer_id=${1:# REQUIRED}
		username=${2}
		weight=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		wait_timeout=${7:30}
		condition=${8:#enabled|disabled|draining}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		type=${13:#primary|secondary}
		port=${14}
		node_id=${15}
		wait=${16:#yes|no}

snippet docker_image
	docker_image: >
		name=${1:# REQUIRED}
		state=${2:#present|absent|build}
		tag=${3:latest}
		nocache=${4:false}
		path=${5}
		docker_url=${6:unix://var/run/docker.sock}
		timeout=${7:600}

snippet rax_dns
	rax_dns: >
		comment=${1}
		username=${2}
		name=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		ttl=${8:3600}
		credentials=${9}
		api_key=${10}
		email=${11}

snippet ec2_elb
	ec2_elb: >
		instance_id=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		security_token=${6}
		region=${7}
		wait_timeout=${8:0}
		ec2_url=${9}
		wait=${10:#yes|no}
		validate_certs=${11:#yes|no}
		enable_availability_zone=${12:#yes|no}
		ec2_elbs=${13}

snippet digital_ocean
	digital_ocean: >
		unique_name=${1:#yes|no}
		virtio=${2:#yes|no}
		region_id=${3}
		backups_enabled=${4:#yes|no}
		image_id=${5}
		wait_timeout=${6:300}
		client_id=${7}
		ssh_pub_key=${8}
		wait=${9:#yes|no}
		name=${10}
		size_id=${11}
		id=${12}
		state=${13:#present|active|absent|deleted}
		command=${14:#droplet|ssh}
		ssh_key_ids=${15}
		private_networking=${16:#yes|no}
		api_key=${17}

snippet keystone_user
	keystone_user: >
		endpoint=${1:http://127.0.0.1:35357/v2.0/}
		description=${2}
		login_user=${3:admin}
		token=${4}
		login_tenant_name=${5}
		state=${6:#present|absent}
		role=${7}
		user=${8}
		login_password=${9:yes}
		password=${10}
		email=${11}
		tenant=${12}

snippet rax_scaling_policy
	rax_scaling_policy: >
		name=${1:# REQUIRED}
		scaling_group=${2:# REQUIRED}
		policy_type=${3:#webhook|schedule}
		username=${4}
		is_percent=${5:false}
		env=${6}
		region=${7:dfw}
		verify_ssl=${8}
		cron=${9}
		desired_capacity=${10}
		state=${11:#present|absent}
		cooldown=${12}
		at=${13}
		credentials=${14}
		api_key=${15}
		change=${16}

snippet rax_meta
	rax_meta: >
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		meta=${8}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		id=${13}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}

snippet quantum_subnet
	quantum_subnet: >
		login_password=${1:true}
		login_username=${2:admin}
		cidr=${3:# REQUIRED}
		network_name=${4:# REQUIRED}
		name=${5:# REQUIRED}
		login_tenant_name=${6:true}
		region_name=${7}
		tenant_name=${8}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}
		allocation_pool_end=${10}
		enable_dhcp=${11:true}
		dns_nameservers=${12}
		state=${13:#present|absent}
		allocation_pool_start=${14}
		gateway_ip=${15}
		ip_version=${16:4}

snippet vsphere_guest
	vsphere_guest: >
		password=${1:# REQUIRED}
		guest=${2:# REQUIRED}
		user=${3:# REQUIRED}
		vcenter_hostname=${4:# REQUIRED}
		resource_pool=${5}
		vm_hw_version=${6}
		force=${7:#yes|no}
		vm_disk=${8}
		esxi=${9}
		vm_nic=${10}
		vm_hardware=${11}
		cluster=${12}
		state=${13:#present|powered_on|absent|powered_on|restarted|reconfigured}
		vmware_guest_facts=${14}
		vm_extra_config=${15}

snippet rax_facts
	rax_facts: >
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		env=${8}
		address=${9}
		credentials=${10}
		api_key=${11}
		id=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet rax_dns_record
	rax_dns_record: >
		name=${1:# REQUIRED}
		data=${2:# REQUIRED}
		type=${3:#A|AAAA|CNAME|MX|NS|SRV|TXT|PTR}
		comment=${4}
		username=${5}
		domain=${6}
		region=${7:dfw}
		verify_ssl=${8}
		server=${9}
		priority=${10}
		state=${11:#present|absent}
		env=${12}
		ttl=${13:3600}
		credentials=${14}
		api_key=${15}
		loadbalancer=${16}

snippet rax_network
	rax_network: >
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		label=${6}
		state=${7:#present|absent}
		env=${8}
		tenant_name=${9}
		credentials=${10}
		cidr=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_tag
	ec2_tag: >
		resource=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		state=${7:#present|absent|list}
		ec2_url=${8}
		validate_certs=${9:#yes|no}

snippet nova_keypair
	nova_keypair: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		public_key=${5}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2
	ec2: >
		image=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		ramdisk=${3}
		kernel=${4}
		volumes=${5}
		count_tag=${6}
		monitoring=${7}
		vpc_subnet_id=${8}
		user_data=${9}
		instance_ids=${10}
		wait_timeout=${11:300}
		profile=${12}
		private_ip=${13}
		assign_public_ip=${14}
		spot_price=${15}
		id=${16}
		source_dest_check=${17:true}
		wait=${18:#yes|no}
		count=${19:1}
		spot_wait_timeout=${20:600}
		aws_access_key=${21}
		group=${22}
		instance_profile_name=${23}
		zone=${24}
		exact_count=${25}
		ebs_optimized=${26:false}
		security_token=${27}
		state=${28:#present|absent|running|stopped}
		aws_secret_key=${29}
		ec2_url=${30}
		placement_group=${31}
		key_name=${32}
		instance_tags=${33}
		group_id=${34}
		validate_certs=${35:#yes|no}
		region=${36}

snippet quantum_network
	quantum_network: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		provider_network_type=${6}
		admin_state_up=${7:true}
		router_external=${8:false}
		tenant_name=${9}
		provider_physical_network=${10}
		state=${11:#present|absent}
		auth_url=${12:http://127.0.0.1:35357/v2.0/}
		shared=${13:false}
		provider_segmentation_id=${14}

snippet rax_cbs
	rax_cbs: >
		size=${1:100}
		volume_type=${2:#SATA|SSD}
		state=${3:#present|absent}
		name=${4:# REQUIRED}
		username=${5}
		api_key=${6}
		tenant_name=${7}
		description=${8}
		identity_type=${9:rackspace}
		tenant_id=${10}
		region=${11:dfw}
		auth_endpoint=${12:https://identity.api.rackspacecloud.com/v2.0/}
		verify_ssl=${13}
		wait_timeout=${14:300}
		meta=${15}
		env=${16}
		snapshot_id=${17}
		credentials=${18}
		wait=${19:#yes|no}

snippet rax_queue
	rax_queue: >
		username=${1}
		name=${2}
		region=${3:dfw}
		verify_ssl=${4}
		state=${5:#present|absent}
		env=${6}
		credentials=${7}
		api_key=${8}

snippet cloudformation
	cloudformation: >
		stack_name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		template=${3:# REQUIRED}
		aws_secret_key=${4}
		aws_access_key=${5}
		disable_rollback=${6:#true|false}
		tags=${7}
		region=${8}
		template_parameters=${9:{}}

snippet rax_identity
	rax_identity: >
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		tenant_name=${8}
		credentials=${9}
		api_key=${10}
		auth_endpoint=${11:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_eip
	ec2_eip: >
		aws_secret_key=${1}
		instance_id=${2}
		aws_access_key=${3}
		security_token=${4}
		reuse_existing_ip_allowed=${5:false}
		region=${6}
		public_ip=${7}
		state=${8:#present|absent}
		in_vpc=${9:false}
		profile=${10}
		ec2_url=${11}
		validate_certs=${12:#yes|no}
		wait_timeout=${13:300}

snippet gc_storage
	gc_storage: >
		gcs_secret_key=${1:# REQUIRED}
		bucket=${2:# REQUIRED}
		gcs_access_key=${3:# REQUIRED}
		mode=${4:#get|put|get_url|get_str|delete|create}
		src=${5}
		force=${6:true}
		permission=${7:private}
		dest=${8}
		object=${9}
		expiration=${10}

snippet rax_scaling_group
	rax_scaling_group: >
		max_entities=${1:# REQUIRED}
		name=${2:# REQUIRED}
		server_name=${3:# REQUIRED}
		image=${4:# REQUIRED}
		min_entities=${5:# REQUIRED}
		flavor=${6:# REQUIRED}
		files=${7}
		username=${8}
		api_key=${9}
		loadbalancers=${10}
		key_name=${11}
		disk_config=${12:#auto|manual}
		verify_ssl=${13}
		state=${14:#present|absent}
		cooldown=${15}
		meta=${16}
		env=${17}
		credentials=${18}
		region=${19:dfw}
		networks=${20:['public', 'private']}

snippet ec2_group
	ec2_group: >
		name=${1:# REQUIRED}
		description=${2:# REQUIRED}
		aws_secret_key=${3}
		rules_egress=${4}
		aws_access_key=${5}
		security_token=${6}
		rules=${7}
		region=${8}
		state=${9:present}
		profile=${10}
		ec2_url=${11}
		vpc_id=${12}
		validate_certs=${13:#yes|no}

snippet quantum_floating_ip
	quantum_floating_ip: >
		login_password=${1:yes}
		instance_name=${2:# REQUIRED}
		login_tenant_name=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}
		internal_network_name=${9}

snippet quantum_router_interface
	quantum_router_interface: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		subnet_name=${4:# REQUIRED}
		router_name=${5:# REQUIRED}
		region_name=${6}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet rax_files
	rax_files: >
		container=${1:# REQUIRED}
		username=${2}
		web_index=${3}
		region=${4:dfw}
		verify_ssl=${5}
		private=${6}
		state=${7:#present|absent}
		clear_meta=${8:#yes|no}
		meta=${9}
		env=${10}
		ttl=${11}
		web_error=${12}
		credentials=${13}
		api_key=${14}
		type=${15:#file|meta}
		public=${16}

snippet ec2_vol
	ec2_vol: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		zone=${5}
		instance=${6}
		region=${7}
		device_name=${8}
		volume_size=${9}
		state=${10:#absent|present}
		iops=${11:100}
		snapshot=${12}
		ec2_url=${13}
		security_token=${14}
		validate_certs=${15:#yes|no}
		id=${16}

snippet virt
	virt: >
		name=${1:# REQUIRED}
		xml=${2}
		state=${3:#running|shutdown|destroyed|paused}
		command=${4:#create|status|start|stop|pause|unpause|shutdown|undefine|destroy|get_xml|autostart|freemem|list_vms|info|nodeinfo|virttype|define}
		uri=${5}

snippet rax_keypair
	rax_keypair: >
		name=${1:# REQUIRED}
		username=${2}
		public_key=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		state=${8:#present|absent}
		env=${9}
		tenant_name=${10}
		credentials=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_elb_lb
	ec2_elb_lb: >
		name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		aws_secret_key=${3}
		subnets=${4}
		aws_access_key=${5}
		health_check=${6}
		security_token=${7}
		region=${8}
		purge_subnets=${9:false}
		ec2_url=${10}
		listeners=${11}
		security_group_ids=${12}
		zones=${13}
		purge_listeners=${14:true}
		profile=${15}
		scheme=${16:internet-facing}
		validate_certs=${17:#yes|no}
		purge_zones=${18:false}

snippet nova_compute
	nova_compute: >
		image_id=${1:# REQUIRED}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		login_tenant_name=${5:yes}
		region_name=${6}
		key_name=${7}
		user_data=${8}
		meta=${9}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		wait_for=${11:180}
		security_groups=${12}
		wait=${13:yes}
		nics=${14}
		state=${15:#present|absent}
		flavor_id=${16:1}

snippet linode
	linode: >
		datacenter=${1}
		swap=${2:512}
		api_key=${3}
		name=${4}
		payment_term=${5:#1|12|24}
		linode_id=${6}
		state=${7:#present|active|started|absent|deleted|stopped|restarted}
		wait_timeout=${8:300}
		plan=${9}
		distribution=${10}
		password=${11}
		ssh_pub_key=${12}
		wait=${13:#yes|no}

snippet ec2_facts
	ec2_facts: validate_certs=${1:#yes|no}

snippet rax_cbs_attachments
	rax_cbs_attachments: >
		volume=${1:# REQUIRED}
		device=${2:# REQUIRED}
		server=${3:# REQUIRED}
		state=${4:#present|absent}
		username=${5}
		tenant_name=${6}
		verify_ssl=${7}
		wait_timeout=${8:300}
		credentials=${9}
		wait=${10:#yes|no}
		identity_type=${11:rackspace}
		tenant_id=${12}
		region=${13:dfw}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}
		env=${15}
		api_key=${16}

snippet docker
	docker: >
		image=${1:# REQUIRED}
		username=${2}
		publish_all_ports=${3:false}
		tty=${4:false}
		env=${5}
		links=${6}
		memory_limit=${7:256mb}
		lxc_conf=${8}
		stdin_open=${9:false}
		volumes=${10}
		password=${11}
		count=${12:1}
		detach=${13:true}
		name=${14}
		hostname=${15}
		docker_url=${16:unix://var/run/docker.sock}
		ports=${17}
		state=${18:#present|running|stopped|absent|killed|restarted}
		command=${19}
		dns=${20}
		volumes_from=${21}
		expose=${22}
		privileged=${23:false}

snippet s3
	s3: >
		bucket=${1:# REQUIRED}
		mode=${2:# REQUIRED}
		aws_secret_key=${3}
		src=${4}
		aws_access_key=${5}
		expiration=${6:600}
		dest=${7}
		object=${8}
		s3_url=${9}
		overwrite=${10:true}
		metadata=${11}

snippet digital_ocean_domain
	digital_ocean_domain: >
		state=${1:#present|active|absent|deleted}
		name=${2}
		client_id=${3}
		ip=${4}
		api_key=${5}
		id=${6}

snippet ec2_snapshot
	ec2_snapshot: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		description=${4}
		security_token=${5}
		snapshot_tags=${6}
		region=${7}
		ec2_url=${8}
		device_name=${9}
		instance_id=${10}
		volume_id=${11}
		validate_certs=${12:#yes|no}

snippet rds_param_group
	rds_param_group: >
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		engine=${4:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		aws_secret_key=${5}
		aws_access_key=${6}
		immediate=${7}
		params=${8:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		description=${9}

snippet gce_pd
	gce_pd: >
		name=${1:# REQUIRED}
		size_gb=${2:10}
		zone=${3:us-central1-b}
		service_account_email=${4}
		image=${5}
		pem_file=${6}
		instance_name=${7}
		state=${8:#active|present|absent|deleted}
		snapshot=${9}
		detach_only=${10:#yes|no}
		project_id=${11}
		mode=${12:#READ_WRITE|READ_ONLY}

snippet gce
	gce: >
		zone=${1:us-central1-a}
		name=${2}
		tags=${3}
		service_account_email=${4}
		image=${5:debian-7}
		disks=${6}
		metadata=${7}
		persistent_boot_disk=${8:false}
		pem_file=${9}
		state=${10:#active|present|absent|deleted}
		machine_type=${11:n1-standard-1}
		project_id=${12}
		instance_names=${13}
		network=${14:default}

snippet rax
	rax: >
		files=${1}
		username=${2}
		tenant_name=${3}
		auto_increment=${4:#yes|no}
		image=${5}
		count_offset=${6:1}
		instance_ids=${7}
		user_data=${8}
		verify_ssl=${9}
		wait_timeout=${10:300}
		tenant_id=${11}
		credentials=${12}
		region=${13:dfw}
		flavor=${14}
		networks=${15:['public', 'private']}
		wait=${16:#yes|no}
		count=${17:1}
		group=${18}
		name=${19}
		identity_type=${20:rackspace}
		extra_client_args=${21}
		exact_count=${22:#yes|no}
		disk_config=${23:#auto|manual}
		auth_endpoint=${24:https://identity.api.rackspacecloud.com/v2.0/}
		state=${25:#present|absent}
		meta=${26}
		env=${27}
		key_name=${28}
		api_key=${29}
		extra_create_args=${30}
		config_drive=${31:#yes|no}

snippet ec2_vpc
	ec2_vpc: >
		resource_tags=${1:# REQUIRED}
		cidr_block=${2:# REQUIRED}
		state=${3:present}
		subnets=${4}
		internet_gateway=${5:#yes|no}
		wait_timeout=${6:300}
		dns_hostnames=${7:#yes|no}
		wait=${8:#yes|no}
		aws_secret_key=${9}
		aws_access_key=${10}
		route_tables=${11}
		dns_support=${12:#yes|no}
		region=${13}
		instance_tenancy=${14:#default|dedicated}
		vpc_id=${15}
		validate_certs=${16:#yes|no}

snippet glance_image
	glance_image: >
		login_password=${1:yes}
		login_username=${2:admin}
		name=${3:# REQUIRED}
		login_tenant_name=${4:yes}
		region_name=${5}
		container_format=${6:bare}
		min_ram=${7}
		owner=${8}
		endpoint_type=${9:#publicURL|internalURL}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		file=${11}
		min_disk=${12}
		is_public=${13:yes}
		disk_format=${14:qcow2}
		copy_from=${15}
		state=${16:#present|absent}
		timeout=${17:180}

snippet rax_clb
	rax_clb: >
		username=${1}
		protocol=${2:#DNS_TCP|DNS_UDP|FTP|HTTP|HTTPS|IMAPS|IMAPv4|LDAP|LDAPS|MYSQL|POP3|POP3S|SMTP|TCP|TCP_CLIENT_FIRST|UDP|UDP_STREAM|SFTP}
		name=${3}
		algorithm=${4:#RANDOM|LEAST_CONNECTIONS|ROUND_ROBIN|WEIGHTED_LEAST_CONNECTIONS|WEIGHTED_ROUND_ROBIN}
		env=${5}
		region=${6:dfw}
		verify_ssl=${7}
		vip_id=${8}
		state=${9:#present|absent}
		wait_timeout=${10:300}
		meta=${11}
		timeout=${12:30}
		credentials=${13}
		api_key=${14}
		type=${15:#PUBLIC|SERVICENET}
		port=${16:80}
		wait=${17:#yes|no}

snippet rds
	rds: >
		command=${1:#create|replicate|delete|facts|modify|promote|snapshot|restore}
		region=${2:# REQUIRED}
		instance_name=${3:# REQUIRED}
		db_engine=${4:#MySQL|oracle-se1|oracle-se|oracle-ee|sqlserver-ee|sqlserver-se|sqlserver-ex|sqlserver-web|postgres}
		iops=${5}
		backup_window=${6}
		backup_retention=${7}
		port=${8}
		security_groups=${9}
		size=${10}
		aws_secret_key=${11}
		subnet=${12}
		vpc_security_groups=${13}
		upgrade=${14:#yes|no}
		zone=${15}
		source_instance=${16}
		parameter_group=${17}
		multi_zone=${18:#yes|no}
		new_instance_name=${19}
		username=${20}
		db_name=${21}
		license_model=${22:#license-included|bring-your-own-license|general-public-license}
		password=${23}
		apply_immediately=${24:#yes|no}
		wait=${25:#yes|no}
		aws_access_key=${26}
		option_group=${27}
		engine_version=${28}
		instance_type=${29}
		wait_timeout=${30:300}
		snapshot=${31}
		maint_window=${32}

snippet route53
	route53: >
		zone=${1:# REQUIRED}
		record=${2:# REQUIRED}
		command=${3:#get|create|delete}
		type=${4:#A|CNAME|MX|AAAA|TXT|PTR|SRV|SPF|NS}
		aws_secret_key=${5}
		aws_access_key=${6}
		retry_interval=${7:500}
		value=${8}
		ttl=${9:3600 (one hour)}
		overwrite=${10}

snippet ec2_asg
	ec2_asg: >
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		availability_zones=${6}
		security_token=${7}
		tags=${8}
		region=${9}
		min_size=${10}
		desired_capacity=${11}
		vpc_zone_identifier=${12}
		launch_config_name=${13}
		health_check_period=${14:500 seconds}
		ec2_url=${15}
		load_balancers=${16}
		validate_certs=${17:#yes|no}
		max_size=${18}
		health_check_type=${19:#EC2|ELB}

snippet ec2_scaling_policy
	ec2_scaling_policy: >
		name=${1:# REQUIRED}
		asg_name=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		security_token=${7}
		adjustment_type=${8:#ChangeInCapacity|ExactCapacity|PercentChangeInCapacity}
		min_adjustment_step=${9}
		scaling_adjustment=${10}
		cooldown=${11}
		ec2_url=${12}
		validate_certs=${13:#yes|no}

snippet riak
	riak: >
		target_node=${1:riak@127.0.0.1}
		config_dir=${2:/etc/riak}
		wait_for_service=${3:#kv}
		http_conn=${4:127.0.0.1:8098}
		wait_for_ring=${5}
		wait_for_handoffs=${6}
		command=${7:#ping|kv_test|join|plan|commit}
		validate_certs=${8:#yes|no}

snippet mysql_user
	mysql_user: >
		name=${1:# REQUIRED}
		login_port=${2:3306}
		login_user=${3}
		login_host=${4:localhost}
		append_privs=${5:#yes|no}
		host=${6:localhost}
		login_unix_socket=${7}
		state=${8:#present|absent}
		login_password=${9}
		check_implicit_admin=${10:false}
		password=${11}
		priv=${12}

snippet mysql_replication
	mysql_replication: >
		master_ssl_cert=${1}
		master_password=${2}
		login_user=${3}
		login_host=${4}
		login_password=${5}
		master_host=${6}
		master_ssl_ca=${7}
		login_unix_socket=${8}
		master_connect_retry=${9}
		master_user=${10}
		master_port=${11}
		master_log_file=${12}
		master_ssl_cipher=${13}
		relay_log_file=${14}
		master_ssl=${15}
		master_ssl_key=${16}
		master_ssl_capath=${17}
		mode=${18:#getslave|getmaster|changemaster|stopslave|startslave}
		master_log_pos=${19}
		relay_log_pos=${20}

snippet postgresql_user
	postgresql_user: >
		name=${1:# REQUIRED}
		login_password=${2}
		login_user=${3:postgres}
		login_host=${4:localhost}
		expires=${5}
		db=${6}
		port=${7:5432}
		state=${8:#present|absent}
		encrypted=${9:false}
		password=${10}
		role_attr_flags=${11:#[NO]SUPERUSER|[NO]CREATEROLE|[NO]CREATEUSER|[NO]CREATEDB|[NO]INHERIT|[NO]LOGIN|[NO]REPLICATION}
		fail_on_user=${12:#yes|no}
		priv=${13}

snippet postgresql_privs
	postgresql_privs: >
		roles=${1:# REQUIRED}
		database=${2:# REQUIRED}
		objs=${3}
		privs=${4}
		state=${5:#present|absent}
		host=${6}
		login=${7:postgres}
		password=${8}
		type=${9:#table|sequence|function|database|schema|language|tablespace|group}
		port=${10:5432}
		grant_option=${11:#yes|no}
		schema=${12}

snippet redis
	redis: >
		command=${1:#slave|flush|config}
		login_port=${2:6379}
		name=${3}
		flush_mode=${4:#all|db}
		master_host=${5}
		login_host=${6:localhost}
		master_port=${7}
		db=${8}
		value=${9}
		login_password=${10}
		slave_mode=${11:#master|slave}

snippet postgresql_db
	postgresql_db: >
		name=${1:# REQUIRED}
		encoding=${2}
		login_user=${3}
		lc_collate=${4}
		lc_ctype=${5}
		port=${6:5432}
		state=${7:#present|absent}
		template=${8}
		login_password=${9}
		owner=${10}
		login_host=${11:localhost}

snippet mysql_variables
	mysql_variables: >
		variable=${1:# REQUIRED}
		login_unix_socket=${2}
		login_password=${3}
		login_user=${4}
		login_host=${5}
		value=${6}

snippet mongodb_user
	mongodb_user: >
		database=${1:# REQUIRED}
		user=${2:# REQUIRED}
		login_port=${3:27017}
		roles=${4:readwrite}
		login_user=${5}
		login_host=${6:localhost}
		state=${7:#present|absent}
		login_password=${8}
		password=${9}
		replica_set=${10}

snippet mysql_db
	mysql_db: >
		name=${1:# REQUIRED}
		login_port=${2:3306}
		encoding=${3}
		login_user=${4}
		login_host=${5:localhost}
		login_unix_socket=${6}
		state=${7:#present|absent|dump|import}
		login_password=${8}
		collation=${9}
		target=${10}

snippet lineinfile
	lineinfile: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		src=${3}
		force=${4:#yes|no}
		insertbefore=${5:#BOF|*regex*}
		selevel=${6:s0}
		create=${7:#yes|no}
		seuser=${8}
		recurse=${9:#yes|no}
		serole=${10}
		backrefs=${11:#yes|no}
		owner=${12}
		state=${13:#file|link|directory|hard|touch|absent}
		mode=${14}
		insertafter=${15:#EOF|*regex*}
		regexp=${16}
		line=${17}
		backup=${18:#yes|no}
		validate=${19}
		group=${20}
		setype=${21}

snippet get_url
	get_url: >
		url=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		path=${3:[]}
		url_password=${4}
		force=${5:#yes|no}
		use_proxy=${6:#yes|no}
		src=${7}
		selevel=${8:s0}
		seuser=${9}
		recurse=${10:#yes|no}
		setype=${11}
		sha256sum=${12}
		serole=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		mode=${15}
		url_username=${16}
		owner=${17}
		group=${18}
		validate_certs=${19:#yes|no}

snippet ini_file
	ini_file: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		section=${3:# REQUIRED}
		force=${4:#yes|no}
		option=${5}
		state=${6:#file|link|directory|hard|touch|absent}
		selevel=${7:s0}
		owner=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		setype=${13}
		value=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet uri
	uri: >
		path=${1:[]}
		url=${2:# REQUIRED}
		force=${3:#yes|no}
		follow_redirects=${4:#all|safe|none}
		owner=${5}
		HEADER_=${6}
		group=${7}
		serole=${8}
		setype=${9}
		status_code=${10:200}
		return_content=${11:#yes|no}
		method=${12:#GET|POST|PUT|HEAD|DELETE|OPTIONS|PATCH}
		body=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		dest=${15}
		selevel=${16:s0}
		force_basic_auth=${17:#yes|no}
		removes=${18}
		user=${19}
		password=${20}
		src=${21}
		seuser=${22}
		recurse=${23:#yes|no}
		creates=${24}
		mode=${25}
		timeout=${26:30}

snippet replace
	replace: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		regexp=${3:# REQUIRED}
		force=${4:#yes|no}
		state=${5:#file|link|directory|hard|touch|absent}
		selevel=${6:s0}
		replace=${7}
		owner=${8}
		validate=${9}
		src=${10}
		group=${11}
		seuser=${12}
		recurse=${13:#yes|no}
		setype=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet assemble
	assemble: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		force=${3:#yes|no}
		remote_src=${4:#True|False}
		selevel=${5:s0}
		state=${6:#file|link|directory|hard|touch|absent}
		owner=${7}
		regexp=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		serole=${13}
		delimiter=${14}
		mode=${15}
		backup=${16:#yes|no}
		setype=${17}

# Copy this file to .env and fill in your values

APP_MONGO_DB_URI=mongodb+srv://username:password@cluster.mongodb.net/promotion_builder
APP_REDIS_URL=redis://default:password@redis.ec2.cloud.redislabs.com:17356
APP_SECURITY_USERNAME=username
APP_SECURITY_PASSWORD={noop}password
APP_JWT_SECRET=secret
APP_JWT_TOKEN_DURATION_SECONDS=86400
APP_MASTER_API_KEY=31752dbc600386b6cab71c76c8e6229f
APP_MAIN_API_HOST=http://localhost:8080
APP_AWS_ACCESS_KEY=AKIAIOSFODNN7EXAMPLE
APP_AWS_SECRET_KEY=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
APP_SQS_REGION=ap-northeast-1
APP_SQS_EVENT_QUEUE_NAME=promotion-event-host_event
APP_SQS_USER_QUEUE_NAME=promotion-event-host_user
Cookbook TESTING doc
====================

Bundler
-------
A ruby environment with Bundler installed is a prerequisite for using
the testing harness shipped with this cookbook. At the time of this
writing, it works with Ruby 1.9.3 and Bundler 1.6.2. All programs
involved, with the exception of Vagrant, can be installed by cd'ing
into the parent directory of this cookbook and running "bundle install"

Rakefile
--------
The Rakefile ships with a number of tasks, each of which can be ran
individually, or in groups. Typing "rake" by itself will perform style
checks with Foodcritic and integration tests with Test Kitchen using
the Vagrant driver by default.

```
$ rake -T
rake integration:cloud    # Run Test Kitchen cloud plugins
rake integration:vagrant  # Run Test Kitchen with Vagrant
rake style                # Run all style checks
rake style:chef           # Lint Chef cookbooks
```

Style Testing
-------------
Chef style tests can be performed with Foodcritic by issuing either
```
bundle exec foodcritic
```
or
```
rake style:chef
```

Integration Testing
-------------------
Integration testing is performed by Test Kitchen. After a
successful converge, tests are uploaded and ran out of band of
Chef. Tests should be designed to ensure that a recipe has
accomplished its goal.

Integration Testing using Vagrant
---------------------------------
Integration tests using Vagrant can be performed with either
```
bundle exec kitchen test
```
or
```
rake integration:vagrant
```

Integration Testing using Cloud providers
-----------------------------------------
Integration tests can be performed on cloud providers using Test Kitchen plugins. This cookbook ships a .kitchen.cloud.yml that references environmental variables present in the shell that kitchen test is ran from. These usually contain authentication tokens for driving IaaS APIs, as well as the paths to ssh private keys needed for Test Kitchen log into them after they've been created.

Examples of environment variables being set in ~/.bash_profile:

```
# digitalocean (APIv1)
export DIGITALOCEAN_API_KEY='your_bits_here'
export DIGITALOCEAN_CLIENT_ID='your_bits_here'
export DIGITALOCEAN_SSH_KEY_IDS='your_bits_here'    #CSV String of IDs
export DIGITALOCEAN_SSH_KEY_PATH='your_bits_here'

#ec2
export AWS_ACCESS_KEY='your_bits_here'
export AWS_SECRET_KEY='your_bits_here'
export AWS_SECURITY_GROUP='your_bits_here'
export AWS_SSH_KEY_ID='your_bits_here'
export AWS_SSH_KEY_PATH='your_bits_here'
```

**Note:** There is currently a bug in kitchen-digitalocean (0.8.2) which forces DIGITALOCEAN_SSH_KEY_IDS to require at least two entires. It is prefectly fine however to use the same key id twice.

Integration tests using cloud drivers can be performed with either
```
export KITCHEN_YAML=.kitchen.cloud.yml
bundle exec kitchen test
```
or
```
rake integration:cloud
```
# TCGA/ICGC PanCancer - Computational Node/Cluster from Scratch with Bindle

This is our SOP for how to launch clusters/nodes using Bindle
specifically for running alignment and variant calling workflows for the TCGA/ICGC PanCancer project.  In addition to
providing production cluster environments for analyzing samples on the clouds
used by the PanCancer project, the Bindle process can also be used to
create workflow development environments.  Unlike AMI or OVA VM snapshots
the Bindle process builds an environment (whether cluster or single node)
up from a base Ubuntu 12.04 box, installing and configuring software as
it goes.  Any cloud-specific considerations are documented at the end of this guide.

Finally, single-node instances can be imaged and relaunched using the process detailed in the [following guide](image_instance.md).  The reason you would do this rather than just build nodes from scratch each time you need one is speed, a VM from a snapshot takes a couple minutes to launch whereas a VM built from the base image up to a PanCancer worker node takes about 30.  We still need to maintain the latter process, though, since we need to be able to create nodes from scratch from time to time.

## Use Cases

There are really three use cases for this technology by the PanCancer project.
First, to create a production environment for running analytical workflows for
PanCancer in environments that do not support image creation.
These are employed by "cloud shepherds" in "Phase II" to
analyze donors with standardized alignment and variant calling workflows.
This environment could also be used by "Phase III" researchers that need a virtual
cluster running Hadoop or SGE for their research.
The second use case is to create a workflow development environment for making and testing
new workflows for the project, especially if scaled-up testing across
a virtual cluster is required. Regardless, the directions for creating a node or
cluster with Bindle are the same.

## Build a PanCancer Workflow Running Environment

### Overview of Steps

* decide on cloud environment and request an account, when you sign up you should get the cloud credentials you need (more detail below). Pancancer has 6 cloud environments, each with their own credentials, endpoints, settings, etc.
* launch an Ubuntu 12.04 VM in that cloud you will use this host as a "launcher host" which will launch other VMs that run workflows and/or are snapshotted for later use 
* ssh to this launcher host and use the instructions detailed below to setup the Bindle provision tool used to launch other VMs
* customize the Bindle template with your cloud settings (ex: ~/.bindle/aws.cfg), this was created for you by the above mentioned setup process
* launch your SeqWare worker node with PanCancer workflows using Bindle's launch\_cluster.pl tool
* you can do one of four things with this worker node:
   * 1) ssh into your SeqWare work node, develop, then launch SeqWare workflow(s) and monitor their results
    * _or_
    * 2) use the environment for developing, building, or using your own tools (e.g. "Phase III" activities), the following environments are available for your use:
       * GridEngine
       * SeqWare configured with the Oozie-SGE workflow engine
       * Hadoop HDFS 
       * Hadoop Oozie
   * _or_
   * 3) snapshot the VM to make an image which can then be launched again (potentially over and over again) and do the two above
   * _or_
   * 4) you can add the IP address to a workflow decider and have that decider schedule workflows here

Typically you would do 1 & 2 for development and 3 then 4 for production

### Detailed Example - Amazon Web Services Single Node/Cluster of Nodes with the HelloWorld Workflow

Here I will show you how to create a single compute node running on AWS and
capable or executing the HelloWorld workflow to ensure your environment is
working.  This setup process will also install other PanCancer workflows along the way.  I chose AWS for its ease of access however please keep in mind
that we run in 6 academic clouds which do have their specific settings, see the section below. The mechanism for other clouds is
identical to the example below, however, so the example shown below should be
extremely helpful in accessing PanCancer clouds.

**Again, Amazon is documented here as an example, if you are following this process in an Academic cloud there will be details that differ.  You are not required to use Amazon to use our infrastructure, it is simply a working example of setting up our architecture in a cloud environment that we know works.**

#### Step 0 - Get an Account

First, sign up for an account on the AWS website, see http://aws.amazon.com for
directions.  Brian O'Connor manages the account for the OICR group so see him if you are in that group.  For users following this guide outside of the OICR group, you will need to establish your own AWS account.  Along the way you will create an SSH pem key for yourself and you will get your Amazon key and secret key.  Keep all three secure.

You also need a GNOS pem key for reading/writing data to GNOS.  See the [PanCancer Research Guide](https://wiki.oicr.on.ca/display/PANCANCER/PCAWG+Researcher%27s+Guide) which will walk you through the process of getting both an ICGC and TCGA GNOS pem key.  The latter is only used on BioNimbus when working with TCGA data.

#### Step 1 - Create a Launcher Host

Next, you can create a "launcher" host. This is your gateway to the system and
allows you to launch individual computational nodes (or clusters of nodes) that
actually do the processing of data.  It also is the location to run the
"decider" that will schedule the BWA (and other) workflows running on your many nodes in
this cloud.  This latter topic will be discussed in another guide focused on
workflow launching and automation.
 
The launcher host also improves the isolation of your computational
infrastructure.  It should be the only host accessible via SSH, should use SSH
keys rather than passwords, use a non-standard SSH port, and, ideally, include
Failtoban or another intrusion deterrent.  For AWS, please see the extensive
documentation on using security groups to isolate instances behind firewalls
and setup firewall rules at http://aws.amazon.com.

For our purposes we use an Ubuntu 12.04 AMI provided by Amazon.  See the
documentation on http://aws.amazon.com for information about programmatic,
command line, and web GUI tools for starting this launcher host.  For the
purposes of this tutorial we assume you have successfully started the launcher
host using the web GUI at http://aws.amazon.com.  

Next, we recommend you use an "m3.xlarge" instance type as this is inexpensive
 to keep running constantly (about $200/month).  You can use a smaller instance type if you want to save money but "m3.xlarge" is what we've standardized on.

We also assume that you have setup your firewall (security group) and have
produced a .pem SSH key file for use to login to this host.  In my case my key
file is called "brian-oicr-3.pem" and, once launched, I can login to my
launcher host over SSH using something similar to the following:

    ssh -i brian-oicr-3.pem ubuntu@ec2-54-221-150-76.compute-1.amazonaws.com

Up to this point the activities we have described are not at all specific to
the PanCancer project.  If you have any issues following these steps please see
the extensive tutorials online for launching a EC2 host on AWS.  Also, please
be aware that Amazon charges by the hour, rounded up.  Don't leave instances running if they aren't used.

#### Step 2 - Install Bindle, Vagrant, and Other Tools on the Launcher

The next step is to configure Vagrant (cloud-agnostic VM launcher),
Bindle (our tool for wrapping Vagrant and setting up a computational
environment/cluster), and various other dependencies to get these to work.  Log
onto your launcher now and download the current release of the architecture-setup project:

    wget https://github.com/ICGC-TCGA-PanCancer/architecture-setup/archive/1.0.5.tar.gz
    tar zxf 1.0.5.tar.gz
    cd architecture-setup-1.0.5

Now we will follow the documents from [PanCancer Architecture Setup](https://github.com/ICGC-TCGA-PanCancer/architecture-setup) which will install Bindle with all associated code for the PanCancer profiles. These docs are authoritative at that site, the example below is just to give you an idea. Make sure you check the Architecture Setup link in case anything has changed.  In particular, pay attention to the GNOS key instructions since each workflow needs this to operate and, for security reasons, it needs to be provided by you the launcher.  Also, you need to provide your AWS pem key as well, put it in the same place for simplicity. I recommend you keep all your keys in ~/.ssh so you know where they are and it's easier to manage:

    # fill in your AWS .pem key on the launcher host, the same you used to login
    $ vim ~/.ssh/brian-oicr-3.pem
    $ chmod 600 ~/.ssh/brian-oicr-3.pem
    # now fill in your ICGC or TCGA .pem key on the launcher host
    $ vim ~/.ssh/gnos.pem
    $ chmod 600 ~/.ssh/gnos.pem

Now setup the environment:

    $ bash setup.sh
    $ ansible-playbook -i inventory site.yml

By default the inventory file points to the local host which will work perfectly for us here.

At this point you should have a launcher with Bindle and associated
tools installed. This is now the machine from which you can create one or more
SeqWare nodes/clusters for use with various workflows, GridEngine, or Hadoop.


#### Step 3 - Configuration

Now that you have Bindle and dependencies installed the next step is
to launch computational nodes or clusters that will run workflows via SeqWare,
launch cluster jobs via GridEngine, or perform MapReduce jobs.  In this step we
will launch a standalone node and in the next command block I will show you how to
launch a whole cluster of nodes that are suitable for larger-scale analysis. Although keep in mind for Phase II of PanCancer we're shifting focus to single nodes and not clusters to improve robustness and isolation.

Assuming you are still logged into your launcher node above, you will do the
following to setup a computational node.  The steps below assume you are
working in the Bindle directory:

    $ cd ~/architecture2/Bindle
    # run the Bindle launcher without a valid cfg file in order to copy over a template
    $ perl bin/launch_cluster.pl --config aws --custom-params singlenode1
    # modify the .cfg file to include your settings, for AWS you need to make sure you fill in "aws.cfg"
    # For more help on filling the .cfg file, please refer to the section below
    $ vim ~/.bindle/aws.cfg

Make sure you have copied your keys to this machine (your GNOS and AWS .pem file, the latter of which is used in the aws.cfg file). I suggest
you use IAM to create limited scope credentials.  See the Amazon site for more
info.

Alternatively, you may want to launch a compute cluster instead of a single
node.  You can customize the number of worker nodes by increasing the number in the Bindle cfg file.  Again, our focus now is single nodes so clusters are undergoing less validation work currently.

One thing you must keep in mind before filling in the config files is not to delete any of the default
parameters you are not going to be needing. Simply, leave them blank if that is the case. 

##### Platform Specific Information

This section of the config file contains all the information that is required to set up the cloud platform.
We will need to fill out all the information in config/aws.cfg. For OpenStack, it is openstack.cfg and for vCloud, it is vcloud.cfg

Let us go through the parameters that might confuse you when you are filling the config file. I will not be going 
through the most obvious parameters (ie. key, secret_key, etc):

    [defaults]
    platform = aws
    aws_key= 
    aws_secret_key= 
    aws_instance_type = 'm1.xlarge' 
    aws_region = 'eu-west-1'
    aws_zone = nil 
    aws_image = 
    aws_ssh_username = ubuntu
    # the name of the key e.g. brian-oicr-3, drop the .pem
    aws_ssh_key_name = 
    aws_ssh_pem_file = ''
   
    # For any single node cluster or a cluster in bionimbus environment, please leave this empty(Ex. '')
    # Else for a multi-node cluster, please specify the devices you want to use to setup gluster
    # To find out the list of devices you can use, execute df | grep /dev/ on an instance currently running on the same platform.
    # (Ex. '--whitelist b,f' if you want to use sdb/xvdb and sdf/xvdf). 
    # Note, if your env. doesn't have devices, use the gluster_directory_path param
    gluster_device_whitelist='--whitelist b'
    # For any single node cluster or a cluster in bionimbus environment, please leave this empty(Ex. '')
    # Else for a multi-node cluster, please specify the directory if you are not using devices to set up gluster
    # (Ex. '--directorypath /mnt/volumes/gluster1')
    gluster_directory_path=''
    
    box = dummy
    box_url = 'https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box'
    host_inventory_file_path=ansible_host_inventory.ini
    ansible_playbook = ../pancancer-bag/pancancer.yml
    seqware_provider=artifactory
    seqware_version='1.1.0-alpha.5'
    
    install_bwa=true
    
    number_of_clusters = 1
    number_of_single_node_clusters = 1
    bwa_workflow_version = 2.6.2
    
For the pancancer project, you will also need to specify which workflows you wish to have installed. Currently, the variables required are:

    install_workflow = True
    workflow_name = BWA,Sanger
    workflows=Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.5,Workflow_Bundle_SangerPancancerCgpCnIndelSnvStr_1.0.1_SeqWare_1.1.0-alpha.5,Workflow_Bundle_BWA_2.6.3_SeqWare_1.1.0-alpha.5


Note that all variables above are also passed along into Ansible. The other platform specific parameters are self explanatory. In the config file, there is a "fillmein" value which indicates that you
defintely have to fill those in to have bindle working properly. The others are default values that you may use unless otherwise stated.

For single nodes, make sure to set "gluster_device_whitelist = '' ".

For AWS, you will probably want to expand your root partition if you are spinning up a node with the intention of imaging it. For example, you would set the variable

    aws_ebs_vols = "aws.block_device_mapping = [{ 'DeviceName' => '/dev/sda1', 'Ebs.VolumeSize' => 1000 },{'DeviceName' => '/dev/sdb', 'NoDevice' => '' }]"

##### Note About Instance Type

In AWS the instance type controls the resource available on the VM (cores, RAM, disk space, etc).  Similarly, flavors are used in OpenStack. You will want to match the instance type you choose with the requirements of the PanCancer workflow being deployed there.  This information should be documented for your in the README that comes with each workflow, most workflows will explicitly mention the instance type to use.  Another great resource is  which helps you to see what types are available and their price.

Keep this in mind especially if you intend on snpshotting the VM since you can't just change the instance type later on boot of the snapshot.  If you provision on an 8 core instance type, then snapshot, then launch on a 16 core instance type the additional cores will not be recognized by SGE properly.  You should instead launch and snapshot the same instance type you intend to use going forward. 

##### Cluster Specific Information

This information exists in small blocks named cluster1, singlenode1, etc. These blocks contain essential information such as number of nodes,
target\_directory, the number of nodes to create, the Ansible categories for hosts, etc.
    
Please note that you can create a new cluster by copy-pasting the existing cluster1
block and modifying the configs for it or you can simply modify cluster1 configs and use that.
Feel free to change the number of nodes (min 1, max recommended 11). Please note that 
if the number of nodes is 1, it means that there will be 1 master and 0 worker nodes. 
An example cluster block will look something like this:

    # Clusters are named cluster1, cluster2, etc.
    # When launching a cluster using launch_cluster.pl
    # use the section name(cluster1 in this case) as a parameter to --launch-cluster
    [cluster1]
   
    # this includes one master and three worker nodes
    number_of_nodes=4
   
    # this specifies the output directory where everything will get installed on the launcher
    target_directory = target-aws-1
   
   
 
To use a specific cluster block, you need to use the section name of that block as a parameter to --custom-params when you
are running the launch_cluster perl script. More on this in the next step.


#### Step 4 - Launch a SeqWare Node/Cluster

Now that you have customized the settings in .cfg file, the next step is to launch a computational node. Note, each cluster gets its own target directory which you can specify the name of in .cfg file when you make a cluster block. Within the target dir you will find a log for each node (simply master.log for a single-node launch) and a directory for each node that is used by the vagrant command line tool (the "master" directory for a single-node launch). The latter is important for controlling your node/cluster once launched. 

Remember to delete the previous working directory

    $ cd ~/architecture2/Bindle
    $ rm -Rf target*
    # now launch the compute node. For --cluster, you specify the name of the cluster block you want to launch from the .cfg file
    $ perl bin/launch_cluster.pl --config aws --custom-params singlenode1

You can follow the progress of this cluster launch in another terminal with.
Use multiple terminals to watch logs for multiple-node clusters if you desire:

    # watch the log
    $ tail -f target-aws-5/master.log

Once this process finishes, you should see no error messages from
"bin/launch_cluster.pl". If so, you are ready to use your cluster/node.

If you want to launch multiple clusters, make sure to specify different target directory names (ex. target-os-1, target-os-2, etc.) in the config file!

#### Step 5 - Log In To Node/Cluster

Vagrant provides a simple way to log into a launched node/cluster.  Typically you will only want/need to login to the master node.  For example:

    # log into the master node
    $ cd target-aws-5/master
    $ vagrant ssh

This will log you into the master node.  You can change user to the seqware
user which will be used for subsequent steps or root if you need to do some
administration of the box.

    # switch user to seqware
    $ sudo su - seqware
    # or switch user to root (not generally needed!)
    $ sudo su -

#### Step 6 - Verify Node/Cluster with HelloWorld

Now that you have a node or a cluster the next step is to launch a sample
HelloWorld SeqWare workflow to ensure all the infrastructure on the box is
functioning correctly.  Depending on the template you used this may or may not
be already installed under the seqware user account. If not, you can download a
copy of the workflow and install it yourself following our guides on
http://seqware.io (see
https://s3.amazonaws.com/oicr.workflow.bundles/released-bundles/Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.4.zip).
The commands below assume the workflow is installed into
provisioned-bundles/Workflow\_Bundle\_HelloWorld\_1.0-SNAPSHOT\_SeqWare_1.1.0-alpha.4.

    # assumes you have logged into your master node and switched to the seqware user
    $ ls provisioned-bundles/
    Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.4
    # now run the workflow
    $ seqware bundle launch --dir provisioned-bundles/Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.4

This command should finish without errors.  If there are problems please report
the errors to the SeqWare user group, see http://seqware.io/community/ for
information about posting to our mailing list.

#### Step 7 - Terminate Node/Cluster

At this point you have successfully ran a workflow.  You can use this node or
cluster to run real workflows or just as a general GridEngine or Hadoop
environment. 

You can also [image the node](image_instance.md) for future use. 

Those topics are beyond the scope of this document but are
covered in other SOPs.  When you finish with a node or cluster you can
terminate it or, depending on the environment, you can suspend it for use
later.  Keep in mind suspend works for single nodes but clusters of nodes
cannot be suspended and then booted up later again on most cloud infrastructure
because IP addresses typically change and this disrupts things like GridEngine
and Hadoop.

    # terminate the cluster/node
    $ perl bin/launcher/destroy_launcher.pl --cluster-name target-aws-1/

You should always check in the AWS console (or OpenStack, vCloud, or other
console for a different cloud) that your nodes have been terminated otherwise
you will be billed for a machine you think is terminated.


#### Next Steps

Much more information can be found in the README for the Bindle project, see https://github.com/CloudBindle

In latter sections of this document you can see more information about:

* differences with other PanCancer clouds environments, what needs to change in the above detailed steps, see "Cloud-Specific Notes" below
* different templates available, for example, ones that automatically install the BWA-Mem workflow, see "Additional Configuration Profiles" below

## Additional Configuration Profiles

This section describes some additional profiles we have available for the
PanCancer project. These are hosted by [pancancer-bag](https://github.com/ICGC-TCGA-PanCancer/monitoring-bag) 

First, please see the general documentation above and the
README for Bindle, the tool we use to build these clusters using
Vagrant. This will walk you through the process of using this software.  This
tool allows us to create clusters in different cloud environments using a
common set of configuration scripts.  We have used this project to prepare two
different profiles, one for building clusters of VMs and another for single,
stand-alone VMs.  In addition, each of those can optionally install our
reference BWA (and potentially other) workflows.  This latter process can be
very time consuming so that is why we provide a profile with and without the
workflow(s) pre-installed.

### Cluster Without Workflows

Simply use Bindle (using the cfg file) at the seqware-bag playbook and omit pancancer-bag. 

### Cluster with the BWA Alignment Workflow

Simply use Bindle (using the cfg file) at the pancancer-bag playbook. 

### Cluster with the Sanger Variant Calling Workflow

TBD

### Cluster with the DKFZ/EMBL Variant Calling Workflow

TBD

### Cluster With the Broad Variant Calling Workflow

TBD

## Cloud-Specific Notes

Each cloud used for PanCancer will be slightly different.  This section covers
information that is particular to each.

### Notes for the EBI Embassy Cloud (vCloud)

The Embassy Cloud at EBI uses vCloud.  The Vagrant vCloud plugin has limited
functionality and, therefore, only single nodes can be launched there.

### Notes for BioNimbus PDC 1.1 (OpenStack)

BioNimbus uses OpenStack and the Vagrant OpenStack plugin is quite stable however the PDC 1.1 environment is in flux. You
can launch VM clusters or single nodes.

You can create a launcher host with nova commands directly:

    # find the image 
    nova image-list
    # find the flavor of machine
    nova flavor-list
    # now combine this info and boot a launcher
    nova boot --image 06e01852-489e-4ae5-aba7-d62de3c3cffd --flavor m1.xlarge --key-name brian-pdc-3 launcher1
    # look at the status
    nova list

When you launch the cluster you need to do the following differently from the examples above:

    # install the OpenStack vagrant plugin
    $ vagrant plugin install vagrant-openstack-plugin
    # make sure you apply the rsync fix described in the README.md

    # example launching a host 
    $ perl bin/launcher/launch_cluster.pl --use-openstack --use-default-config --launch-cluster cluster1

There are several items you need to take care of post-provisioning to ensure you have a working cluster:

* generate your keypair using the web conole (or add the public key using command line tools: "nova keypair-add brian-pdc-3 > brian-pdc-3.pem; chmod 600 brian-pdc-3.pem; ssh-keygen -f brian-pdc-3.pem -y >> ~/.ssh/authorized_keys").
* make sure you patch the rsync issue, see README.md for this project
* you need to run SeqWare workflows as your own user not seqware. This has several side effects:
    * when you launch your cluster, login to the master node
    * "sudo su - seqware" and disable the seqware cronjob
    * make the following directories in your home directory: provisioned-bundles, released-bundles, crons, logs, jars, workflow-dev, and .seqware
    * copy the seqware binary to somewhere on your user path
    * copy the .bash_profile contents from the seqware account to your account
    * copy the .seqware/settings file from the seqware account to your account, modify paths
    * change the OOZIE_WORK_DIR variable to a shared gluster directory such as /glusterfs/data/ICGC1/scratch, BioNimbus will tell you where this should be
    * create a directory on HDFS in /user/$USERNAME, chown this directory to your usesrname.  For example: "sudo su - hdfs;  hadoop fs -mkdir /user/BOCONNOR; hadoop fs -chown BOCONNOR /user/BOCONNOR"
    * copy the seqware cronjob to your own user directory, modify the scripts to have your paths, install the cronjob
    * install the workflow(s) you want, these may already be in your released-bundles directory e.g. "seqware bundle install --zip Workflow_Bundle_BWA_2.2.0_SeqWare_1.0.13.zip"
    * probably want to manually install the BWA workflow rather than via the Bindle provisioning process. This lets you install as your own user in your own directory and not in the seqware directory (or you need to update the SeqWare metadb to point to the right place).  You can see below an example of changing the SeqWare MetaDB to point to your provisioned workflow bundle path:

    update workflow set current_working_dir =  '/glusterfs/netapp/homes1/BOCONNOR/provisioned-bundles/Workflow_Bundle_BWA_2.2.0_SeqWare_1.0.13' where workflow_id = 50;

After these changes you should have a working SeqWare environment set to run workflows as your user.

#### More PDC 1.1 Tips

You can snapshot the VM but if you do you need to ensure the hostname is reset on reboot and tomcat is restarted.  Add the following to /etc/rc.local (the exit 0 is already at the end of the script):

    iptables -D EGRESS -j DROP
    hostname master
    /etc/init.d/tomcat7 restart
    qconf -aattr queue slots "[master=`nproc`]" main.q
    qconf -mattr queue load_thresholds "np_load_avg=`nproc`" main.q
    qconf -rattr exechost complex_values h_vmem=`free -b |grep Mem | cut -d" " -f5` master
    exit 0

That will ensure the machine comes back after reboot.

Also, if you are going to snapshot in OpenStack you will want to make sure you setup SGE according to the instance type you will use in the future e.g. the flavor.  Here I'm adjusting for a 16 core, 64G machine:

    sudo qconf -aattr queue slots "[master=16]" main.q
    sudo qconf -mattr queue load_thresholds "np_load_avg=16" main.q
    sudo qconf -rattr exechost complex_values h_vmem=63000000000 master

To snapshot see the following command:

    nova image-create ff625cf8-e5c9-46b6-9c08-2c6189eb3f9f Ubuntu-12.04-LTS-v1.5.3-CgpCnIndelSnvStr-1.0-SNAPSHOT-SeqWare-1.1.0a5-v1

To launch from an image:

    nova boot --image 0c24790a-9813-41b7-bafa-4006ae8cc215 --flavor m2.xxlarge --key-name brian-pdc-3 fleet_sanger_master_1

Adjust as need be, name your new image as you like.  You can then launch a new copy of this VM image using the standard nova commands.

####Architecture 2.0 Tips

Modify the following file to set the username:

    roles/bindle-profiles/vars/main.yml

Make a symlink since many tools expect /home to be the location of your home dir:

    sudo ln -s /glusterfs/netapp/homes1/BOCONNOR /home/

We had to add a group_name, that needs to be changed to root.

Need to comment out this section in sites.yml

    #- hosts: bindle
    #  sudo: True
    #  tasks:
    #  # this seems like the wrong place for this, but not sure what the best place for it is if we cannot redistribute 
    #  # with the AMI
    #  - name: Copy over pem key for BWA
    #    copy: src=/home/ubuntu/.ssh/gnos.pem dest=/home/ubuntu/.ssh/gnos.pem mode=600
    #  # change permissions
    #  # chamge owner

Ansible uses a temp dir that needs to be changed, this can't be the default which points to the home dir on gluster.

    # in /etc/ansible/ansible.cfg
    remote_tmp=/tmp/.ansible/tmp
    
There's a username that needs to be changed

   # BOCONNOR@i-000007be:~/architecture2/Bindle$ vim ../pancancer-bag/roles/genetorrent/vars/main.yml 
   ---
   user_name: "seqware"

Take a look at my bindle config, Denis and I made changes tonight, copy it here from the launcher on PDC.

   seqware bundle launch --dir /home/seqware/provisioned-bundles/Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.5
   Workflow job completed ...
    Application Path   : hdfs://master:8020/user/seqware/seqware_workflow/oozie-0c08a822-f9a4-470c-8787-59198b874936
    Application Name   : HelloWorld
    Application Status : SUCCEEDED
    Application Actions:
   Name: :start: Type: :START: Status: OK
   Name: start_0 Type: sge Status: OK
   Name: provisionFile_file_in_0_1 Type: sge Status: OK
   Name: bash_mkdir_2 Type: sge Status: OK
   Name: fork_2 Type: :FORK: Status: OK
   Name: bash_cp_3 Type: sge Status: OK
   Name: bash_cp_4 Type: sge Status: OK
   Name: join_2 Type: :JOIN: Status: OK
   Name: provisionFile_out_5 Type: sge Status: OK
   Name: done Type: fs Status: OK
   Name: end Type: :END: Status: OK

LEFT OFF WITH: Denis and I got the launcher to bring up a worker host all the way through to workflow install then it was skipped (BWA).  Need to figure this out but at least I can manually install:

    GATHERING FACTS *************************************************************** 
    ok: [master]
    
    TASK: [workflow | Detect workflows installed] ********************************* 
    skipping: [master]
    
    TASK: [workflow | Download workflows] ***************************************** 
    skipping: [master] => (item=workflows)
    
    TASK: [workflow | Setup bundle permissions] *********************************** 
    skipping: [master] => (item=workflows)
    
    TASK: [workflow | Install workflow bundles] *********************************** 
    skipping: [master] => (item=workflows)
    
    TASK: [workflow | Remove workflow bundles] ************************************ 
    skipping: [master] => (item=workflows)

####R Package Problem

Note: on BioNimbus I ran into an issue with r-cran-rcolorbrewer not being up to date with R 3.x.  See http://stackoverflow.com/questions/16503554/r-3-0-0-update-has-left-loads-of-2-x-packages-incompatible

    sudo R
    update.packages(ask=FALSE, checkBuilt=TRUE)

#### Routing Problem

You need to deactivate the drop permission on iptables to ensure the server can reach the GNOS repositories.  The following should be added to the /etc/rc.local

    iptables -D EGRESS -j DROP

#### Retry Settings

See https://seqware.github.io/docs/6-pipeline/user-configuration/ for details on how to configure Oozie to auto-retry failed jobs.  For PDC I'm setting this to 3 retries for production environments and hopefully this will prevent any transient issues from killing a workflow (but will make failures more of a problem since the workflows will take longer to fail).

By default it looks like this is already setup but I expanded the error codes for PDC 1.1 to include SGE 1 and 2 in addition to the ones we already have:

    oozie.service.ActionCheckerService.action.check.interval5
oozie.service.ActionCheckerService.action.check.delay10
oozie.service.LiteWorkflowStoreService.user.retry.max30
oozie.action.retries.max30
oozie.service.WorkflowAppService.WorkflowDefinitionMaxLength10000000
oozie.service.LiteWorkflowStoreService.user.retry.error.code.extSGE1,SGE2,SGE82,SGE137
oozie.validate.ForkJoinfalse

### Notes for OICR (OpenStack)

OICR uses OpenStack internally for testing and the Vagrant OpenStack plugin is
quite stable.  The cluster is not available to the general PanCancer group.

See https://wiki.oicr.on.ca/display/SOFTENG/Cloud+Environments

General steps:

* generate your keypair using the web conole

Here are some difference from the docs above:

    # install the open stack vagrant plugin
    $ vagrant plugin install vagrant-openstack-plugin

    # example launching a host
    $ perl bin/launch_cluster.pl --config openstack --custom-params singlenode1
    # you'll then edit this
    $ vim /home/ubuntu/.bindle/openstack.cfg

You need to create a float IP address (or use one that's not in use) and include in the config, see https://sweng.os.oicr.on.ca/horizon/project/access_and_security/

Also note, here are the additional things I had to do to get this to work:

* I absolutely had to use float IP addresses for all nodes. Without the float IPs addresses the nodes could not reach the Internet and provisioning failed.
* see our internal wiki for more settings

### Notes for Annai Systems (BioComputeFarm)

Annai provides an OpenStack cluster that works quite well.  You can use it for
both nodes and clusters.

### Notes for Amazon (AWS)

OICR uses AWS internally for testing and the AWS Vagrant plugin is quite
stable. The cluster is available for any PanCancer user but is not officially
part of the Phase II activities.

Some issues I had to address on Amazon:

* some AMIs will automount the first ephemeral disk on /mnt, others will not. This causes issues with the provisioning process. We need to improve our support of these various configurations. With the current code, any device on /dev/sdf or above will automatically be formated, mounted, and added to gluster
* the network security group you launch master and workers in must allow incoming connections from itself otherwise the nodes will not be able to communicate with each other
* need to add "SW_CONTROL_NODE_MEMORY=4000" to AMI... need to add to SeqWare bag.

### Notes for Barcelona (VirtualBox)

Cloud are not available for both of these environments.  Instead, please use
VirtualBox to launch a single node and then use the "Export Appliance..."
command to create an OVA file.  This OVA can be converted into a KVM/Xen VM and
deployed as many times as needed to process production data.

### Notes for DKFZ (OpenStack)

DKFZ uses OpenStack and the Vagrant OpenStack plugin can be used to launch VM clusters or single nodes.

#### Step 1 - Create a Launcher instance

Log in the DKFZ's console and create a new instance that will be your "Launcher" host. This instance will allow you to launch individual computational nodes (or clusters of nodes) that actually do the processing of data. 
When creating the instance, use the image name "Ubuntu 12.04-Cloud-2014-04-10" and your pre-defined key-pair.

#### Step 2 - Install Bindle on the Launcher instance

SSH into the DKFZ jumpserver using your unique account credentials (username and pem key should be unique for each user in DKFZ). Copy over the private key used to start the instance in Step 1. From the jumpserver, SSH into the new launcher instance using user "ubuntu" , your private key and the private IP of that instance (visible in the console).

Once logged in the new instance, follow the instructions in the generic section to setup Ansible on this box.

#### Step 3 - Configure Bindle on the Launcher instance

Next step is to get the config generated, you need to use "openstack" not "aws" as used in the generic instructions above.

   $ cd ~/architecture2/Bindle
   # Run the Bindle launcher without a valid cfg file in order to copy over a template
   $ perl bin/launch_cluster.pl --config openstack --custom-params singlenode1

Modify the "~/.bindle/openstack.cfg" file to include your Openstack settings, an example being provided below:

    [defaults]
    platform =openstack 
    os_user= OICR_user    #add the correct Console username we share in DKFZ, it's the same for everybody
    os_api_key= OICR_user #add the correct Console password we share in DKFZ, it's the same for everybody
    os_instance_type= pc.sn.large    #change if needed
    os_image=Ubuntu12.04-Cloud-DKFZ
    os_endpoint='http://192.168.252.12:5000/v2.0/tokens'
    ansible_playbook = ../pancancer-bag/pancancer.yml
    seqware_provider=artifactory
    seqware_version='1.1.0-alpha.5'
    install_bwa=true
    os_ssh_pem_file=/home/ubuntu/.ssh/launch_key.pem       #the path to where you uploaded the private key file that is     paired with the public key in the console
    install_workflow = True
    workflow_name = BWA
    workflows=Workflow_Bundle_BWA_2.6.3_SeqWare_1.1.0-alpha.5,Workflow_Bundle_HelloWorld_1.0-SNAPSHOT_SeqWare_1.1.0-alpha.5
    bwa_workflow_version = 2.6.3
    os_ssh_key_name=key_name   #the name of the SSH key as defined in the Openstack console
    os_ssh_username=ubuntu
    os_tenant="Pancancer Project"
    os_network="Pancancer-Net"
    gluster_device_whitelist=''
    gluster_directory_path='--directorypath /mnt/volumes/gluster1'
    box = dummy
    box_url = 'https://github.com/cloudbau/vagrant-openstack-plugin/raw/master/dummy.box'
    number_of_clusters = 1
    number_of_single_node_clusters = 1
    env= Openstack DKFZ new
    
    [cluster1]
    number_of_nodes=2
    os_floating_ip=
    target_directory=target-os-dkfz-cluster-1
    
    [singlenode1]
    number_of_nodes=1
    os_floating_ip=
    target_directory=target-os-dkfz-singlenode-1
    

Create a file containing your GNOS pem file that will be injected by Bindle in the new instances, so they can download GNOS data.

    vim ~/.ssh/gnos.pem
    chmod 600 ~/.ssh/gnos.pem

##### Note About Network

Amazon uses "Security Groups" to control access between VMs and the world.  You need to make sure:

* the security group used by your launcher allows for ssh (otherwise you wouldn't have gotten this far) 
* the security group used by your launcher allows for all ports between itself and the "default" security group since that's what Bindle uses

#### Step 4 - Other DKFZ specific changes

In order for the Launcher to be able to reach the Openstack API, please run the following commands:

    sudo iptables -t nat -A OUTPUT -d 192.168.252.11/32 -j DNAT --to-destination 193.174.55.66
    sudo iptables -t nat -A OUTPUT -d 192.168.252.12/32 -j DNAT --to-destination 193.174.55.78

Also, add these lines in "/etc/rc.local" before the "exit 0" line, so the changes survive a reboot:

    iptables -t nat -A OUTPUT -d 192.168.252.11/32 -j DNAT --to-destination 193.174.55.66
    iptables -t nat -A OUTPUT -d 192.168.252.12/32 -j DNAT --to-destination 193.174.55.78

#### Step 5 - Launch a SeqWare Node/Cluster

Now that you have customized the settings in "~/bindle/opensyack.cfg" file, the next step is to launch a computational node. Note, each cluster gets its own target directory as specified in the [singlenode1] block.  Within the target dir you will find a log for each node (simply master.log for a single-node launch) and a directory for each node that is used by the vagrant command line tool (the "master" directory for a single-node launch). The latter is important for controlling your node/cluster once launched.

   $ cd ~/architecture2/Bindle
   $ rm -Rf target*
   
Now launch the compute node:

   $ perl bin/launch_cluster.pl --config openstack --custom-params singlenode1

Follow the rest of the instructions provided in this document (https://github.com/ICGC-TCGA-PanCancer/pancancer-info/blob/develop/docs/prod_instance_with_bindle.md#step-5---log-in-to-nodecluster) to SSH into the new instance provisioned by Bindle and run the HelloWorld test workflow.


## Overview

Building your own Android library enables other developers to take advantage of code that you've written.  You can share existing activities, services, images, drawables, resource strings, and layout files that enable other people to leverage your work such as those documented in the [[must have libraries|Must-Have-Libraries]] guide.  Also, if your code base begins to take longer times to compile and/or run, creating a library also enables you to iterate faster by working on a smaller component.  

If you plan to share only standard Java code, you can distribute them packaged as Java Archive Resources (`.jar`) files.  However, if you intend to include resources such as layouts, drawables, or string resources, or even an additional `AndroidManifest.xml` file, you must create an Android Archive Resource [`.aar` file](http://tools.android.com/tech-docs/new-build-system/aar-format) file instead.  An `.aar` file can include the following types of files:

* /AndroidManifest.xml (mandatory)
* /classes.jar (mandatory)
* /res/ (mandatory)
* /R.txt (mandatory)
* /assets/ (optional)
* /libs/*.jar (optional)
* /jni//*.so (optional)
* /proguard.txt (optional)
* /lint.jar (optional)

### Creating a new Android Library

When you create a new Android project, a new application is always created.  You can use this application to test your library.  After creating the project, go to `New` -> `New Module`:



Select `Android Library`.  There is the option to choose `Java library`, but there is a major difference in that an Android library will include not only the Java classes but the resource files, image files, and Android manifest file normally associated with Android.  



Next, you will be prompted to provide a name and the module name.  The name will simply be used to [label](http://developer.android.com/guide/topics/manifest/manifest-intro.html#iconlabel) the application in the Android Manifest file, while the module name will correspond to the directory to be created:



When you click Next, a directory with the module name will be generated along with other files including a resource and Java folder:



In addition, a `build.gradle` file will be created.  One major difference is that Android applications use the `com.android.application` plugin.  Android libraries will use the `com.android.library` plugin.  This statement at the top signals to the [Android Gradle plug-in](http://developer.android.com/tools/building/plugin-for-gradle.html) to generate an `.aar` file instead of an `.apk` file normally installed on Android devices.

```gradle
// Android library
apply plugin: 'com.android.library'
```

### Compiling a Library

Android applications usually have a build and debug variation.  The `buildTypes` parameter designates the settings for each type of configuration.

```gradle
android {
  buildTypes {
    release {

    } 
    debug {

    }
}
```

You can compile the library with Android Studio, or type `./gradlew build` at the command line.  The output will be stored under the library's subdirectory under `build/outputs/aar`.   Unlike Android applications in which `debug` or `release` versions can be generated, only release versions by default are published as documented [here](http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Referencing-a-Library).  

If you wish to build multiple variations, you will need to add this statement to your library `build.gradle` file:

```gradle
android {
     publishNonDefault true
}
```

When using this statement, different `.aar` packages are generated for each build type specified.  To reference them once they are published, see [[this section|Building-your-own-Android-library#add-the-gradle-dependency]].

If you wish to reference the library from your demo application within the same Android project, you will need to explicitly specify which library to use with the `configuration` parameter.    You need to add this statement to your `app/build.gradle`:

```gradle
dependencies {
    // colon needs to prefixed with the library path
    debugCompile project(path: ':mylibrary', configuration: 'debug')
    releaseCompile project(path: ':mylibrary', configuration: 'release')
}
```

#### Using with ButterKnife

If you intend use the library with [ButterKnife](https://github.com/JakeWharton/butterknife/issues/45), in the past it did not work with Android libraries and you had to convert your code back to `findViewById` calls.  You should upgrade to at least v8.2.0 and follow this [[section|Reducing-View-Boilerplate-with-Butterknife#using-in-your-own-android-libraries]] to enable your libraries to use it.

### Publishing

To publish your library, you can either make it available to a public or private repository.  jCenter and Maven Central are the most popular ones, though jCenter has become the default one used in Android Studio.  For understanding the differences between jCenter or Maven Central, see this [blog link](http://inthecheesefactory.com/blog/how-to-upload-library-to-jcenter-maven-central-as-dependency/en).   

To publish your library straight from GitHub you can use [JitPack](https://jitpack.io). Once you create a GitHub release JitPack will build your library from source and will publish it automatically. 

#### Setting up through jCenter

First, [signup](https://bintray.com/) for a BinTray account.  You will want to create a GPG signing key:
and go to [Edit Profile](https://bintray.com/profile/edit) to add this private/public key pair.

```bash
gpg --gen-key
```

Find the public key ID generated by finding the 8-digit hex after "pub 2048/XXXXXXXX":

```bash
gpg --list-keys
gpg --keyserver hkp://pool.sks-keyservers.net --send-keys [PUBLIC_KEY_ID]
```

Export your keys.  You will want to copy/paste these sections into the `GPG Signing` section:

```bash
gpg -a --export yourmail@email.com > public_key_sender.asc
gpg -a --export-secret-key yourmail@email.com > private_key_sender.asc
```

Click on the `API Key` section when editing your profile.  You will need to provide your username and API Key by setting it locally in your `gradle.properties` file:

```gradle
bintrayUser=user
bintrayApiKey=key
```

Take a look at the examples provided by BinTray [here](https://github.com/bintray/bintray-examples/tree/master/gradle-bintray-plugin-examples).  In particular, you should follow the `android-maven-example`.

Next, edit your root `build.gradle` file.  Add the `android-maven-gradle-plugin`, which will be used to generate the Maven-compatible archive to be shared, as well as the JFrog plugin:

```gradle
buildscript {
   repositories {
      jcenter()
   }
   dependencies {
     // used to generate a POM file
     classpath 'com.github.dcendents:android-maven-gradle-plugin:1.5'
   }
}
 
// Plugin used to upload authenticated files to BinTray through Gradle
plugins {
   id "com.jfrog.bintray" version "1.7.3"
}
```

Inside your `library/build.gradle` file, you will want to apply the Android Maven Gradle and JFrog plugin:

```gradle
apply plugin: 'com.android.library'
apply plugin: 'com.github.dcendents.android-maven'
apply plugin: 'com.jfrog.bintray'
```

Next, you will need to define constants that will be used to generate the XML files used by Maven to understand information about the package. Gradle compile statements are usually follow the form of `GROUP_ID:ARTIFACT_ID:VERSION`, such as 'com.squareup.picasso:picasso:2.5.2', so we should always to make sure these values are set.

```gradle
// If your directory matches the name, you do not need to set archivesBaseName.
archivesBaseName = "android-oauth-handler"

install {
    repositories.mavenInstaller {
        pom.project {
            group "com.codepath.libraries"
            artifactId "android-oauth-handler"
            version "1.0.0"
        }
    }
}
```

The remaining section should be added for authenticating uploads to BinTray.  Note that the `configurations` option alerts the plugin to upload the final packages generated. 

```gradle
bintray {	
	user = project.hasProperty('bintrayUser') ? project.property('bintrayUser') : System.getenv('BINTRAY_USER')
	key = project.hasProperty('bintrayApiKey') ? project.property('bintrayApiKey') : System.getenv('BINTRAY_API_KEY')
        // jFrog plugin must be declared for this line to work
	configurations = ['archives']
        // Package info for BinTray
	pkg {
		repo = 'maven'
		name = 'android-oauth-handler'
		userOrg = user
		licenses = ['Apache-2.0']
		vcsUrl = 'https://github.com/bintray/gradle-bintray-plugin.git'
		version {
			name = '0.1'
			desc = 'Gradle Bintray Plugin 1.0 final'
			vcsTag = '0.1'
		}	
	}
}
```

If you want to test to see if the package works locally, type:

```bash
./gradlew install
```

The package will be installed in your ~/.m2/repository.  If you wish to try the library out, you can add this private Maven repository to the root `build.gradle` config of the application that will be using te 

```gradle
allprojects {

    repositories {
        // add first
        maven { url "${System.env.HOME}/.m2/repository" }
        jcenter()
    }
```

To upload your package, just type:

```bash
# Set your Bintray user ID below
export BINTRAY_USER="codepath"
# Set your Bintray API key below
export BINTRAY_API_KEY="YOUR_BINTRAY_API_KEY_HERE"
./gradlew bintrayUpload 
```

#### Setting up a private Amazon S3 Maven repository

Another approach is to setup a private Maven repository, which also be done through Amazon's Web Services (AWS) and the Simple Storage Service [(S3)](https://aws.amazon.com/s3/).  Gradle supports the ability to access private S3 repositories with a secret access key and ID used to authenticate with Amazon:

#### Adding the private Maven repository 

To add the S3 repository to the list, you will need to add the credentials to access the S3 bucket to your root `build.gradle` file:

```gradle
allprojects {
    repositories {
        jcenter()

        maven {
            url "s3://yourmavenrepo-bucket/android/snapshots"
            credentials(AwsCredentials) {
                accessKey AWS_ACCESS_KEY
                secretKey AWS_SECRET_KEY
            }
        }
    }
}
```

Instead of adding the keys directly, it is recommended that you add it to your `local.properties` to your local machine:

```gradle
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
```

In order to publish the plugin, we need to create a separate Gradle file that can be use in our library configuration.  Create a file called `gradle/gradle-mvn-push.gradle`, which will apply the Gradle-Maven plugin and specify the location of the S3 bucket when using the `./gradlew publish` command:

```gradle
// Inspired from https://gist.github.com/adrianbk/c4982e5ebacc6b6ed902

apply plugin: 'maven-publish'

def isReleaseBuild() {
    return VERSION_NAME.contains("SNAPSHOT") == false
}

def getOutputDir() {
    if (isReleaseBuild()) {
        return "${project.buildDir}/releases"
    } else {
        return "${project.buildDir}/snapshots"
    }
}

def getDestUrl() {
    if (isReleaseBuild()) {
        return "s3://my-bucket/releases"
    } else {
        return "s3://my-bucket/snapshots"
    }
}

publishing {
    publications {
        myPublication (MavenPublication) {
            groupId GROUP
            artifactId POM_ARTIFACT_ID
            version VERSION_NAME
        }
   }
   repositories {
      maven {
        url getDestUrl()
        credentials(AwsCredentials) {
         accessKey = "key"
         secretKey = "password"
        }
      }
   }
}

```

We will then apply the statements in this file by applying it within the library's Gradle file (i.e. `mylibrary/build.gradle`):

```gradle
apply from: rootProject.file('gradle/gradle-mvn-push.gradle')
```

Edit the root project's `gradle.properties` too:

```gradle
VERSION_NAME=0.4-SNAPSHOT
VERSION_CODE=0.4
GROUP=com.codepath
```

Finally, we need to setup the metadata necessary for publishing.  Edit the library's `gradle.properties` and set the values:

```gradle
POM_NAME=My Library
POM_ARTIFACT_ID=library
POM_PACKAGING=aar
```

#### Support for Amazon IAM Roles

Currently Gradle's Amazon S3 integration only supports access keys and does not support [Identity Access Management (IAM)](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) roles.  There is an existing backlog as reported in this [ticket](https://discuss.gradle.org/t/sts-iam-role-credentials-for-s3-maven-repository/14010) but currently it is not officially supported.

To take advantage of a client that does, you can output the repository to a local file and use the AWS command-line S3 client to copy the snapshot dirs:  

```gradle

uploadArchives {
    repositories {
        mavenDeployer {
          repository(url: "file:///" + getOutputDir())
        }
    }
}

task copyToS3(type: Exec) {
    commandLine 'aws', 's3', 'cp', '--recursive', getOutputDir(), getDestUrl()
}

copyToS3.dependsOn uploadArchives
```

To publish and execute the task to copy the build to S3, the command to enter is `./gradlew copyToS3`.

#### Add the Gradle dependency

Once the private S3 repository has been added to the list, you can simply add this line to the Gradle dependency list.  The Android Gradle plug-in will search through all possible repositories searching for a match.  Add this line to your `app/build.gradle` file:
 
```gradle
dependencies {
  compile 'com.codepath:mylibrary:0.4-SNAPSHOT'
}
```

If you published multiple versions of your package as described [[here|Building-your-own-Android-library#building-different-versions]], you will need to specify the build type (i.e. `release` or `debug`).  Gradle may assume that if you specify the build type to search for a `.jar` file, so you must also specify the `@aar`.  Using this syntax doesn't follow the dependencies included in the package, so `transitive=true` must also be included as described in this [Google discussion](https://groups.google.com/forum/#!msg/adt-dev/Ll2JcCfgBsQ/eHjJ8EcZI5MJ).

```gradle
dependencies {
  releaseCompile('com.codepath:mylibrary:0.4:release@aar') {
    transitive=true
  }
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
  }
}
```

#### Prevent caching

If you are making constant changes to your snapshot versions and wish to have the latest updates pulled each time, you can mark the dependency as a **changing module** in Gradle:

```gradle
dependencies {
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
    changing=true
  }
}
```
  
Gradle will normally cache the module for 24 hours for those marked as changing, but you can lower this setting:

```gradle
configurations.all {
    // check for updates every build
    resolutionStrategy.cacheChangingModulesFor 0, 'seconds'
}
```

#### Issues with JDK 8u60 

If you are trying to access a private Amazon S3 repository, you may see an `AWS authentication requires a valid Date or x-amz-date header` error.  It is a known issue with [Gradle](https://issues.gradle.org/browse/GRADLE-3338) and Java versions.  

To fix this issue, you will need to upgrade to Gradle v2.8 by editing your `gradle/wrapper.properties`:
```gradle
distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-all.zip
```

Even though the default Gradle version used in Android projects is 2.4, the build should compile without needing to make any changes.

### Using with ProGuard

If you intend to export your release, you should also include any configurations in case ProGuard is applied to your library.  If you specify `consumerProguardFiles` in your library config, the ProGuard rules will be added  during the compilation.  

```gradle
android {
   defaultConfig {
      minifyEnabled true
      consumerProguardFiles 'consumer-proguard-rules.pro'
   }
}
```

Make sure to create a `consumer-proguard-rules.pro` file.  See [[Configuring ProGuard]] for more details.

If you use the default configuration, ProGuard will obfuscate and alter the name of your library classes, making it impossible for Android projects to reference them.  The most basic example of ensuring your library classes are exported is shown below:

```
-dontobfuscate
# See https://speakerdeck.com/chalup/proguard 
-optimizations !code/allocation/variable

-keep public class * {
    public protected *;
}
```

See the [ProGuard documentation](http://proguard.sourceforge.net/manual/usage.html) for more information about the syntax of this file.  See [this example](https://stuff.mit.edu/afs/sipb/project/android/sdk/android-sdk-linux/tools/proguard/docs/index.html#manual/examples.html) of an Android library definition.

### Resource Merging

If your Android library defines an `AndroidManifest.xml` or any other resource files (i.e. `strings.xml`, `colors.xml`), these resource will be automatically merged with your application.  In this way, you do not have to redeclare permissions that are needed in your library in your main application.   However, if your library declares color styles that may conflict with the appearance in your main application, you may need to rename these styles.

If you do wish to understand how the final `AndroidManifest.xml` is generated, you can decode the final `.apk` file using a third-party tool called [apktool](http://ibotpeaches.github.io/Apktool/).  Instructions to install are located [here](http://ibotpeaches.github.io/Apktool/install/).  If you are upgrading the `apktool` version, you may need to delete the `$HOME/apktool/framework/1.apk` file.  

Once you have the tool installed, you simply need to type this line:

```bash
apktool decode <.apk file>
```

The tool should decode your `.apk` file and allow you to better understand how the final resource files are generated.  

### References

* 
* 
* 
* 
## Overview

Building your own Android library enables other developers to take advantage of code that you've written.  You can share existing activities, services, images, drawables, resource strings, and layout files that enable other people to leverage your work such as those documented in the [[must have libraries|Must-Have-Libraries]] guide.  Also, if your code base begins to take longer times to compile and/or run, creating a library also enables you to iterate faster by working on a more smaller component.  

If you plan to share only standard Java code, you can distribute them packaged as Java Archive Resources (`.jar`) files.  However, if you intend to include resources such as layouts, drawables, or string resources, or even an additional `AndroidManifest.xml` file, you must create an Android Archive Resource [`.aar` file](http://tools.android.com/tech-docs/new-build-system/aar-format) file instead.  An `.aar` file can include the following types of files:

* /AndroidManifest.xml (mandatory)
* /classes.jar (mandatory)
* /res/ (mandatory)
* /R.txt (mandatory)
* /assets/ (optional)
* /libs/*.jar (optional)
* /jni//*.so (optional)
* /proguard.txt (optional)
* /lint.jar (optional)

### Creating a new Android Library

When you create a new Android project, a new application is always created.  You can use this application to test your library.  After creating the project, go to `New` -> `New Module`:



Select `Android Library`.  There is the option to choose `Java library`, but there is a major difference in that an Android library will include not only the Java classes but the resource files, image files, and Android manifest file normally associated with Android.  



Next, you will be prompted to provide a name and the module name.  The name will simply be used to [label](http://developer.android.com/guide/topics/manifest/manifest-intro.html#iconlabel) the application in the Android Manifest file, while the module name will correspond to the directory to be created:



When you click Next, a directory with the module name will be generated along with other files including a resource and Java folder:



In addition, a `build.gradle` file will be created.  One major difference is that Android applications use the `com.android.application` plugin.  Android libraries will use the `com.android.library` plugin.  This statement at the top signals to the [Android Gradle plug-in](http://developer.android.com/tools/building/plugin-for-gradle.html) to generate an `.aar` file instead of an `.apk` file normally installed on Android devices.

```gradle
// Android library
apply plugin: 'com.android.library'
```

### Compiling a Library

Android applications usually have a build and debug variation.  The `buildTypes` parameter designates the settings for each type of configuration.

```gradle
android {
  buildTypes {
    release {

    } 
    debug {

    }
}
```

You can compile the library with Android Studio, or type `./gradlew build` at the command line.  The output will be stored under the library's subdirectory under `build/outputs/aar`.   Unlike Android applications in which `debug` or `release` versions can be generated, only release versions by default are published as documented [here](http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Referencing-a-Library).  

If you wish to build multiple variations, you will need to add this statement to your library `build.gradle` file:

```gradle
android {
     publishNonDefault true
}
```

When using this statement, different `.aar` packages are generated for each build type specified.  To reference them once they are published, see [[this section|Building-your-own-Android-library#add-the-gradle-dependency]].

If you wish to reference the library from your demo application within the same Android project, you will need to explicitly specify which library to use with the `configuration` parameter.    You need to add this statement to your `app/build.gradle`:

```gradle
dependencies {
    // colon needs to prefixed with the library path
    debugCompile project(path: ':mylibrary', configuration: 'debug')
    releaseCompile project(path: ':mylibrary', configuration: 'release')
}
```

#### Using with ButterKnife

If you intend use the library with [ButterKnife](https://github.com/JakeWharton/butterknife/issues/45), in the past it did not work with Android libraries and you had to convert your code back to `findViewById` calls.  You should upgrade to at least v8.2.0 and follow this [[section|Reducing-View-Boilerplate-with-Butterknife#using-in-your-own-android-libraries]] to enable your libraries to use it.

### Publishing

To publish your library, you can either make it available to a public or private repository.  jCenter and Maven Central are the most popular ones, though jCenter has become the default one used in Android Studio.  For understanding the differences between jCenter or Maven Central, see this [blog link](http://inthecheesefactory.com/blog/how-to-upload-library-to-jcenter-maven-central-as-dependency/en).   

#### Setting up through jCenter

First, [signup](https://bintray.com/) for a BinTray account.  You will want to create a GPG signing key:
and go to [Edit Profile](https://bintray.com/profile/edit) to add this private/public key pair.

```bash
gpg --gen-key
```

Find the public key ID generated by finding the 8-digit hex after "pub 2048/XXXXXXXX":

```bash
gpg --list-keys
gpg --keyserver hkp://pool.sks-keyservers.net --send-keys [PUBLIC_KEY_ID]
```

Export your keys.  You will want to copy/paste these sections into the `GPG Signing` section:

```bash
gpg -a --export yourmail@email.com > public_key_sender.asc
gpg -a --export-secret-key yourmail@email.com > private_key_sender.asc
```

Click on the `API Key` section when editing your profile.  You will need to provide your username and API Key by setting it locally in your `gradle.properties` file:

```gradle
bintrayUser=user
bintrayApiKey=key
```

Take a look at the examples provided by BinTray [here](https://github.com/bintray/bintray-examples/tree/master/gradle-bintray-plugin-examples).  In particular, you should follow the `android-maven-example`.

Next, edit your root `build.gradle` file.  Add the `android-maven-gradle-plugin`, which will be used to generate the Maven-compatible archive to be shared, as well as the JFrog plugin:

```gradle
buildscript {
   repositories {
      jcenter()
   }
   dependencies {
     // used to generate a POM file
     classpath 'com.github.dcendents:android-maven-gradle-plugin:1.5'
   }
}
 
// Plugin used to upload authenticated files to BinTray through Gradle
plugins {
   id "com.jfrog.bintray" version "1.5"
}
```

Inside your `library/build.gradle` file, you will want to apply the Android Maven Gradle and JFrog plugin:

```gradle
apply plugin: 'com.android.library'
apply plugin: 'com.github.dcendents.android-maven'
apply plugin: 'com.jfrog.bintray'
```

Next, you will need to define constants that will be used to generate the XML files used by Maven to understand information about the package. Gradle compile statements are usually follow the form of `GROUP_ID:ARTIFACT_ID:VERSION`, such as 'com.squareup.picasso:picasso:2.5.2', so we should always to make sure these values are set.

```gradle
// If your directory matches the name, you do not need to set archivesBaseName.
archivesBaseName = "android-oauth-handler"

install {
    repositories.mavenInstaller {
        pom.project {
            group "com.codepath.libraries"
            artifactId "android-oauth-handler"
            version "1.0.0"
        }
    }
}
```

The remaining section should be added for authenticating uploads to BinTray.  Note that the `configurations` option alerts the plugin to upload the final packages generated. 

```gradle
bintray {	
	user = project.hasProperty('bintrayUser') ? project.property('bintrayUser') : System.getenv('BINTRAY_USER')
	key = project.hasProperty('bintrayApiKey') ? project.property('bintrayApiKey') : System.getenv('BINTRAY_API_KEY')
        // jFrog plugin must be declared for this line to work
	configurations = ['archives']
        // Package info for BinTray
	pkg {
		repo = 'maven'
		name = 'android-oauth-handler'
		userOrg = user
		licenses = ['Apache-2.0']
		vcsUrl = 'https://github.com/bintray/gradle-bintray-plugin.git'
		version {
			name = '0.1'
			desc = 'Gradle Bintray Plugin 1.0 final'
			vcsTag = '0.1'
		}	
	}
}
```

To upload your package, just type:

```bash

```bash
# Set your Bintray user ID below
export BINTRAY_USER="codepath"
# Set your Bintray API key below
export BINTRAY_API_KEY="YOUR_BINTRAY_API_KEY_HERE"
./gradlew :bintrayUpload // i.e. ./gradlew app:bintrayUpload
```

You need to specify a subproject currently because of this [reported bug](https://github.com/bintray/gradle-bintray-plugin/issues/74#issuecomment-244616013).

#### Setting up a private Amazon S3 Maven repository

Another approach is to setup a private Maven repository, which also be done through Amazon's Web Services (AWS) and the Simple Storage Service [(S3)](https://aws.amazon.com/s3/).  Gradle supports the ability to access private S3 repositories with a secret access key and ID used to authenticate with Amazon:

#### Adding the private Maven repository 

To add the S3 repository to the list, you will need to add the credentials to access the S3 bucket to your root `build.gradle` file:

```gradle
allprojects {
    repositories {
        jcenter()

        maven {
            url "s3://yourmavenrepo-bucket/android/snapshots"
            credentials(AwsCredentials) {
                accessKey AWS_ACCESS_KEY
                secretKey AWS_SECRET_KEY
            }
        }
    }
}
```

Instead of adding the keys directly, it is recommended that you add it to your `local.properties` to your local machine:

```gradle
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
```

In order to publish the plugin, we need to create a separate Gradle file that can be use in our library configuration.  Create a file called `gradle/gradle-mvn-push.gradle`, which will apply the Gradle-Maven plugin and specify the location of the S3 bucket when using the `./gradlew publish` command:

```gradle
// Inspired from https://gist.github.com/adrianbk/c4982e5ebacc6b6ed902

apply plugin: 'maven-publish'

def isReleaseBuild() {
    return VERSION_NAME.contains("SNAPSHOT") == false
}

def getOutputDir() {
    if (isReleaseBuild()) {
        return "${project.buildDir}/releases"
    } else {
        return "${project.buildDir}/snapshots"
    }
}

def getDestUrl() {
    if (isReleaseBuild()) {
        return "s3://my-bucket/releases"
    } else {
        return "s3://my-bucket/snapshots"
    }
}

publishing {
    publications {
        myPublication (MavenPublication) {
            groupId GROUP
            artifactId POM_ARTIFACT_ID
            version VERSION_NAME
        }
   }
   repositories {
      maven {
        url getDestUrl()
        credentials(AwsCredentials) {
         accessKey = "key"
         secretKey = "password"
        }
      }
   }
}

```

We will then apply the statements in this file by applying it within the library's Gradle file (i.e. `mylibrary/build.gradle`):

```gradle
apply from: rootProject.file('gradle/gradle-mvn-push.gradle')
```

Edit the root project's `gradle.properties` too:

```gradle
VERSION_NAME=0.4-SNAPSHOT
VERSION_CODE=0.4
GROUP=com.codepath
```

Finally, we need to setup the metadata necessary for publishing.  Edit the library's `gradle.properties` and set the values:

```gradle
POM_NAME=My Library
POM_ARTIFACT_ID=library
POM_PACKAGING=aar
```

#### Support for Amazon IAM Roles

Currently Gradle's Amazon S3 integration only supports access keys and does not support [Identity Access Management (IAM)](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) roles.  There is an existing backlog as reported in this [ticket](https://discuss.gradle.org/t/sts-iam-role-credentials-for-s3-maven-repository/14010) but currently it is not officially supported.

To take advantage of a client that does, you can output the repository to a local file and use the AWS command-line S3 client to copy the snapshot dirs:  

```gradle

uploadArchives {
    repositories {
        mavenDeployer {
          repository(url: "file:///" + getOutputDir())
        }
    }
}

task copyToS3(type: Exec) {
    commandLine 'aws', 's3', 'cp', '--recursive', getOutputDir(), getDestUrl()
}

copyToS3.dependsOn uploadArchives
```

To publish and execute the task to copy the build to S3, the command to enter is `./gradlew copyToS3`.

#### Add the Gradle dependency

Once the private S3 repository has been added to the list, you can simply add this line to the Gradle dependency list.  The Android Gradle plug-in will search through all possible repositories searching for a match.  Add this line to your `app/build.gradle` file:
 
```gradle
dependencies {
  compile 'com.codepath:mylibrary:0.4-SNAPSHOT'
}
```

If you published multiple versions of your package as described [[here|Building-your-own-Android-library#building-different-versions]], you will need to specify the build type (i.e. `release` or `debug`).  Gradle may assume that if you specify the build type to search for a `.jar` file, so you must also specify the `@aar`.  Using this syntax doesn't follow the dependencies included in the package, so `transitive=true` must also be included as described in this [Google discussion](https://groups.google.com/forum/#!msg/adt-dev/Ll2JcCfgBsQ/eHjJ8EcZI5MJ).

```gradle
dependencies {
  releaseCompile('com.codepath:mylibrary:0.4:release@aar') {
    transitive=true
  }
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
  }
}
```

#### Prevent caching

If you are making constant changes to your snapshot versions and wish to have the latest updates pulled each time, you can mark the dependency as a **changing module** in Gradle:

```gradle
dependencies {
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
    changing=true
  }
}
```
  
Gradle will normally cache the module for 24 hours for those marked as changing, but you can lower this setting:

```gradle
configurations.all {
    // check for updates every build
    resolutionStrategy.cacheChangingModulesFor 0, 'seconds'
}
```

#### Issues with JDK 8u60 

If you are trying to access a private Amazon S3 repository, you may see an `AWS authentication requires a valid Date or x-amz-date header` error.  It is a known issue with [Gradle](https://issues.gradle.org/browse/GRADLE-3338) and Java versions.  

To fix this issue, you will need to upgrade to Gradle v2.8 by editing your `gradle/wrapper.properties`:
```gradle
distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-all.zip
```

Even though the default Gradle version used in Android projects is 2.4, the build should compile without needing to make any changes.

### Using with ProGuard

If you intend to export your release, you should also include any configurations in case ProGuard is applied to your library.  If you specify `consumerProguardFiles` in your library config, the ProGuard rules will be added  during the compilation.  

```gradle
android {
   defaultConfig {
      minifyEnabled true
      consumerProguardFiles 'consumer-proguard-rules.pro'
   }
}
```

Make sure to create a `consumer-proguard-rules.pro` file.  See [[Configuring ProGuard]] for more details.

If you use the default configuration, ProGuard will obfuscate and alter the name of your library classes, making it impossible for Android projects to reference them.  The most basic example of ensuring your library classes are exported is shown below:

```
-dontobfuscate
# See https://speakerdeck.com/chalup/proguard 
-optimizations !code/allocation/variable

-keep public class * {
    public protected *;
}
```

See the [ProGuard documentation](http://proguard.sourceforge.net/manual/usage.html) for more information about the syntax of this file.  See [this example](https://stuff.mit.edu/afs/sipb/project/android/sdk/android-sdk-linux/tools/proguard/docs/index.html#manual/examples.html) of an Android library definition.

### Resource Merging

If your Android library defines an `AndroidManifest.xml` or any other resource files (i.e. `strings.xml`, `colors.xml`), these resource will be automatically merged with your application.  In this way, you do not have to redeclare permissions that are needed in your library in your main application.   However, if your library declares color styles that may conflict with the appearance in your main application, you may need to rename these styles.

If you do wish to understand how the final `AndroidManifest.xml` is generated, you can decode the final `.apk` file using a third-party tool called [apktool](http://ibotpeaches.github.io/Apktool/).  Instructions to install are located [here](http://ibotpeaches.github.io/Apktool/install/).  If you are upgrading the `apktool` version, you may need to delete the `$HOME/apktool/framework/1.apk` file.  

Once you have the tool installed, you simply need to type this line:

```bash
apktool decode <.apk file>
```

The tool should decode your `.apk` file and allow you to better understand how the final resource files are generated.  

### References

* 
* 
* 
* snippet rollbar_deployment
	rollbar_deployment: >
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		revision=${3:# REQUIRED}
		comment=${4}
		rollbar_user=${5}
		url=${6:https://api.rollbar.com/api/1/deploy/}
		user=${7}
		validate_certs=${8:#yes|no}

snippet logentries
	logentries: path=${1:# REQUIRED} state=${2:#present|absent}

snippet nagios
	nagios: >
		action=${1:#downtime|enable_alerts|disable_alerts|silence|unsilence|silence_nagios|unsilence_nagios|command}
		command=${2:# REQUIRED}
		services=${3:# REQUIRED}
		host=${4}
		author=${5:ansible}
		minutes=${6:30}
		cmdfile=${7:auto-detected}

snippet newrelic_deployment
	newrelic_deployment: >
		token=${1:# REQUIRED}
		application_id=${2}
		description=${3}
		changelog=${4}
		appname=${5}
		environment=${6}
		user=${7}
		revision=${8}
		validate_certs=${9:#yes|no}
		app_name=${10}

snippet librato_annotation
	librato_annotation: >
		links=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		user=${4:# REQUIRED}
		description=${5}
		start_time=${6}
		name=${7}
		source=${8}
		end_time=${9}

snippet stackdriver
	stackdriver: >
		key=${1:# REQUIRED}
		repository=${2}
		level=${3:#INFO|WARN|ERROR}
		annotated_by=${4:ansible}
		deployed_to=${5}
		deployed_by=${6:ansible}
		instance_id=${7}
		msg=${8}
		event_epoch=${9}
		revision_id=${10}
		event=${11:#annotation|deploy}

snippet pagerduty
	pagerduty: >
		name=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|started|ongoing}
		user=${4:# REQUIRED}
		service=${5}
		hours=${6:1}
		validate_certs=${7:#yes|no}
		desc=${8:created by ansible}

snippet boundary_meter
	boundary_meter: >
		apikey=${1:# REQUIRED}
		apiid=${2:# REQUIRED}
		name=${3:# REQUIRED}
		state=${4:#present|absent}
		validate_certs=${5:#yes|no}

snippet airbrake_deployment
	airbrake_deployment: >
		environment=${1:# REQUIRED}
		token=${2:# REQUIRED}
		repo=${3}
		user=${4}
		url=${5:https://airbrake.io/deploys}
		validate_certs=${6:#yes|no}
		revision=${7}

snippet pingdom
	pingdom: >
		checkid=${1:# REQUIRED}
		passwd=${2:# REQUIRED}
		state=${3:#running|paused}
		uid=${4:# REQUIRED}
		key=${5:# REQUIRED}

snippet datadog_event
	datadog_event: >
		text=${1:# REQUIRED}
		title=${2:# REQUIRED}
		api_key=${3:# REQUIRED}
		date_happened=${4:now}
		alert_type=${5:#error|warning|info|success}
		tags=${6}
		priority=${7:#normal|low}
		aggregation_key=${8}
		validate_certs=${9:#yes|no}

snippet monit
	monit: state=${1:#present|started|stopped|restarted|monitored|unmonitored|reloaded} name=${2:# REQUIRED}

snippet redhat_subscription
	redhat_subscription: >
		username=${1}
		server_hostname=${2:current value from c(/etc/rhsm/rhsm.conf) is the default}
		state=${3:#present|absent}
		autosubscribe=${4:false}
		activationkey=${5}
		server_insecure=${6:current value from c(/etc/rhsm/rhsm.conf) is the default}
		password=${7}
		rhsm_baseurl=${8:current value from c(/etc/rhsm/rhsm.conf) is the default}
		pool=${9:^$}

snippet zypper
	zypper: name=${1:# REQUIRED} state=${2:#present|latest|absent} disable_gpg_check=${3:#yes|no}

snippet homebrew_cask
	homebrew_cask: name=${1:# REQUIRED} state=${2:#installed|uninstalled}

snippet yum
	yum: >
		name=${1:# REQUIRED}
		state=${2:#present|latest|absent}
		disablerepo=${3}
		enablerepo=${4}
		list=${5}
		disable_gpg_check=${6:#yes|no}
		conf_file=${7}

snippet apt_repository
	apt_repository: >
		repo=${1:# REQUIRED}
		update_cache=${2:#yes|no}
		state=${3:#absent|present}
		validate_certs=${4:#yes|no}
		mode=${5:420}

snippet pkgutil
	pkgutil: state=${1:#present|absent|latest} name=${2:# REQUIRED} site=${3}

snippet apt
	apt: >
		dpkg_options=${1:force-confdef,force-confold}
		upgrade=${2:#yes|safe|full|dist}
		force=${3:#yes|no}
		name=${4}
		purge=${5:#yes|no}
		state=${6:#latest|absent|present}
		update_cache=${7:#yes|no}
		default_release=${8}
		cache_valid_time=${9:false}
		deb=${10}
		install_recommends=${11:#yes|no}

snippet svr4pkg
	svr4pkg: >
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		category=${3:#true|false}
		src=${4}
		zone=${5:#current|all}
		response_file=${6}
		proxy=${7}

snippet npm
	npm: >
		executable=${1}
		name=${2}
		global=${3:#yes|no}
		state=${4:#present|absent|latest}
		production=${5:#yes|no}
		registry=${6}
		version=${7}
		path=${8}

snippet pkgng
	pkgng: >
		name=${1:# REQUIRED}
		cached=${2:#yes|no}
		state=${3:#present|absent}
		pkgsite=${4}
		annotation=${5}

snippet openbsd_pkg
	openbsd_pkg: state=${1:#present|latest|absent} name=${2:# REQUIRED}

snippet gem
	gem: >
		name=${1:# REQUIRED}
		include_dependencies=${2:#yes|no}
		executable=${3}
		repository=${4}
		user_install=${5:yes}
		pre_release=${6:no}
		state=${7:#present|absent|latest}
		version=${8}
		gem_source=${9}

snippet layman
	layman: name=${1:# REQUIRED} list_url=${2} state=${3:#present|absent|updated}

snippet composer
	composer: >
		working_dir=${1:# REQUIRED}
		prefer_dist=${2:#yes|no}
		prefer_source=${3:#yes|no}
		no_scripts=${4:#yes|no}
		no_dev=${5:#yes|no}
		no_plugins=${6:#yes|no}
		optimize_autoloader=${7:#yes|no}

snippet pkgin
	pkgin: name=${1:# REQUIRED} state=${2:#present|absent}

snippet zypper_repository
	zypper_repository: >
		repo=${1}
		state=${2:#absent|present}
		description=${3}
		disable_gpg_check=${4:#yes|no}
		name=${5}

snippet pip
	pip: >
		virtualenv=${1}
		virtualenv_site_packages=${2:#yes|no}
		virtualenv_command=${3:virtualenv}
		chdir=${4}
		requirements=${5}
		name=${6}
		executable=${7}
		extra_args=${8}
		state=${9:#present|absent|latest}
		version=${10}

snippet cpanm
	cpanm: >
		notest=${1:false}
		from_path=${2}
		name=${3}
		locallib=${4:false}
		mirror=${5:false}

snippet homebrew_tap
	homebrew_tap: tap=${1:# REQUIRED} state=${2:#present|absent}

snippet portinstall
	portinstall: name=${1:# REQUIRED} state=${2:#present|absent} use_packages=${3:#yes|no}

snippet homebrew
	homebrew: >
		name=${1:# REQUIRED}
		update_homebrew=${2:#yes|no}
		install_options=${3}
		state=${4:#head|latest|present|absent|linked|unlinked}
		upgrade_all=${5:#yes|no}

snippet rhn_channel
	rhn_channel: >
		sysname=${1:# REQUIRED}
		name=${2:# REQUIRED}
		url=${3:# REQUIRED}
		password=${4:# REQUIRED}
		user=${5:# REQUIRED}
		state=${6:present}

snippet apt_key
	apt_key: >
		keyserver=${1}
		url=${2}
		data=${3}
		keyring=${4}
		state=${5:#absent|present}
		file=${6}
		validate_certs=${7:#yes|no}
		id=${8}

snippet opkg
	opkg: name=${1:# REQUIRED} state=${2:#present|absent} update_cache=${3:#yes|no}

snippet rhn_register
	rhn_register: >
		username=${1}
		channels=${2:[]}
		state=${3:#present|absent}
		activationkey=${4}
		password=${5}
		server_url=${6:current value of i(serverurl) from c(/etc/sysconfig/rhn/up2date) is the default}

snippet easy_install
	easy_install: >
		name=${1:# REQUIRED}
		virtualenv=${2}
		virtualenv_site_packages=${3:#yes|no}
		virtualenv_command=${4:virtualenv}
		executable=${5}

snippet swdepot
	swdepot: state=${1:#present|latest|absent} name=${2:# REQUIRED} depot=${3}

snippet rpm_key
	rpm_key: key=${1:# REQUIRED} state=${2:#present|absent} validate_certs=${3:#yes|no}

snippet portage
	portage: >
		nodeps=${1:#yes}
		onlydeps=${2:#yes}
		newuse=${3:#yes}
		package=${4}
		oneshot=${5:#yes}
		update=${6:#yes}
		deep=${7:#yes}
		quiet=${8:#yes}
		sync=${9:#yes|web}
		state=${10:#present|installed|emerged|absent|removed|unmerged}
		depclean=${11:#yes}
		noreplace=${12:#yes}
		verbose=${13:#yes}

snippet macports
	macports: name=${1:# REQUIRED} state=${2:#present|absent|active|inactive} update_cache=${3:#yes|no}

snippet pacman
	pacman: recurse=${1:#yes|no} state=${2:#present|absent} update_cache=${3:#yes|no} name=${4}

snippet apt_rpm
	apt_rpm: pkg=${1:# REQUIRED} state=${2:#absent|present} update_cache=${3:#yes|no}

snippet urpmi
	urpmi: >
		pkg=${1:# REQUIRED}
		force=${2:#yes|no}
		state=${3:#absent|present}
		no-suggests=${4:#yes|no}
		update_cache=${5:#yes|no}

snippet add_host
	add_host: name=${1:# REQUIRED} groups=${2}

snippet group_by
	group_by: key=${1:# REQUIRED}

snippet win_feature
	win_feature: >
		name=${1:# REQUIRED}
		include_management_tools=${2:#True|False}
		include_sub_features=${3:#True|False}
		state=${4:#present|absent}
		restart=${5:#True|False}

snippet win_stat
	win_stat: path=${1:# REQUIRED} get_md5=${2:true}

snippet win_service
	win_service: name=${1:# REQUIRED} start_mode=${2:#auto|manual|disabled} state=${3:#started|stopped|restarted}

snippet win_get_url
	win_get_url: url=${1:# REQUIRED} dest=${2:true}

snippet win_group
	win_group: name=${1:# REQUIRED} state=${2:#present|absent} description=${3}

snippet slurp
	slurp: src=${1:# REQUIRED}

snippet win_user
	win_user: password=${1:# REQUIRED} name=${2:# REQUIRED} state=${3:#present|absent}

snippet win_ping
	win_ping: data=${1:pong}

snippet setup
	setup: filter=${1:*} fact_path=${2:/etc/ansible/facts.d}

snippet win_msi
	win_msi: path=${1:# REQUIRED} creates=${2} state=${3:#present|absent}

snippet mail
	mail: >
		subject=${1:# REQUIRED}
		body=${2:$subject}
		from=${3:root}
		to=${4:root}
		headers=${5}
		cc=${6}
		charset=${7:us-ascii}
		bcc=${8}
		attach=${9}
		host=${10:localhost}
		port=${11:25}

snippet sns
	sns: >
		topic=${1:# REQUIRED}
		msg=${2:# REQUIRED}
		aws_secret_key=${3}
		aws_access_key=${4}
		http=${5}
		sqs=${6}
		region=${7}
		sms=${8}
		https=${9}
		email=${10}
		subject=${11}

snippet twilio
	twilio: >
		msg=${1:# REQUIRED}
		auth_token=${2:# REQUIRED}
		from_number=${3:# REQUIRED}
		to_number=${4:# REQUIRED}
		account_sid=${5:# REQUIRED}

snippet osx_say
	osx_say: msg=${1:# REQUIRED} voice=${2}

snippet grove
	grove: >
		message=${1:# REQUIRED}
		channel_token=${2:# REQUIRED}
		service=${3:ansible}
		url=${4}
		icon_url=${5}
		validate_certs=${6:#yes|no}

snippet hipchat
	hipchat: >
		room=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from=${4:ansible}
		color=${5:#yellow|red|green|purple|gray|random}
		msg_format=${6:#text|html}
		api=${7:https://api.hipchat.com/v1/rooms/message}
		notify=${8:#yes|no}
		validate_certs=${9:#yes|no}

snippet jabber
	jabber: >
		to=${1:# REQUIRED}
		user=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		password=${4:# REQUIRED}
		host=${5}
		encoding=${6}
		port=${7:5222}

snippet slack
	slack: >
		domain=${1:# REQUIRED}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		username=${4:ansible}
		icon_url=${5}
		parse=${6:#full|none}
		icon_emoji=${7}
		link_names=${8:#1|0}
		validate_certs=${9:#yes|no}
		channel=${10}

snippet flowdock
	flowdock: >
		type=${1:#inbox|chat}
		token=${2:# REQUIRED}
		msg=${3:# REQUIRED}
		from_name=${4}
		from_address=${5}
		tags=${6}
		external_user_name=${7}
		project=${8}
		source=${9}
		link=${10}
		reply_to=${11}
		subject=${12}
		validate_certs=${13:#yes|no}

snippet typetalk
	typetalk: topic=${1:# REQUIRED} client_secret=${2:# REQUIRED} client_id=${3:# REQUIRED} msg=${4:# REQUIRED}

snippet irc
	irc: >
		msg=${1:# REQUIRED}
		channel=${2:# REQUIRED}
		key=${3}
		color=${4:#none|yellow|red|green|blue|black}
		server=${5:localhost}
		nick=${6:ansible}
		passwd=${7}
		timeout=${8:30}
		port=${9:6667}

snippet campfire
	campfire: >
		msg=${1:# REQUIRED}
		token=${2:# REQUIRED}
		subscription=${3:# REQUIRED}
		room=${4:# REQUIRED}
		notify=${5:#56k|bell|bezos|bueller|clowntown|cottoneyejoe|crickets|dadgummit|dangerzone|danielsan|deeper|drama|greatjob|greyjoy|guarantee|heygirl|horn|horror|inconceivable|live|loggins|makeitso|noooo|nyan|ohmy|ohyeah|pushit|rimshot|rollout|rumble|sax|secret|sexyback|story|tada|tmyk|trololo|trombone|unix|vuvuzela|what|whoomp|yeah|yodel}

snippet nexmo
	nexmo: >
		src=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		api_secret=${3:# REQUIRED}
		api_key=${4:# REQUIRED}
		msg=${5:# REQUIRED}
		validate_certs=${6:#yes|no}

snippet mqtt
	mqtt: >
		topic=${1:# REQUIRED}
		payload=${2:# REQUIRED}
		username=${3}
		qos=${4:#0|1|2}
		port=${5:1883}
		server=${6:localhost}
		client_id=${7:hostname + pid}
		retain=${8:false}
		password=${9}

snippet async_status
	async_status: jid=${1:# REQUIRED} mode=${2:#status|cleanup}

snippet apache2_module
	apache2_module: name=${1:# REQUIRED} state=${2:#present|absent}

snippet jira
	jira: >
		username=${1:# REQUIRED}
		uri=${2:# REQUIRED}
		operation=${3:#create|comment|edit|fetch|transition}
		password=${4:# REQUIRED}
		comment=${5}
		description=${6}
		fields=${7}
		summary=${8}
		project=${9}
		assignee=${10}
		status=${11}
		issuetype=${12}
		issue=${13}

snippet ejabberd_user
	ejabberd_user: >
		username=${1:# REQUIRED}
		host=${2:# REQUIRED}
		password=${3}
		logging=${4:#true|false|yes|no}
		state=${5:#present|absent}

snippet jboss
	jboss: deployment=${1:# REQUIRED} src=${2} deploy_path=${3:/var/lib/jbossas/standalone/deployments} state=${4:#present|absent}

snippet django_manage
	django_manage: >
		app_path=${1:# REQUIRED}
		command=${2:#cleanup|collectstatic|flush|loaddata|migrate|runfcgi|syncdb|test|validate}
		virtualenv=${3}
		settings=${4}
		pythonpath=${5}
		database=${6}
		apps=${7}
		cache_table=${8}
		merge=${9}
		skip=${10}
		link=${11}
		fixtures=${12}
		failfast=${13:#yes|no}

snippet supervisorctl
	supervisorctl: >
		state=${1:#present|started|stopped|restarted}
		name=${2:# REQUIRED}
		username=${3}
		supervisorctl_path=${4}
		password=${5}
		config=${6}
		server_url=${7}

snippet htpasswd
	htpasswd: >
		name=${1:# REQUIRED}
		path=${2:# REQUIRED}
		state=${3:#present|absent}
		create=${4:#yes|no}
		password=${5}
		crypt_scheme=${6:#apr_md5_crypt|des_crypt|ldap_sha1|plaintext}

snippet rabbitmq_parameter
	rabbitmq_parameter: >
		name=${1:# REQUIRED}
		component=${2:# REQUIRED}
		node=${3:rabbit}
		vhost=${4:/}
		state=${5:#present|absent}
		value=${6}

snippet rabbitmq_policy
	rabbitmq_policy: >
		name=${1:# REQUIRED}
		tags=${2:# REQUIRED}
		pattern=${3:# REQUIRED}
		node=${4:rabbit}
		priority=${5:0}
		state=${6:#present|absent}
		vhost=${7:/}

snippet rabbitmq_plugin
	rabbitmq_plugin: names=${1:# REQUIRED} state=${2:#enabled|disabled} new_only=${3:#yes|no} prefix=${4}

snippet rabbitmq_user
	rabbitmq_user: >
		user=${1:# REQUIRED}
		node=${2:rabbit}
		force=${3:#yes|no}
		tags=${4}
		read_priv=${5:^$}
		write_priv=${6:^$}
		state=${7:#present|absent}
		configure_priv=${8:^$}
		vhost=${9:/}
		password=${10}

snippet rabbitmq_vhost
	rabbitmq_vhost: name=${1:# REQUIRED} node=${2:rabbit} tracing=${3:#yes|no} state=${4:#present|absent}

snippet raw
	raw: ${1} executable=${2}

snippet script
	script: ${1} creates=${2} removes=${3}

snippet command
	command: ${1} >
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet shell
	shell: ${1} >
		creates=${2}
		chdir=${3}
		removes=${4}
		executable=${5}

snippet stat
	stat: path=${1:# REQUIRED} get_md5=${2:true} follow=${3:false}

snippet acl
	acl: >
		name=${1:# REQUIRED}
		default=${2:#yes|no}
		entity=${3}
		state=${4:#query|present|absent}
		follow=${5:#yes|no}
		etype=${6:#user|group|mask|other}
		entry=${7}
		permissions=${8}

snippet unarchive
	unarchive: dest=${1:# REQUIRED} src=${2:# REQUIRED} copy=${3:#yes|no} creates=${4}

snippet fetch
	fetch: >
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		validate_md5=${3:#yes|no}
		fail_on_missing=${4:#yes|no}
		flat=${5}

snippet file
	file: >
		path=${1:[]}
		src=${2}
		serole=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:#yes|no}
		setype=${8}
		group=${9}
		state=${10:#file|link|directory|hard|touch|absent}
		mode=${11}
		owner=${12}

snippet xattr
	xattr: >
		name=${1:# REQUIRED}
		key=${2}
		follow=${3:#yes|no}
		state=${4:#read|present|all|keys|absent}
		value=${5}

snippet copy
	copy: >
		dest=${1:# REQUIRED}
		src=${2}
		directory_mode=${3}
		force=${4:#yes|no}
		selevel=${5:s0}
		seuser=${6}
		recurse=${7:false}
		serole=${8}
		content=${9}
		setype=${10}
		mode=${11}
		owner=${12}
		group=${13}
		validate=${14}
		backup=${15:#yes|no}

snippet synchronize
	synchronize: >
		dest=${1:# REQUIRED}
		src=${2:# REQUIRED}
		dirs=${3:#yes|no}
		links=${4:#yes|no}
		copy_links=${5:#yes|no}
		compress=${6:#yes|no}
		rsync_timeout=${7:0}
		rsync_opts=${8}
		owner=${9:#yes|no}
		set_remote_user=${10:true}
		rsync_path=${11}
		recursive=${12:#yes|no}
		group=${13:#yes|no}
		existing_only=${14:#yes|no}
		archive=${15:#yes|no}
		checksum=${16:#yes|no}
		times=${17:#yes|no}
		perms=${18:#yes|no}
		mode=${19:#push|pull}
		dest_port=${20:22}
		delete=${21:#yes|no}

snippet template
	template: dest=${1:# REQUIRED} src=${2:# REQUIRED} validate=${3} backup=${4:#yes|no}

snippet bigip_node
	bigip_node: >
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		name=${6}
		partition=${7:common}
		description=${8}

snippet bigip_monitor_http
	bigip_monitor_http: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		receive_disable=${3:# REQUIRED}
		name=${4:# REQUIRED}
		receive=${5:# REQUIRED}
		send=${6:# REQUIRED}
		server=${7:# REQUIRED}
		interval=${8}
		parent=${9:http}
		ip=${10}
		port=${11}
		partition=${12:common}
		state=${13:#present|absent}
		time_until_up=${14}
		timeout=${15}
		parent_partition=${16:common}

snippet arista_vlan
	arista_vlan: vlan_id=${1:# REQUIRED} state=${2:#present|absent} logging=${3:#true|false|yes|no} name=${4}

snippet bigip_monitor_tcp
	bigip_monitor_tcp: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		name=${3:# REQUIRED}
		receive=${4:# REQUIRED}
		send=${5:# REQUIRED}
		server=${6:# REQUIRED}
		interval=${7}
		parent=${8:#tcp|tcp_echo|tcp_half_open}
		ip=${9}
		port=${10}
		partition=${11:common}
		state=${12:#present|absent}
		time_until_up=${13}
		timeout=${14}
		parent_partition=${15:common}
		type=${16:#TTYPE_TCP|TTYPE_TCP_ECHO|TTYPE_TCP_HALF_OPEN}

snippet openvswitch_bridge
	openvswitch_bridge: bridge=${1:# REQUIRED} state=${2:#present|absent} timeout=${3:5}

snippet dnsimple
	dnsimple: >
		solo=${1}
		domain=${2}
		account_email=${3}
		record_ids=${4}
		value=${5}
		priority=${6}
		record=${7}
		state=${8:#present|absent}
		ttl=${9:3600 (one hour)}
		type=${10:#A|ALIAS|CNAME|MX|SPF|URL|TXT|NS|SRV|NAPTR|PTR|AAAA|SSHFP|HINFO|POOL}
		account_api_token=${11}

snippet dnsmadeeasy
	dnsmadeeasy: >
		domain=${1:# REQUIRED}
		account_secret=${2:# REQUIRED}
		account_key=${3:# REQUIRED}
		state=${4:#present|absent}
		record_name=${5}
		record_ttl=${6:1800}
		record_type=${7:#A|AAAA|CNAME|HTTPRED|MX|NS|PTR|SRV|TXT}
		record_value=${8}
		validate_certs=${9:#yes|no}

snippet openvswitch_port
	openvswitch_port: bridge=${1:# REQUIRED} port=${2:# REQUIRED} state=${3:#present|absent} timeout=${4:5}

snippet bigip_pool_member
	bigip_pool_member: >
		state=${1:#present|absent}
		server=${2:# REQUIRED}
		host=${3:# REQUIRED}
		user=${4:# REQUIRED}
		password=${5:# REQUIRED}
		port=${6:# REQUIRED}
		pool=${7:# REQUIRED}
		ratio=${8}
		description=${9}
		connection_limit=${10}
		partition=${11:common}
		rate_limit=${12}

snippet arista_lag
	arista_lag: >
		interface_id=${1:# REQUIRED}
		lacp=${2:#active|passive|off}
		state=${3:#present|absent}
		minimum_links=${4}
		logging=${5:#true|false|yes|no}
		links=${6}

snippet arista_interface
	arista_interface: >
		interface_id=${1:# REQUIRED}
		duplex=${2:#auto|half|full}
		logging=${3:#true|false|yes|no}
		description=${4}
		admin=${5:#up|down}
		speed=${6:#auto|100m|1g|10g}
		mtu=${7:1500}

snippet bigip_facts
	bigip_facts: >
		include=${1:#address_class|certificate|client_ssl_profile|device_group|interface|key|node|pool|rule|self_ip|software|system_info|traffic_group|trunk|virtual_address|virtual_server|vlan}
		user=${2:# REQUIRED}
		password=${3:# REQUIRED}
		server=${4:# REQUIRED}
		filter=${5}
		session=${6:true}

snippet arista_l2interface
	arista_l2interface: >
		interface_id=${1:# REQUIRED}
		state=${2:#present|absent}
		logging=${3:#true|false|yes|no}
		tagged_vlans=${4}
		vlan_tagging=${5:#enable|disable}
		untagged_vlan=${6:default}

snippet bigip_pool
	bigip_pool: >
		name=${1:# REQUIRED}
		server=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		lb_method=${5:#round_robin|ratio_member|least_connection_member|observed_member|predictive_member|ratio_node_address|least_connection_node_address|fastest_node_address|observed_node_address|predictive_node_address|dynamic_ratio|fastest_app_response|least_sessions|dynamic_ratio_member|l3_addr|unknown|weighted_least_connection_member|weighted_least_connection_node_address|ratio_session|ratio_least_connection_member|ratio_least_connection_node_address}
		quorum=${6}
		partition=${7:common}
		slow_ramp_time=${8}
		state=${9:#present|absent}
		service_down_action=${10:#none|reset|drop|reselect}
		port=${11}
		host=${12}
		monitors=${13}
		monitor_type=${14:#and_list|m_of_n}

snippet netscaler
	netscaler: >
		name=${1:hostname}
		nsc_host=${2:# REQUIRED}
		user=${3:# REQUIRED}
		password=${4:# REQUIRED}
		type=${5:#server|service}
		nsc_protocol=${6:https}
		action=${7:#enable|disable}
		validate_certs=${8:#yes|no}

snippet accelerate
	accelerate: >
		timeout=${1:300}
		minutes=${2:30}
		port=${3:5099}
		multi_key=${4:false}
		ipv6=${5:false}

snippet debug
	debug: msg=${1:hello world!} var=${2}

snippet wait_for
	wait_for: >
		delay=${1:0}
		state=${2:#present|started|stopped|absent}
		timeout=${3:300}
		search_regex=${4}
		path=${5}
		host=${6:127.0.0.1}
		port=${7}

snippet assert
	assert: that=${1:# REQUIRED}

snippet set_fact
	set_fact: key_value=${1:# REQUIRED}

snippet pause
	pause: seconds=${1} minutes=${2} prompt=${3}

snippet include_vars
	include_vars: ${1}

snippet fail
	fail: msg=${1:'failed as requested from task'}

snippet fireball
	fireball: minutes=${1:30} port=${2:5099}

snippet mount
	mount: >
		src=${1:# REQUIRED}
		name=${2:# REQUIRED}
		fstype=${3:# REQUIRED}
		state=${4:#present|absent|mounted|unmounted}
		dump=${5}
		fstab=${6:/etc/fstab}
		passno=${7}
		opts=${8}

snippet seboolean
	seboolean: state=${1:#yes|no} name=${2:# REQUIRED} persistent=${3:#yes|no}

snippet at
	at: >
		count=${1:# REQUIRED}
		units=${2:#minutes|hours|days|weeks}
		state=${3:#present|absent}
		command=${4}
		unique=${5:false}
		script_file=${6}

snippet authorized_key
	authorized_key: >
		user=${1:# REQUIRED}
		key=${2:# REQUIRED}
		key_options=${3}
		state=${4:#present|absent}
		path=${5:(homedir)+/.ssh/authorized_keys}
		manage_dir=${6:#yes|no}

snippet locale_gen
	locale_gen: name=${1:# REQUIRED} state=${2:#present|absent}

snippet user
	user: >
		name=${1:# REQUIRED}
		comment=${2}
		ssh_key_bits=${3:2048}
		update_password=${4:#always|on_create}
		non_unique=${5:#yes|no}
		force=${6:#yes|no}
		ssh_key_type=${7:rsa}
		ssh_key_passphrase=${8}
		groups=${9}
		home=${10}
		move_home=${11:#yes|no}
		password=${12}
		generate_ssh_key=${13:#yes|no}
		append=${14:#yes|no}
		uid=${15}
		ssh_key_comment=${16:ansible-generated}
		group=${17}
		createhome=${18:#yes|no}
		system=${19:#yes|no}
		remove=${20:#yes|no}
		state=${21:#present|absent}
		ssh_key_file=${22:$home/.ssh/id_rsa}
		login_class=${23}
		shell=${24}

snippet cron
	cron: >
		name=${1}
		hour=${2:*}
		job=${3}
		cron_file=${4}
		reboot=${5:#yes|no}
		month=${6:*}
		state=${7:#present|absent}
		special_time=${8:#reboot|yearly|annually|monthly|weekly|daily|hourly}
		user=${9:root}
		backup=${10:false}
		day=${11:*}
		minute=${12:*}
		weekday=${13:*}

snippet lvol
	lvol: >
		lv=${1:# REQUIRED}
		vg=${2:# REQUIRED}
		state=${3:#present|absent}
		force=${4:#yes|no}
		size=${5}

snippet debconf
	debconf: >
		name=${1:# REQUIRED}
		value=${2}
		vtype=${3:#string|boolean|select|multiselect|note|text|password|title}
		question=${4}
		unseen=${5:false}

snippet firewalld
	firewalld: >
		state=${1:enabled}
		permanent=${2:true}
		zone=${3:#work|drop|internal|external|trusted|home|dmz|public|block}
		service=${4}
		timeout=${5:0}
		rich_rule=${6}
		port=${7}

snippet capabilities
	capabilities: capability=${1:# REQUIRED} path=${2:# REQUIRED} state=${3:#present|absent}

snippet group
	group: name=${1:# REQUIRED} state=${2:#present|absent} gid=${3} system=${4:#yes|no}

snippet modprobe
	modprobe: name=${1:# REQUIRED} state=${2:#present|absent} params=${3}

snippet alternatives
	alternatives: path=${1:# REQUIRED} name=${2:# REQUIRED} link=${3}

snippet filesystem
	filesystem: dev=${1:# REQUIRED} fstype=${2:# REQUIRED} force=${3:#yes|no} opts=${4}

snippet sysctl
	sysctl: >
		name=${1:# REQUIRED}
		reload=${2:#yes|no}
		state=${3:#present|absent}
		sysctl_set=${4:#yes|no}
		ignoreerrors=${5:#yes|no}
		sysctl_file=${6:/etc/sysctl.conf}
		value=${7}

snippet hostname
	hostname: name=${1:# REQUIRED}

snippet kernel_blacklist
	kernel_blacklist: name=${1:# REQUIRED} blacklist_file=${2} state=${3:#present|absent}

snippet lvg
	lvg: >
		vg=${1:# REQUIRED}
		vg_options=${2}
		pvs=${3}
		force=${4:#yes|no}
		pesize=${5:4}
		state=${6:#present|absent}

snippet ufw
	ufw: >
		insert=${1}
		direction=${2:#in|out|incoming|outgoing}
		from_port=${3}
		logging=${4:#on|off|low|medium|high|full}
		log=${5:#yes|no}
		proto=${6:#any|tcp|udp|ipv6|esp|ah}
		to_port=${7}
		from_ip=${8:any}
		rule=${9:#allow|deny|reject|limit}
		name=${10}
		policy=${11:#allow|deny|reject}
		state=${12:#enabled|disabled|reloaded|reset}
		interface=${13}
		to_ip=${14:any}
		delete=${15:#yes|no}

snippet service
	service: >
		name=${1:# REQUIRED}
		state=${2:#started|stopped|restarted|reloaded}
		sleep=${3}
		runlevel=${4:default}
		pattern=${5}
		enabled=${6:#yes|no}
		arguments=${7}

snippet zfs
	zfs: >
		state=${1:#present|absent}
		name=${2:# REQUIRED}
		setuid=${3:#on|off}
		zoned=${4:#on|off}
		primarycache=${5:#all|none|metadata}
		logbias=${6:#latency|throughput}
		sync=${7:#on|off}
		copies=${8:#1|2|3}
		sharenfs=${9}
		sharesmb=${10}
		canmount=${11:#on|off|noauto}
		mountpoint=${12}
		casesensitivity=${13:#sensitive|insensitive|mixed}
		utf8only=${14:#on|off}
		xattr=${15:#on|off}
		compression=${16:#on|off|lzjb|gzip|gzip-1|gzip-2|gzip-3|gzip-4|gzip-5|gzip-6|gzip-7|gzip-8|gzip-9|lz4|zle}
		shareiscsi=${17:#on|off}
		aclmode=${18:#discard|groupmask|passthrough}
		exec=${19:#on|off}
		dedup=${20:#on|off}
		aclinherit=${21:#discard|noallow|restricted|passthrough|passthrough-x}
		readonly=${22:#on|off}
		recordsize=${23}
		jailed=${24:#on|off}
		secondarycache=${25:#all|none|metadata}
		refquota=${26}
		quota=${27}
		volsize=${28}
		vscan=${29:#on|off}
		reservation=${30}
		atime=${31:#on|off}
		normalization=${32:#none|formC|formD|formKC|formKD}
		volblocksize=${33}
		checksum=${34:#on|off|fletcher2|fletcher4|sha256}
		devices=${35:#on|off}
		nbmand=${36:#on|off}
		refreservation=${37}
		snapdir=${38:#hidden|visible}

snippet open_iscsi
	open_iscsi: >
		auto_node_startup=${1:#True|False}
		target=${2}
		show_nodes=${3:#True|False}
		node_auth=${4:chap}
		node_pass=${5}
		discover=${6:#True|False}
		portal=${7}
		login=${8:#True|False}
		node_user=${9}
		port=${10:3260}

snippet selinux
	selinux: state=${1:#enforcing|permissive|disabled} policy=${2} conf=${3:/etc/selinux/config}

snippet hg
	hg: >
		repo=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		purge=${3:#yes|no}
		executable=${4}
		force=${5:#yes|no}
		revision=${6:default}

snippet git
	git: >
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		executable=${3}
		remote=${4:origin}
		recursive=${5:#yes|no}
		reference=${6}
		accept_hostkey=${7:#yes|no}
		update=${8:#yes|no}
		ssh_opts=${9}
		depth=${10}
		version=${11:head}
		bare=${12:#yes|no}
		force=${13:#yes|no}
		key_file=${14}

snippet bzr
	bzr: >
		dest=${1:# REQUIRED}
		name=${2:# REQUIRED}
		executable=${3}
		version=${4:head}
		force=${5:#yes|no}

snippet subversion
	subversion: >
		dest=${1:# REQUIRED}
		repo=${2:# REQUIRED}
		username=${3}
		executable=${4}
		force=${5:#yes|no}
		export=${6:#yes|no}
		password=${7}
		revision=${8:head}

snippet github_hooks
	github_hooks: >
		repo=${1:# REQUIRED}
		oauthkey=${2:# REQUIRED}
		user=${3:# REQUIRED}
		action=${4:#create|cleanall}
		validate_certs=${5:#yes|no}
		hookurl=${6}

snippet digital_ocean_sshkey
	digital_ocean_sshkey: >
		state=${1:#present|absent}
		name=${2}
		client_id=${3}
		api_key=${4}
		id=${5}
		ssh_pub_key=${6}

snippet ovirt
	ovirt: >
		user=${1:# REQUIRED}
		password=${2:# REQUIRED}
		url=${3:# REQUIRED}
		instance_name=${4:# REQUIRED}
		instance_mem=${5}
		instance_cores=${6:1}
		instance_cpus=${7:1}
		image=${8}
		instance_disksize=${9}
		instance_nic=${10}
		instance_network=${11:rhevm}
		sdomain=${12}
		instance_os=${13}
		zone=${14}
		disk_alloc=${15:#thin|preallocated}
		region=${16}
		instance_type=${17:#server|desktop}
		state=${18:#present|absent|shutdown|started|restarted}
		resource_type=${19:#new|template}
		disk_int=${20:#virtio|ide}

snippet ec2_ami
	ec2_ami: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		security_token=${5}
		delete_snapshot=${6}
		region=${7}
		state=${8:present}
		instance_id=${9}
		image_id=${10}
		no_reboot=${11:#yes|no}
		wait_timeout=${12:300}
		ec2_url=${13}
		wait=${14:#yes|no}
		validate_certs=${15:#yes|no}
		description=${16}

snippet ec2_metric_alarm
	ec2_metric_alarm: >
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		comparison=${4}
		alarm_actions=${5}
		ok_actions=${6}
		security_token=${7}
		evaluation_periods=${8}
		metric=${9}
		description=${10}
		namespace=${11}
		period=${12}
		ec2_url=${13}
		profile=${14}
		insufficient_data_actions=${15}
		statistic=${16}
		threshold=${17}
		aws_access_key=${18}
		validate_certs=${19:#yes|no}
		unit=${20}
		dimensions=${21}

snippet elasticache
	elasticache: >
		name=${1:# REQUIRED}
		state=${2:#present|absent|rebooted}
		engine=${3:memcached}
		aws_secret_key=${4}
		cache_port=${5:11211}
		security_group_ids=${6:['default']}
		cache_engine_version=${7:1.4.14}
		region=${8}
		num_nodes=${9}
		node_type=${10:cache.m1.small}
		cache_security_groups=${11:['default']}
		hard_modify=${12:#yes|no}
		aws_access_key=${13}
		zone=${14}
		wait=${15:#yes|no}

snippet ec2_lc
	ec2_lc: >
		name=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		spot_price=${7}
		security_token=${8}
		key_name=${9}
		region=${10}
		user_data=${11}
		image_id=${12}
		volumes=${13}
		ec2_url=${14}
		instance_monitoring=${15:false}
		validate_certs=${16:#yes|no}
		security_groups=${17}

snippet quantum_router_gateway
	quantum_router_gateway: >
		router_name=${1:# REQUIRED}
		login_tenant_name=${2:yes}
		login_password=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet quantum_floating_ip_associate
	quantum_floating_ip_associate: >
		instance_name=${1:# REQUIRED}
		login_tenant_name=${2:true}
		login_password=${3:yes}
		login_username=${4:admin}
		ip_address=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2_key
	ec2_key: >
		name=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		key_material=${7}
		state=${8:present}
		wait_timeout=${9:300}
		ec2_url=${10}
		validate_certs=${11:#yes|no}
		wait=${12:false}

snippet ec2_ami_search
	ec2_ami_search: >
		release=${1:# REQUIRED}
		distro=${2:#ubuntu}
		stream=${3:#server|desktop}
		virt=${4:#paravirtual|hvm}
		region=${5:#ap-northeast-1|ap-southeast-1|ap-southeast-2|eu-west-1|sa-east-1|us-east-1|us-west-1|us-west-2}
		arch=${6:#i386|amd64}
		store=${7:#ebs|instance-store}

snippet gce_lb
	gce_lb: >
		httphealthcheck_host=${1}
		protocol=${2:#tcp|udp}
		pem_file=${3}
		members=${4}
		httphealthcheck_port=${5:80}
		httphealthcheck_name=${6}
		name=${7}
		external_ip=${8}
		service_account_email=${9}
		region=${10}
		httphealthcheck_unhealthy_count=${11:2}
		httphealthcheck_healthy_count=${12:2}
		httphealthcheck_path=${13:/}
		port_range=${14}
		state=${15:#active|present|absent|deleted}
		httphealthcheck_timeout=${16:5}
		project_id=${17}
		httphealthcheck_interval=${18:5}

snippet rax_files_objects
	rax_files_objects: >
		container=${1:# REQUIRED}
		username=${2}
		src=${3}
		dest=${4}
		region=${5:dfw}
		expires=${6}
		verify_ssl=${7}
		state=${8:#present|absent}
		clear_meta=${9:#yes|no}
		meta=${10}
		env=${11}
		credentials=${12}
		api_key=${13}
		type=${14:#file|meta}
		method=${15:#get|put|delete}
		structure=${16:#True|no}

snippet quantum_router
	quantum_router: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		admin_state_up=${6:true}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet azure
	azure: >
		image=${1:# REQUIRED}
		storage_account=${2:# REQUIRED}
		name=${3:# REQUIRED}
		location=${4:# REQUIRED}
		role_size=${5:small}
		virtual_network_name=${6}
		wait_timeout_redirects=${7:300}
		wait_timeout=${8:600}
		user=${9}
		password=${10}
		wait=${11:#yes|no}
		management_cert_path=${12}
		hostname=${13}
		ssh_cert_path=${14}
		state=${15:present}
		subscription_id=${16}
		endpoints=${17:22}

snippet gce_net
	gce_net: >
		fwname=${1}
		name=${2}
		src_range=${3}
		allowed=${4}
		src_tags=${5}
		pem_file=${6}
		state=${7:#active|present|absent|deleted}
		service_account_email=${8}
		ipv4_range=${9}
		project_id=${10}

snippet rds_subnet_group
	rds_subnet_group: >
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		subnets=${5}
		aws_access_key=${6}
		description=${7}

snippet rax_clb_nodes
	rax_clb_nodes: >
		load_balancer_id=${1:# REQUIRED}
		username=${2}
		weight=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		wait_timeout=${7:30}
		condition=${8:#enabled|disabled|draining}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		type=${13:#primary|secondary}
		port=${14}
		node_id=${15}
		wait=${16:#yes|no}

snippet docker_image
	docker_image: >
		name=${1:# REQUIRED}
		state=${2:#present|absent|build}
		tag=${3:latest}
		nocache=${4:false}
		path=${5}
		docker_url=${6:unix://var/run/docker.sock}
		timeout=${7:600}

snippet rax_dns
	rax_dns: >
		comment=${1}
		username=${2}
		name=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		ttl=${8:3600}
		credentials=${9}
		api_key=${10}
		email=${11}

snippet ec2_elb
	ec2_elb: >
		instance_id=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		security_token=${6}
		region=${7}
		wait_timeout=${8:0}
		ec2_url=${9}
		wait=${10:#yes|no}
		validate_certs=${11:#yes|no}
		enable_availability_zone=${12:#yes|no}
		ec2_elbs=${13}

snippet digital_ocean
	digital_ocean: >
		unique_name=${1:#yes|no}
		virtio=${2:#yes|no}
		region_id=${3}
		backups_enabled=${4:#yes|no}
		image_id=${5}
		wait_timeout=${6:300}
		client_id=${7}
		ssh_pub_key=${8}
		wait=${9:#yes|no}
		name=${10}
		size_id=${11}
		id=${12}
		state=${13:#present|active|absent|deleted}
		command=${14:#droplet|ssh}
		ssh_key_ids=${15}
		private_networking=${16:#yes|no}
		api_key=${17}

snippet keystone_user
	keystone_user: >
		endpoint=${1:http://127.0.0.1:35357/v2.0/}
		description=${2}
		login_user=${3:admin}
		token=${4}
		login_tenant_name=${5}
		state=${6:#present|absent}
		role=${7}
		user=${8}
		login_password=${9:yes}
		password=${10}
		email=${11}
		tenant=${12}

snippet rax_scaling_policy
	rax_scaling_policy: >
		name=${1:# REQUIRED}
		scaling_group=${2:# REQUIRED}
		policy_type=${3:#webhook|schedule}
		username=${4}
		is_percent=${5:false}
		env=${6}
		region=${7:dfw}
		verify_ssl=${8}
		cron=${9}
		desired_capacity=${10}
		state=${11:#present|absent}
		cooldown=${12}
		at=${13}
		credentials=${14}
		api_key=${15}
		change=${16}

snippet rax_meta
	rax_meta: >
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		meta=${8}
		env=${9}
		address=${10}
		credentials=${11}
		api_key=${12}
		id=${13}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}

snippet quantum_subnet
	quantum_subnet: >
		login_password=${1:true}
		login_username=${2:admin}
		cidr=${3:# REQUIRED}
		network_name=${4:# REQUIRED}
		name=${5:# REQUIRED}
		login_tenant_name=${6:true}
		region_name=${7}
		tenant_name=${8}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}
		allocation_pool_end=${10}
		enable_dhcp=${11:true}
		dns_nameservers=${12}
		state=${13:#present|absent}
		allocation_pool_start=${14}
		gateway_ip=${15}
		ip_version=${16:4}

snippet vsphere_guest
	vsphere_guest: >
		password=${1:# REQUIRED}
		guest=${2:# REQUIRED}
		user=${3:# REQUIRED}
		vcenter_hostname=${4:# REQUIRED}
		resource_pool=${5}
		vm_hw_version=${6}
		force=${7:#yes|no}
		vm_disk=${8}
		esxi=${9}
		vm_nic=${10}
		vm_hardware=${11}
		cluster=${12}
		state=${13:#present|powered_on|absent|powered_on|restarted|reconfigured}
		vmware_guest_facts=${14}
		vm_extra_config=${15}

snippet rax_facts
	rax_facts: >
		username=${1}
		tenant_name=${2}
		name=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		env=${8}
		address=${9}
		credentials=${10}
		api_key=${11}
		id=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet rax_dns_record
	rax_dns_record: >
		name=${1:# REQUIRED}
		data=${2:# REQUIRED}
		type=${3:#A|AAAA|CNAME|MX|NS|SRV|TXT|PTR}
		comment=${4}
		username=${5}
		domain=${6}
		region=${7:dfw}
		verify_ssl=${8}
		server=${9}
		priority=${10}
		state=${11:#present|absent}
		env=${12}
		ttl=${13:3600}
		credentials=${14}
		api_key=${15}
		loadbalancer=${16}

snippet rax_network
	rax_network: >
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		label=${6}
		state=${7:#present|absent}
		env=${8}
		tenant_name=${9}
		credentials=${10}
		cidr=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_tag
	ec2_tag: >
		resource=${1:# REQUIRED}
		aws_secret_key=${2}
		profile=${3}
		aws_access_key=${4}
		security_token=${5}
		region=${6}
		state=${7:#present|absent|list}
		ec2_url=${8}
		validate_certs=${9:#yes|no}

snippet nova_keypair
	nova_keypair: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		public_key=${5}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}

snippet ec2
	ec2: >
		image=${1:# REQUIRED}
		instance_type=${2:# REQUIRED}
		ramdisk=${3}
		kernel=${4}
		volumes=${5}
		count_tag=${6}
		monitoring=${7}
		vpc_subnet_id=${8}
		user_data=${9}
		instance_ids=${10}
		wait_timeout=${11:300}
		profile=${12}
		private_ip=${13}
		assign_public_ip=${14}
		spot_price=${15}
		id=${16}
		source_dest_check=${17:true}
		wait=${18:#yes|no}
		count=${19:1}
		spot_wait_timeout=${20:600}
		aws_access_key=${21}
		group=${22}
		instance_profile_name=${23}
		zone=${24}
		exact_count=${25}
		ebs_optimized=${26:false}
		security_token=${27}
		state=${28:#present|absent|running|stopped}
		aws_secret_key=${29}
		ec2_url=${30}
		placement_group=${31}
		key_name=${32}
		instance_tags=${33}
		group_id=${34}
		validate_certs=${35:#yes|no}
		region=${36}

snippet quantum_network
	quantum_network: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		region_name=${5}
		provider_network_type=${6}
		admin_state_up=${7:true}
		router_external=${8:false}
		tenant_name=${9}
		provider_physical_network=${10}
		state=${11:#present|absent}
		auth_url=${12:http://127.0.0.1:35357/v2.0/}
		shared=${13:false}
		provider_segmentation_id=${14}

snippet rax_cbs
	rax_cbs: >
		size=${1:100}
		volume_type=${2:#SATA|SSD}
		state=${3:#present|absent}
		name=${4:# REQUIRED}
		username=${5}
		api_key=${6}
		tenant_name=${7}
		description=${8}
		identity_type=${9:rackspace}
		tenant_id=${10}
		region=${11:dfw}
		auth_endpoint=${12:https://identity.api.rackspacecloud.com/v2.0/}
		verify_ssl=${13}
		wait_timeout=${14:300}
		meta=${15}
		env=${16}
		snapshot_id=${17}
		credentials=${18}
		wait=${19:#yes|no}

snippet rax_queue
	rax_queue: >
		username=${1}
		name=${2}
		region=${3:dfw}
		verify_ssl=${4}
		state=${5:#present|absent}
		env=${6}
		credentials=${7}
		api_key=${8}

snippet cloudformation
	cloudformation: >
		stack_name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		template=${3:# REQUIRED}
		aws_secret_key=${4}
		aws_access_key=${5}
		disable_rollback=${6:#true|false}
		tags=${7}
		region=${8}
		template_parameters=${9:{}}

snippet rax_identity
	rax_identity: >
		username=${1}
		identity_type=${2:rackspace}
		tenant_id=${3}
		region=${4:dfw}
		verify_ssl=${5}
		state=${6:#present|absent}
		env=${7}
		tenant_name=${8}
		credentials=${9}
		api_key=${10}
		auth_endpoint=${11:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_eip
	ec2_eip: >
		aws_secret_key=${1}
		instance_id=${2}
		aws_access_key=${3}
		security_token=${4}
		reuse_existing_ip_allowed=${5:false}
		region=${6}
		public_ip=${7}
		state=${8:#present|absent}
		in_vpc=${9:false}
		profile=${10}
		ec2_url=${11}
		validate_certs=${12:#yes|no}
		wait_timeout=${13:300}

snippet gc_storage
	gc_storage: >
		gcs_secret_key=${1:# REQUIRED}
		bucket=${2:# REQUIRED}
		gcs_access_key=${3:# REQUIRED}
		mode=${4:#get|put|get_url|get_str|delete|create}
		src=${5}
		force=${6:true}
		permission=${7:private}
		dest=${8}
		object=${9}
		expiration=${10}

snippet rax_scaling_group
	rax_scaling_group: >
		max_entities=${1:# REQUIRED}
		name=${2:# REQUIRED}
		server_name=${3:# REQUIRED}
		image=${4:# REQUIRED}
		min_entities=${5:# REQUIRED}
		flavor=${6:# REQUIRED}
		files=${7}
		username=${8}
		api_key=${9}
		loadbalancers=${10}
		key_name=${11}
		disk_config=${12:#auto|manual}
		verify_ssl=${13}
		state=${14:#present|absent}
		cooldown=${15}
		meta=${16}
		env=${17}
		credentials=${18}
		region=${19:dfw}
		networks=${20:['public', 'private']}

snippet ec2_group
	ec2_group: >
		name=${1:# REQUIRED}
		description=${2:# REQUIRED}
		aws_secret_key=${3}
		rules_egress=${4}
		aws_access_key=${5}
		security_token=${6}
		rules=${7}
		region=${8}
		state=${9:present}
		profile=${10}
		ec2_url=${11}
		vpc_id=${12}
		validate_certs=${13:#yes|no}

snippet quantum_floating_ip
	quantum_floating_ip: >
		login_password=${1:yes}
		instance_name=${2:# REQUIRED}
		login_tenant_name=${3:yes}
		login_username=${4:admin}
		network_name=${5:# REQUIRED}
		region_name=${6}
		state=${7:#present|absent}
		auth_url=${8:http://127.0.0.1:35357/v2.0/}
		internal_network_name=${9}

snippet quantum_router_interface
	quantum_router_interface: >
		login_tenant_name=${1:yes}
		login_password=${2:yes}
		login_username=${3:admin}
		subnet_name=${4:# REQUIRED}
		router_name=${5:# REQUIRED}
		region_name=${6}
		tenant_name=${7}
		state=${8:#present|absent}
		auth_url=${9:http://127.0.0.1:35357/v2.0/}

snippet rax_files
	rax_files: >
		container=${1:# REQUIRED}
		username=${2}
		web_index=${3}
		region=${4:dfw}
		verify_ssl=${5}
		private=${6}
		state=${7:#present|absent}
		clear_meta=${8:#yes|no}
		meta=${9}
		env=${10}
		ttl=${11}
		web_error=${12}
		credentials=${13}
		api_key=${14}
		type=${15:#file|meta}
		public=${16}

snippet ec2_vol
	ec2_vol: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		name=${4}
		zone=${5}
		instance=${6}
		region=${7}
		device_name=${8}
		volume_size=${9}
		state=${10:#absent|present}
		iops=${11:100}
		snapshot=${12}
		ec2_url=${13}
		security_token=${14}
		validate_certs=${15:#yes|no}
		id=${16}

snippet virt
	virt: >
		name=${1:# REQUIRED}
		xml=${2}
		state=${3:#running|shutdown|destroyed|paused}
		command=${4:#create|status|start|stop|pause|unpause|shutdown|undefine|destroy|get_xml|autostart|freemem|list_vms|info|nodeinfo|virttype|define}
		uri=${5}

snippet rax_keypair
	rax_keypair: >
		name=${1:# REQUIRED}
		username=${2}
		public_key=${3}
		identity_type=${4:rackspace}
		tenant_id=${5}
		region=${6:dfw}
		verify_ssl=${7}
		state=${8:#present|absent}
		env=${9}
		tenant_name=${10}
		credentials=${11}
		api_key=${12}
		auth_endpoint=${13:https://identity.api.rackspacecloud.com/v2.0/}

snippet ec2_elb_lb
	ec2_elb_lb: >
		name=${1:# REQUIRED}
		state=${2:# REQUIRED}
		aws_secret_key=${3}
		subnets=${4}
		aws_access_key=${5}
		health_check=${6}
		security_token=${7}
		region=${8}
		purge_subnets=${9:false}
		ec2_url=${10}
		listeners=${11}
		security_group_ids=${12}
		zones=${13}
		purge_listeners=${14:true}
		profile=${15}
		scheme=${16:internet-facing}
		validate_certs=${17:#yes|no}
		purge_zones=${18:false}

snippet nova_compute
	nova_compute: >
		image_id=${1:# REQUIRED}
		login_password=${2:yes}
		login_username=${3:admin}
		name=${4:# REQUIRED}
		login_tenant_name=${5:yes}
		region_name=${6}
		key_name=${7}
		user_data=${8}
		meta=${9}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		wait_for=${11:180}
		security_groups=${12}
		wait=${13:yes}
		nics=${14}
		state=${15:#present|absent}
		flavor_id=${16:1}

snippet linode
	linode: >
		datacenter=${1}
		swap=${2:512}
		api_key=${3}
		name=${4}
		payment_term=${5:#1|12|24}
		linode_id=${6}
		state=${7:#present|active|started|absent|deleted|stopped|restarted}
		wait_timeout=${8:300}
		plan=${9}
		distribution=${10}
		password=${11}
		ssh_pub_key=${12}
		wait=${13:#yes|no}

snippet ec2_facts
	ec2_facts: validate_certs=${1:#yes|no}

snippet rax_cbs_attachments
	rax_cbs_attachments: >
		volume=${1:# REQUIRED}
		device=${2:# REQUIRED}
		server=${3:# REQUIRED}
		state=${4:#present|absent}
		username=${5}
		tenant_name=${6}
		verify_ssl=${7}
		wait_timeout=${8:300}
		credentials=${9}
		wait=${10:#yes|no}
		identity_type=${11:rackspace}
		tenant_id=${12}
		region=${13:dfw}
		auth_endpoint=${14:https://identity.api.rackspacecloud.com/v2.0/}
		env=${15}
		api_key=${16}

snippet docker
	docker: >
		image=${1:# REQUIRED}
		username=${2}
		publish_all_ports=${3:false}
		tty=${4:false}
		env=${5}
		links=${6}
		memory_limit=${7:256mb}
		lxc_conf=${8}
		stdin_open=${9:false}
		volumes=${10}
		password=${11}
		count=${12:1}
		detach=${13:true}
		name=${14}
		hostname=${15}
		docker_url=${16:unix://var/run/docker.sock}
		ports=${17}
		state=${18:#present|running|stopped|absent|killed|restarted}
		command=${19}
		dns=${20}
		volumes_from=${21}
		expose=${22}
		privileged=${23:false}

snippet s3
	s3: >
		bucket=${1:# REQUIRED}
		mode=${2:# REQUIRED}
		aws_secret_key=${3}
		src=${4}
		aws_access_key=${5}
		expiration=${6:600}
		dest=${7}
		object=${8}
		s3_url=${9}
		overwrite=${10:true}
		metadata=${11}

snippet digital_ocean_domain
	digital_ocean_domain: >
		state=${1:#present|active|absent|deleted}
		name=${2}
		client_id=${3}
		ip=${4}
		api_key=${5}
		id=${6}

snippet ec2_snapshot
	ec2_snapshot: >
		aws_secret_key=${1}
		profile=${2}
		aws_access_key=${3}
		description=${4}
		security_token=${5}
		snapshot_tags=${6}
		region=${7}
		ec2_url=${8}
		device_name=${9}
		instance_id=${10}
		volume_id=${11}
		validate_certs=${12:#yes|no}

snippet rds_param_group
	rds_param_group: >
		name=${1:# REQUIRED}
		region=${2:# REQUIRED}
		state=${3:#present|absent}
		engine=${4:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		aws_secret_key=${5}
		aws_access_key=${6}
		immediate=${7}
		params=${8:#mysql5.1|mysql5.5|mysql5.6|oracle-ee-11.2|oracle-se-11.2|oracle-se1-11.2|postgres9.3|sqlserver-ee-10.5|sqlserver-ee-11.0|sqlserver-ex-10.5|sqlserver-ex-11.0|sqlserver-se-10.5|sqlserver-se-11.0|sqlserver-web-10.5|sqlserver-web-11.0}
		description=${9}

snippet gce_pd
	gce_pd: >
		name=${1:# REQUIRED}
		size_gb=${2:10}
		zone=${3:us-central1-b}
		service_account_email=${4}
		image=${5}
		pem_file=${6}
		instance_name=${7}
		state=${8:#active|present|absent|deleted}
		snapshot=${9}
		detach_only=${10:#yes|no}
		project_id=${11}
		mode=${12:#READ_WRITE|READ_ONLY}

snippet gce
	gce: >
		zone=${1:us-central1-a}
		name=${2}
		tags=${3}
		service_account_email=${4}
		image=${5:debian-7}
		disks=${6}
		metadata=${7}
		persistent_boot_disk=${8:false}
		pem_file=${9}
		state=${10:#active|present|absent|deleted}
		machine_type=${11:n1-standard-1}
		project_id=${12}
		instance_names=${13}
		network=${14:default}

snippet rax
	rax: >
		files=${1}
		username=${2}
		tenant_name=${3}
		auto_increment=${4:#yes|no}
		image=${5}
		count_offset=${6:1}
		instance_ids=${7}
		user_data=${8}
		verify_ssl=${9}
		wait_timeout=${10:300}
		tenant_id=${11}
		credentials=${12}
		region=${13:dfw}
		flavor=${14}
		networks=${15:['public', 'private']}
		wait=${16:#yes|no}
		count=${17:1}
		group=${18}
		name=${19}
		identity_type=${20:rackspace}
		extra_client_args=${21}
		exact_count=${22:#yes|no}
		disk_config=${23:#auto|manual}
		auth_endpoint=${24:https://identity.api.rackspacecloud.com/v2.0/}
		state=${25:#present|absent}
		meta=${26}
		env=${27}
		key_name=${28}
		api_key=${29}
		extra_create_args=${30}
		config_drive=${31:#yes|no}

snippet ec2_vpc
	ec2_vpc: >
		resource_tags=${1:# REQUIRED}
		cidr_block=${2:# REQUIRED}
		state=${3:present}
		subnets=${4}
		internet_gateway=${5:#yes|no}
		wait_timeout=${6:300}
		dns_hostnames=${7:#yes|no}
		wait=${8:#yes|no}
		aws_secret_key=${9}
		aws_access_key=${10}
		route_tables=${11}
		dns_support=${12:#yes|no}
		region=${13}
		instance_tenancy=${14:#default|dedicated}
		vpc_id=${15}
		validate_certs=${16:#yes|no}

snippet glance_image
	glance_image: >
		login_password=${1:yes}
		login_username=${2:admin}
		name=${3:# REQUIRED}
		login_tenant_name=${4:yes}
		region_name=${5}
		container_format=${6:bare}
		min_ram=${7}
		owner=${8}
		endpoint_type=${9:#publicURL|internalURL}
		auth_url=${10:http://127.0.0.1:35357/v2.0/}
		file=${11}
		min_disk=${12}
		is_public=${13:yes}
		disk_format=${14:qcow2}
		copy_from=${15}
		state=${16:#present|absent}
		timeout=${17:180}

snippet rax_clb
	rax_clb: >
		username=${1}
		protocol=${2:#DNS_TCP|DNS_UDP|FTP|HTTP|HTTPS|IMAPS|IMAPv4|LDAP|LDAPS|MYSQL|POP3|POP3S|SMTP|TCP|TCP_CLIENT_FIRST|UDP|UDP_STREAM|SFTP}
		name=${3}
		algorithm=${4:#RANDOM|LEAST_CONNECTIONS|ROUND_ROBIN|WEIGHTED_LEAST_CONNECTIONS|WEIGHTED_ROUND_ROBIN}
		env=${5}
		region=${6:dfw}
		verify_ssl=${7}
		vip_id=${8}
		state=${9:#present|absent}
		wait_timeout=${10:300}
		meta=${11}
		timeout=${12:30}
		credentials=${13}
		api_key=${14}
		type=${15:#PUBLIC|SERVICENET}
		port=${16:80}
		wait=${17:#yes|no}

snippet rds
	rds: >
		command=${1:#create|replicate|delete|facts|modify|promote|snapshot|restore}
		region=${2:# REQUIRED}
		instance_name=${3:# REQUIRED}
		db_engine=${4:#MySQL|oracle-se1|oracle-se|oracle-ee|sqlserver-ee|sqlserver-se|sqlserver-ex|sqlserver-web|postgres}
		iops=${5}
		backup_window=${6}
		backup_retention=${7}
		port=${8}
		security_groups=${9}
		size=${10}
		aws_secret_key=${11}
		subnet=${12}
		vpc_security_groups=${13}
		upgrade=${14:#yes|no}
		zone=${15}
		source_instance=${16}
		parameter_group=${17}
		multi_zone=${18:#yes|no}
		new_instance_name=${19}
		username=${20}
		db_name=${21}
		license_model=${22:#license-included|bring-your-own-license|general-public-license}
		password=${23}
		apply_immediately=${24:#yes|no}
		wait=${25:#yes|no}
		aws_access_key=${26}
		option_group=${27}
		engine_version=${28}
		instance_type=${29}
		wait_timeout=${30:300}
		snapshot=${31}
		maint_window=${32}

snippet route53
	route53: >
		zone=${1:# REQUIRED}
		record=${2:# REQUIRED}
		command=${3:#get|create|delete}
		type=${4:#A|CNAME|MX|AAAA|TXT|PTR|SRV|SPF|NS}
		aws_secret_key=${5}
		aws_access_key=${6}
		retry_interval=${7:500}
		value=${8}
		ttl=${9:3600 (one hour)}
		overwrite=${10}

snippet ec2_asg
	ec2_asg: >
		name=${1:# REQUIRED}
		state=${2:#present|absent}
		aws_secret_key=${3}
		profile=${4}
		aws_access_key=${5}
		availability_zones=${6}
		security_token=${7}
		tags=${8}
		region=${9}
		min_size=${10}
		desired_capacity=${11}
		vpc_zone_identifier=${12}
		launch_config_name=${13}
		health_check_period=${14:500 seconds}
		ec2_url=${15}
		load_balancers=${16}
		validate_certs=${17:#yes|no}
		max_size=${18}
		health_check_type=${19:#EC2|ELB}

snippet ec2_scaling_policy
	ec2_scaling_policy: >
		name=${1:# REQUIRED}
		asg_name=${2:# REQUIRED}
		state=${3:#present|absent}
		aws_secret_key=${4}
		profile=${5}
		aws_access_key=${6}
		security_token=${7}
		adjustment_type=${8:#ChangeInCapacity|ExactCapacity|PercentChangeInCapacity}
		min_adjustment_step=${9}
		scaling_adjustment=${10}
		cooldown=${11}
		ec2_url=${12}
		validate_certs=${13:#yes|no}

snippet riak
	riak: >
		target_node=${1:riak@127.0.0.1}
		config_dir=${2:/etc/riak}
		wait_for_service=${3:#kv}
		http_conn=${4:127.0.0.1:8098}
		wait_for_ring=${5}
		wait_for_handoffs=${6}
		command=${7:#ping|kv_test|join|plan|commit}
		validate_certs=${8:#yes|no}

snippet mysql_user
	mysql_user: >
		name=${1:# REQUIRED}
		login_port=${2:3306}
		login_user=${3}
		login_host=${4:localhost}
		append_privs=${5:#yes|no}
		host=${6:localhost}
		login_unix_socket=${7}
		state=${8:#present|absent}
		login_password=${9}
		check_implicit_admin=${10:false}
		password=${11}
		priv=${12}

snippet mysql_replication
	mysql_replication: >
		master_ssl_cert=${1}
		master_password=${2}
		login_user=${3}
		login_host=${4}
		login_password=${5}
		master_host=${6}
		master_ssl_ca=${7}
		login_unix_socket=${8}
		master_connect_retry=${9}
		master_user=${10}
		master_port=${11}
		master_log_file=${12}
		master_ssl_cipher=${13}
		relay_log_file=${14}
		master_ssl=${15}
		master_ssl_key=${16}
		master_ssl_capath=${17}
		mode=${18:#getslave|getmaster|changemaster|stopslave|startslave}
		master_log_pos=${19}
		relay_log_pos=${20}

snippet postgresql_user
	postgresql_user: >
		name=${1:# REQUIRED}
		login_password=${2}
		login_user=${3:postgres}
		login_host=${4:localhost}
		expires=${5}
		db=${6}
		port=${7:5432}
		state=${8:#present|absent}
		encrypted=${9:false}
		password=${10}
		role_attr_flags=${11:#[NO]SUPERUSER|[NO]CREATEROLE|[NO]CREATEUSER|[NO]CREATEDB|[NO]INHERIT|[NO]LOGIN|[NO]REPLICATION}
		fail_on_user=${12:#yes|no}
		priv=${13}

snippet postgresql_privs
	postgresql_privs: >
		roles=${1:# REQUIRED}
		database=${2:# REQUIRED}
		objs=${3}
		privs=${4}
		state=${5:#present|absent}
		host=${6}
		login=${7:postgres}
		password=${8}
		type=${9:#table|sequence|function|database|schema|language|tablespace|group}
		port=${10:5432}
		grant_option=${11:#yes|no}
		schema=${12}

snippet redis
	redis: >
		command=${1:#slave|flush|config}
		login_port=${2:6379}
		name=${3}
		flush_mode=${4:#all|db}
		master_host=${5}
		login_host=${6:localhost}
		master_port=${7}
		db=${8}
		value=${9}
		login_password=${10}
		slave_mode=${11:#master|slave}

snippet postgresql_db
	postgresql_db: >
		name=${1:# REQUIRED}
		encoding=${2}
		login_user=${3}
		lc_collate=${4}
		lc_ctype=${5}
		port=${6:5432}
		state=${7:#present|absent}
		template=${8}
		login_password=${9}
		owner=${10}
		login_host=${11:localhost}

snippet mysql_variables
	mysql_variables: >
		variable=${1:# REQUIRED}
		login_unix_socket=${2}
		login_password=${3}
		login_user=${4}
		login_host=${5}
		value=${6}

snippet mongodb_user
	mongodb_user: >
		database=${1:# REQUIRED}
		user=${2:# REQUIRED}
		login_port=${3:27017}
		roles=${4:readwrite}
		login_user=${5}
		login_host=${6:localhost}
		state=${7:#present|absent}
		login_password=${8}
		password=${9}
		replica_set=${10}

snippet mysql_db
	mysql_db: >
		name=${1:# REQUIRED}
		login_port=${2:3306}
		encoding=${3}
		login_user=${4}
		login_host=${5:localhost}
		login_unix_socket=${6}
		state=${7:#present|absent|dump|import}
		login_password=${8}
		collation=${9}
		target=${10}

snippet lineinfile
	lineinfile: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		src=${3}
		force=${4:#yes|no}
		insertbefore=${5:#BOF|*regex*}
		selevel=${6:s0}
		create=${7:#yes|no}
		seuser=${8}
		recurse=${9:#yes|no}
		serole=${10}
		backrefs=${11:#yes|no}
		owner=${12}
		state=${13:#file|link|directory|hard|touch|absent}
		mode=${14}
		insertafter=${15:#EOF|*regex*}
		regexp=${16}
		line=${17}
		backup=${18:#yes|no}
		validate=${19}
		group=${20}
		setype=${21}

snippet get_url
	get_url: >
		url=${1:# REQUIRED}
		dest=${2:# REQUIRED}
		path=${3:[]}
		url_password=${4}
		force=${5:#yes|no}
		use_proxy=${6:#yes|no}
		src=${7}
		selevel=${8:s0}
		seuser=${9}
		recurse=${10:#yes|no}
		setype=${11}
		sha256sum=${12}
		serole=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		mode=${15}
		url_username=${16}
		owner=${17}
		group=${18}
		validate_certs=${19:#yes|no}

snippet ini_file
	ini_file: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		section=${3:# REQUIRED}
		force=${4:#yes|no}
		option=${5}
		state=${6:#file|link|directory|hard|touch|absent}
		selevel=${7:s0}
		owner=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		setype=${13}
		value=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet uri
	uri: >
		path=${1:[]}
		url=${2:# REQUIRED}
		force=${3:#yes|no}
		follow_redirects=${4:#all|safe|none}
		owner=${5}
		HEADER_=${6}
		group=${7}
		serole=${8}
		setype=${9}
		status_code=${10:200}
		return_content=${11:#yes|no}
		method=${12:#GET|POST|PUT|HEAD|DELETE|OPTIONS|PATCH}
		body=${13}
		state=${14:#file|link|directory|hard|touch|absent}
		dest=${15}
		selevel=${16:s0}
		force_basic_auth=${17:#yes|no}
		removes=${18}
		user=${19}
		password=${20}
		src=${21}
		seuser=${22}
		recurse=${23:#yes|no}
		creates=${24}
		mode=${25}
		timeout=${26:30}

snippet replace
	replace: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		regexp=${3:# REQUIRED}
		force=${4:#yes|no}
		state=${5:#file|link|directory|hard|touch|absent}
		selevel=${6:s0}
		replace=${7}
		owner=${8}
		validate=${9}
		src=${10}
		group=${11}
		seuser=${12}
		recurse=${13:#yes|no}
		setype=${14}
		serole=${15}
		mode=${16}
		backup=${17:#yes|no}

snippet assemble
	assemble: >
		dest=${1:# REQUIRED}
		path=${2:[]}
		force=${3:#yes|no}
		remote_src=${4:#True|False}
		selevel=${5:s0}
		state=${6:#file|link|directory|hard|touch|absent}
		owner=${7}
		regexp=${8}
		src=${9}
		group=${10}
		seuser=${11}
		recurse=${12:#yes|no}
		serole=${13}
		delimiter=${14}
		mode=${15}
		backup=${16:#yes|no}
		setype=${17}
## Overview

Building your own Android library enables other developers to take advantage of code that you've written.  You can share existing activities, services, images, drawables, resource strings, and layout files that enable other people to leverage your work such as those documented in the [[must have libraries|Must-Have-Libraries]] guide.  Also, if your code base begins to take longer times to compile and/or run, creating a library also enables you to iterate faster by working on a smaller component.  

If you plan to share only standard Java code, you can distribute them packaged as Java Archive Resources (`.jar`) files.  However, if you intend to include resources such as layouts, drawables, or string resources, or even an additional `AndroidManifest.xml` file, you must create an Android Archive Resource [`.aar` file](http://tools.android.com/tech-docs/new-build-system/aar-format) file instead.  An `.aar` file can include the following types of files:

* /AndroidManifest.xml (mandatory)
* /classes.jar (mandatory)
* /res/ (mandatory)
* /R.txt (mandatory)
* /assets/ (optional)
* /libs/*.jar (optional)
* /jni//*.so (optional)
* /proguard.txt (optional)
* /lint.jar (optional)

### Creating a new Android Library

When you create a new Android project, a new application is always created.  You can use this application to test your library.  After creating the project, go to `New` -> `New Module`:



Select `Android Library`.  There is the option to choose `Java library`, but there is a major difference in that an Android library will include not only the Java classes but the resource files, image files, and Android manifest file normally associated with Android.  



Next, you will be prompted to provide a name and the module name.  The name will simply be used to [label](http://developer.android.com/guide/topics/manifest/manifest-intro.html#iconlabel) the application in the Android Manifest file, while the module name will correspond to the directory to be created:



When you click Next, a directory with the module name will be generated along with other files including a resource and Java folder:



In addition, a `build.gradle` file will be created.  One major difference is that Android applications use the `com.android.application` plugin.  Android libraries will use the `com.android.library` plugin.  This statement at the top signals to the [Android Gradle plug-in](http://developer.android.com/tools/building/plugin-for-gradle.html) to generate an `.aar` file instead of an `.apk` file normally installed on Android devices.

```gradle
// Android library
apply plugin: 'com.android.library'
```

### Compiling a Library

Android applications usually have a build and debug variation.  The `buildTypes` parameter designates the settings for each type of configuration.

```gradle
android {
  buildTypes {
    release {

    } 
    debug {

    }
}
```

You can compile the library with Android Studio, or type `./gradlew build` at the command line.  The output will be stored under the library's subdirectory under `build/outputs/aar`.   Unlike Android applications in which `debug` or `release` versions can be generated, only release versions by default are published as documented [here](http://tools.android.com/tech-docs/new-build-system/user-guide#TOC-Referencing-a-Library).  

If you wish to build multiple variations, you will need to add this statement to your library `build.gradle` file:

```gradle
android {
     publishNonDefault true
}
```

When using this statement, different `.aar` packages are generated for each build type specified.  To reference them once they are published, see [[this section|Building-your-own-Android-library#add-the-gradle-dependency]].

If you wish to reference the library from your demo application within the same Android project, you will need to explicitly specify which library to use with the `configuration` parameter.    You need to add this statement to your `app/build.gradle`:

```gradle
dependencies {
    // colon needs to prefixed with the library path
    debugCompile project(path: ':mylibrary', configuration: 'debug')
    releaseCompile project(path: ':mylibrary', configuration: 'release')
}
```

#### Using with ButterKnife

If you intend use the library with [ButterKnife](https://github.com/JakeWharton/butterknife/issues/45), in the past it did not work with Android libraries and you had to convert your code back to `findViewById` calls.  You should upgrade to at least v8.2.0 and follow this [[section|Reducing-View-Boilerplate-with-Butterknife#using-in-your-own-android-libraries]] to enable your libraries to use it.

### Publishing

To publish your library, you can either make it available to a public or private repository.  jCenter and Maven Central are the most popular ones, though jCenter has become the default one used in Android Studio.  For understanding the differences between jCenter or Maven Central, see this [blog link](http://inthecheesefactory.com/blog/how-to-upload-library-to-jcenter-maven-central-as-dependency/en).   

To publish your library straight from GitHub you can use [JitPack](https://jitpack.io). Once you create a GitHub release JitPack will build your library from source and will publish it automatically. 

#### Setting up through jCenter

First, [signup](https://bintray.com/) for a BinTray account.  You will want to create a GPG signing key:
and go to [Edit Profile](https://bintray.com/profile/edit) to add this private/public key pair.

```bash
gpg --gen-key
```

Find the public key ID generated by finding the 8-digit hex after "pub 2048/XXXXXXXX":

```bash
gpg --list-keys
gpg --keyserver hkp://pool.sks-keyservers.net --send-keys [PUBLIC_KEY_ID]
```

Export your keys.  You will want to copy/paste these sections into the `GPG Signing` section:

```bash
gpg -a --export yourmail@email.com > public_key_sender.asc
gpg -a --export-secret-key yourmail@email.com > private_key_sender.asc
```

Click on the `API Key` section when editing your profile.  You will need to provide your username and API Key by setting it locally in your `gradle.properties` file:

```gradle
bintrayUser=user
bintrayApiKey=key
```

Take a look at the examples provided by BinTray [here](https://github.com/bintray/bintray-examples/tree/master/gradle-bintray-plugin-examples).  In particular, you should follow the `android-maven-example`.

Next, edit your root `build.gradle` file.  Add the `android-maven-gradle-plugin`, which will be used to generate the Maven-compatible archive to be shared, as well as the JFrog plugin:

```gradle
buildscript {
   repositories {
      jcenter()
   }
   dependencies {
     // used to generate a POM file
     classpath 'com.github.dcendents:android-maven-gradle-plugin:1.5'
   }
}
 
// Plugin used to upload authenticated files to BinTray through Gradle
plugins {
   id "com.jfrog.bintray" version "1.7.3"
}
```

Inside your `library/build.gradle` file, you will want to apply the Android Maven Gradle and JFrog plugin:

```gradle
apply plugin: 'com.android.library'
apply plugin: 'com.github.dcendents.android-maven'
apply plugin: 'com.jfrog.bintray'
```

Next, you will need to define constants that will be used to generate the XML files used by Maven to understand information about the package. Gradle compile statements are usually follow the form of `GROUP_ID:ARTIFACT_ID:VERSION`, such as 'com.squareup.picasso:picasso:2.5.2', so we should always to make sure these values are set.

```gradle
// If your directory matches the name, you do not need to set archivesBaseName.
archivesBaseName = "android-oauth-handler"

install {
    repositories.mavenInstaller {
        pom.project {
            groupId "com.codepath.libraries"
            artifactId "android-oauth-handler"
            version "1.0.0"
        }
    }
}
```

The remaining section should be added for authenticating uploads to BinTray.  Note that the `configurations` option alerts the plugin to upload the final packages generated. 

```gradle
bintray {	
	user = project.hasProperty('bintrayUser') ? project.property('bintrayUser') : System.getenv('BINTRAY_USER')
	key = project.hasProperty('bintrayApiKey') ? project.property('bintrayApiKey') : System.getenv('BINTRAY_API_KEY')
        // jFrog plugin must be declared for this line to work
	configurations = ['archives']
        // Package info for BinTray
	pkg {
		repo = 'maven'
		name = 'android-oauth-handler'
		userOrg = user
		licenses = ['Apache-2.0']
		vcsUrl = 'https://github.com/bintray/gradle-bintray-plugin.git'
		version {
			name = '0.1'
			desc = 'Gradle Bintray Plugin 1.0 final'
			vcsTag = '0.1'
		}	
	}
}
```

If you want to test to see if the package works locally, type:

```bash
./gradlew install
```

The package will be installed in your ~/.m2/repository.  If you wish to try the library out, you can add this private Maven repository to the root `build.gradle` config of the application that will be using te 

```gradle
allprojects {

    repositories {
        // add first
        maven { url "${System.env.HOME}/.m2/repository" }
        jcenter()
    }
```

To upload your package, just type:

```bash
# Set your Bintray user ID below
export BINTRAY_USER="codepath"
# Set your Bintray API key below
export BINTRAY_API_KEY="YOUR_BINTRAY_API_KEY_HERE"
./gradlew bintrayUpload 
```

#### Setting up a private Amazon S3 Maven repository

Another approach is to setup a private Maven repository, which also be done through Amazon's Web Services (AWS) and the Simple Storage Service [(S3)](https://aws.amazon.com/s3/).  Gradle supports the ability to access private S3 repositories with a secret access key and ID used to authenticate with Amazon:

#### Adding the private Maven repository 

To add the S3 repository to the list, you will need to add the credentials to access the S3 bucket to your root `build.gradle` file:

```gradle
allprojects {
    repositories {
        jcenter()

        maven {
            url "s3://yourmavenrepo-bucket/android/snapshots"
            credentials(AwsCredentials) {
                accessKey AWS_ACCESS_KEY
                secretKey AWS_SECRET_KEY
            }
        }
    }
}
```

Instead of adding the keys directly, it is recommended that you add it to your `local.properties` to your local machine:

```gradle
AWS_ACCESS_KEY=
AWS_SECRET_KEY=
```

In order to publish the plugin, we need to create a separate Gradle file that can be use in our library configuration.  Create a file called `gradle/gradle-mvn-push.gradle`, which will apply the Gradle-Maven plugin and specify the location of the S3 bucket when using the `./gradlew publish` command:

```gradle
// Inspired from https://gist.github.com/adrianbk/c4982e5ebacc6b6ed902

apply plugin: 'maven-publish'

def isReleaseBuild() {
    return VERSION_NAME.contains("SNAPSHOT") == false
}

def getOutputDir() {
    if (isReleaseBuild()) {
        return "${project.buildDir}/releases"
    } else {
        return "${project.buildDir}/snapshots"
    }
}

def getDestUrl() {
    if (isReleaseBuild()) {
        return "s3://my-bucket/releases"
    } else {
        return "s3://my-bucket/snapshots"
    }
}

def getArtifactFilePath() {
    if (isReleaseBuild()) {
        return "$buildDir/outputs/aar/${POM_ARTIFACT_ID}-release.aar"
    } else {
        return "$buildDir/outputs/aar/${POM_ARTIFACT_ID}-debug.aar"
    }
}

publishing {
    publications {
        myPublication (MavenPublication) {
            groupId GROUP
            artifactId POM_ARTIFACT_ID
            version VERSION_NAME
            artifact getArtifactFilePath()
        }
   }
   repositories {
      maven {
        url getDestUrl()
        credentials(AwsCredentials) {
         accessKey = "key"
         secretKey = "password"
        }
      }
   }
}

```

We will then apply the statements in this file by applying it within the library's Gradle file (i.e. `mylibrary/build.gradle`):

```gradle
apply from: rootProject.file('gradle/gradle-mvn-push.gradle')
```

Edit the root project's `gradle.properties` too:

```gradle
VERSION_NAME=0.4-SNAPSHOT
VERSION_CODE=0.4
GROUP=com.codepath
```

Finally, we need to setup the metadata necessary for publishing.  Edit the library's `gradle.properties` and set the values:

```gradle
POM_NAME=My Library
POM_ARTIFACT_ID=library
POM_PACKAGING=aar
```

#### Support for Amazon IAM Roles

Currently Gradle's Amazon S3 integration only supports access keys and does not support [Identity Access Management (IAM)](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html) roles.  There is an existing backlog as reported in this [ticket](https://discuss.gradle.org/t/sts-iam-role-credentials-for-s3-maven-repository/14010) but currently it is not officially supported.

To take advantage of a client that does, you can output the repository to a local file and use the AWS command-line S3 client to copy the snapshot dirs:  

```gradle

uploadArchives {
    repositories {
        mavenDeployer {
          repository(url: "file:///" + getOutputDir())
        }
    }
}

task copyToS3(type: Exec) {
    commandLine 'aws', 's3', 'cp', '--recursive', getOutputDir(), getDestUrl()
}

copyToS3.dependsOn uploadArchives
```

To publish and execute the task to copy the build to S3, the command to enter is `./gradlew copyToS3`.

#### Add the Gradle dependency

Once the private S3 repository has been added to the list, you can simply add this line to the Gradle dependency list.  The Android Gradle plug-in will search through all possible repositories searching for a match.  Add this line to your `app/build.gradle` file:
 
```gradle
dependencies {
  implementation 'com.codepath:mylibrary:0.4-SNAPSHOT'
}
```

If you published multiple versions of your package as described [[here|Building-your-own-Android-library#building-different-versions]], you will need to specify the build type (i.e. `release` or `debug`).  Gradle may assume that if you specify the build type to search for a `.jar` file, so you must also specify the `@aar`.  Using this syntax doesn't follow the dependencies included in the package, so `transitive=true` must also be included as described in this [Google discussion](https://groups.google.com/forum/#!msg/adt-dev/Ll2JcCfgBsQ/eHjJ8EcZI5MJ).

```gradle
dependencies {
  releaseCompile('com.codepath:mylibrary:0.4:release@aar') {
    transitive=true
  }
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
  }
}
```

#### Prevent caching

If you are making constant changes to your snapshot versions and wish to have the latest updates pulled each time, you can mark the dependency as a **changing module** in Gradle:

```gradle
dependencies {
  debugCompile('com.codepath:mylibrary:0.4-SNAPSHOT:debug@aar') {
    transitive=true
    changing=true
  }
}
```
  
Gradle will normally cache the module for 24 hours for those marked as changing, but you can lower this setting:

```gradle
configurations.all {
    // check for updates every build
    resolutionStrategy.cacheChangingModulesFor 0, 'seconds'
}
```

#### Issues with JDK 8u60 

If you are trying to access a private Amazon S3 repository, you may see an `AWS authentication requires a valid Date or x-amz-date header` error.  It is a known issue with [Gradle](https://issues.gradle.org/browse/GRADLE-3338) and Java versions.  

To fix this issue, you will need to upgrade to Gradle v2.8 by editing your `gradle/wrapper.properties`:
```gradle
distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-all.zip
```

Even though the default Gradle version used in Android projects is 2.4, the build should compile without needing to make any changes.

### Using with ProGuard

If you intend to export your release, you should also include any configurations in case ProGuard is applied to your library.  If you specify `consumerProguardFiles` in your library config, the ProGuard rules will be added  during the compilation.  

```gradle
android {
   defaultConfig {
      minifyEnabled true
      consumerProguardFiles 'consumer-proguard-rules.pro'
   }
}
```

Make sure to create a `consumer-proguard-rules.pro` file.  See [[Configuring ProGuard]] for more details.

If you use the default configuration, ProGuard will obfuscate and alter the name of your library classes, making it impossible for Android projects to reference them.  The most basic example of ensuring your library classes are exported is shown below:

```
-dontobfuscate
# See https://speakerdeck.com/chalup/proguard 
-optimizations !code/allocation/variable

-keep public class * {
    public protected *;
}
```

See the [ProGuard documentation](http://proguard.sourceforge.net/manual/usage.html) for more information about the syntax of this file.  See [this example](https://stuff.mit.edu/afs/sipb/project/android/sdk/android-sdk-linux/tools/proguard/docs/index.html#manual/examples.html) of an Android library definition.

### Resource Merging

If your Android library defines an `AndroidManifest.xml` or any other resource files (i.e. `strings.xml`, `colors.xml`), these resource will be automatically merged with your application.  In this way, you do not have to redeclare permissions that are needed in your library in your main application.   However, if your library declares color styles that may conflict with the appearance in your main application, you may need to rename these styles.

If you do wish to understand how the final `AndroidManifest.xml` is generated, you can decode the final `.apk` file using a third-party tool called [apktool](http://ibotpeaches.github.io/Apktool/).  Instructions to install are located [here](http://ibotpeaches.github.io/Apktool/install/).  If you are upgrading the `apktool` version, you may need to delete the `$HOME/apktool/framework/1.apk` file.  

Once you have the tool installed, you simply need to type this line:

```bash
apktool decode <.apk file>
```

The tool should decode your `.apk` file and allow you to better understand how the final resource files are generated.  

### References

* 
* 
* 
* 
resources:
  - name: pipes_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Qhode/pipes"
      branch: master

  - name: prov_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Qhode/provision"
      branch: master

  # Terraform State for Kermit
  - name: kermit_saas_state
    type: state

  # Terraform State for ship-bits NAT
  - name: shipbits_ami_state
    type: state

  # PEM key for AWS RC account
  - name: kermit_aws_pem
    type: integration
    integration: aws-rc-pem

  # PEM key for AWS ship-bits account
  - name: shipbits_aws_pem
    type: integration
    integration: ship-bits-pem

  # CREDS for AWS RC account
  - name: kermit_aws_key
    type: integration
    integration: aws_rc_access

  # CREDS for AWS ship-bits account
  - name: shipbits_aws_key
    type: integration
    integration: aws_bits_access

  # CREDS for RT instance
  - name: rt_creds
    type: integration
    integration: rt_key

  # CREDS for github
  - name: github_creds
    type: integration
    integration: qhode_gh

  # CREDS for aws
  - name: aws_ship_bits_creds
    type: integration
    integration: aws_bits_access

  - name: kermit_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit"
      branch: master

  - name: installer_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/ribbit"
      branch: master

  - name: knodeInit_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit-nodeInit"
      branch: master

  - name: kreqProc_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit-reqProc"
      branch: master

  - name: kreqKick_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit-reqKick"
      branch: master

  - name: kexecTemplates_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit-execTemplates"
      branch: master

  - name: kdb_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/postgres"
      branch: master

  - name: kvault_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/vault"
      branch: master

  - name: kmsg_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/rabbitmq"
      branch: master

  - name: kredis_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/redis"

  - name: u16node_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u16node"
      branch: master

  - name: u18node_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u18node"
      branch: master

  - name: c7node_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/c7node"
      branch: master

  - name: c7go_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/c7go"
      branch: master

  - name: c7cpp_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/c7cpp"
      branch: master

  - name: c7java_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/c7java"
      branch: master

  - name: u16go_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u16go"
      branch: master

  - name: u16cpp_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u16cpp"
      branch: master

  - name: u16java_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u16java"
      branch: master

  - name: u18go_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u18go"
      branch: master

  - name: u18cpp_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u18cpp"
      branch: master

  - name: u18java_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u18java"
      branch: master

  - name: kdb_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-postgres"
      versionName: master

  - name: kvault_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-vault"
      versionName: master

  - name: kmsg_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-msg"
      versionName: master

  - name: kredis_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-redis"
      versionName: master

  - name: kmicro_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-micro"
      versionName: master

  - name: kscriptsbase_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-scriptsbase"
      versionName: master

  - name: kapi_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-api"
      versionName: master

  - name: kwww_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-www"
      versionName: master

  - name: installer_img
    type: image
    versionTemplate:
      sourceName: "jfrog/pipelines-installer"
      versionName: master

  - name: kreqProc_img
    type: image
    integration: qhodeDH
    versionTemplate:
      sourceName: "drydock/kermit-u16reqproc"
      versionName: master

  - name: kermit_bits_cli
    type: cliConfig
    integration: aws_bits_access
    pointer:
      region: us-east-1

  - name: drydock_cli
    type: cliConfig
    integration: qhodeDH

  - name: aws_x8664_u16_img_params
    type: params
    version:
      params:
        SOURCE_AMI: "ami-66506c1c"
        VPC_ID: "vpc-266f3241"
        SUBNET_ID: "subnet-6df12f24"
        SECURITY_GROUP_ID: "sg-f634518c"
        REGION: "us-east-1"

  - name: distrobase_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/distrobase"
      branch: master

  - name: u16microbase_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "dry-dock/u16microbase"
      branch: master

  - name: distrobase_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-distrobase"
      versionName: master

  - name: u16microbase_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u16microbase"
      versionName: master

  - name: u18java_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u18java"
      versionName: master

  - name: u18go_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u18go"
      versionName: master

  - name: u18cpp_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u18cpp"
      versionName: master

  - name: u18node_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u18node"
      versionName: master

  - name: u16java_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u16java"
      versionName: master

  - name: u16go_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u16go"
      versionName: master

  - name: u16cpp_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u16cpp"
      versionName: master

  - name: u16node_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-u16node"
      versionName: master

  - name: c7java_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-c7java"
      versionName: master

  - name: c7go_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-c7go"
      versionName: master

  - name: c7cpp_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-c7cpp"
      versionName: master

  - name: c7node_x8664_img
    type: image
    versionTemplate:
      sourceName: "pipelines-docker.jfrog.io/jfrog/pipelines-c7node"
      versionName: master

  - name: reqExec_repo
    type: gitRepo
    integration: "qhode_gh"
    pointer:
      sourceName: "Shippable/kermit-reqExec"
      branch: master

jobs:
  - name: kermit_prov
    type: runSh
    steps:
      - IN: kermit_aws_pem
        switch: off
      - IN: kermit_aws_key
        switch: off
      - IN: kermit_saas_state
        switch: off
      - IN: prov_repo
        switch: off
      - TASK:
          script:
            - pushd $(shipctl get_resource_state "prov_repo")
            - ./provision.sh kermit saas rc-us-east-1
      - OUT: kermit_saas_state
    on_success:
      script:
        - echo "SUCCESS"
    on_failure:
      - script: echo 'FAILURE!'
    always:
      script:
        - ./archiveProvisionState.sh kermit saas
        - popd

  - name: shipbits_ami_prov
    type: runSh
    steps:
      - IN: shipbits_aws_pem
        switch: off
      - IN: shipbits_aws_key
        switch: off
      - IN: shipbits_ami_state
        switch: off
      - IN: prov_repo
        switch: off
      - TASK:
          script:
            - pushd $(shipctl get_resource_state "prov_repo")
            - ./provision.sh shipbits ami ship-bits
      - OUT: shipbits_ami_state
    on_success:
      script:
        - echo "SUCCESS"
    on_failure:
      - script: echo 'FAILURE!'
    always:
      script:
        - ./archiveProvisionState.sh shipbits ami
        - popd

  - name: kdb_build
    type: runSh
    steps:
      - IN: kdb_repo
      - IN: rt_creds
      - TASK:
          name: kdb_build
          runtime:
            options:
              env:
                - IMG_OUT: "kdb_img"
                - RES_REPO: "kdb_repo"
                - IMG: "postgres"
                - REL_VER: "master"
          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - docker build -t=$IMG_NAME:$REL_VER .
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kdb_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kvault_build
    type: runSh
    steps:
      - IN: kvault_repo
      - IN: rt_creds
      - TASK:
          name: kvault_build
          runtime:
            options:
              env:
                - IMG_OUT: "kvault_img"
                - RES_REPO: "kvault_repo"
                - IMG: "vault"
                - REL_VER: "master"
          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - docker build -t=$IMG_NAME:$REL_VER .
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd

      - OUT: kvault_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kmsg_build
    type: runSh
    steps:
      - IN: kmsg_repo
      - IN: rt_creds
      - TASK:
          name: kmsg_build
          runtime:
            options:
              env:
                - IMG_OUT: "kmsg_img"
                - RES_REPO: "kmsg_repo"
                - IMG: "msg"
                - REL_VER: "master"
          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - docker build -t=$IMG_NAME:$REL_VER .
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kmsg_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kredis_build
    type: runSh
    steps:
      - IN: kredis_repo
      - IN: rt_creds
      - TASK:
          name: kredis_build
          runtime:
            options:
              env:
                - IMG_OUT: "kredis_img"
                - RES_REPO: "kredis_repo"
                - IMG: "redis"
                - REL_VER: "master"
          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - docker build -t=$IMG_NAME:$REL_VER .
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd

      - OUT: kredis_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kmicro_build
    type: runSh
    steps:
      - IN: kermit_repo
      - IN: rt_creds
        switch: off
      - TASK:
          name: micro_build
          runtime:
            options:
              env:
                - IMG_OUT: "kmicro_img"
                - RES_REPO: "kermit_repo"
                - IMG: "kmicro"
                - REL_VER: "master"

          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - echo "Building $IMG_NAME:$REL_VER"
            - docker build -t=$IMG_NAME:$REL_VER .
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kmicro_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kscriptsbase_build
    type: runSh
    steps:
      - IN: kexecTemplates_repo
      - IN: kermit_repo
      - IN: rt_creds
        switch: off
      - TASK:
          name: scriptsbase_build
          runtime:
            options:
              env:
                - IMG_OUT: "kscriptsbase_img"
                - EXEC_TEMPLATES_REPO: "kexecTemplates_repo"
                - MICRO_REPO: "kermit_repo"
                - IMG: "kscriptsbase"
                - BASE_IMAGE: "drydock/u18node"
                - REL_VER: "master"
          script:
            - pushd $(shipctl get_resource_state "$EXEC_TEMPLATES_REPO")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - EXECREPO_COMMIT=$(shipctl get_resource_version_key "$EXEC_TEMPLATES_REPO" "shaData.commitSha")
            - MICROREPO_COMMIT=$(shipctl get_resource_version_key "$MICRO_REPO" "shaData.commitSha")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - export BASE_IMAGE="$BASE_IMAGE"
            - export BASE_TAG="$REL_VER"
            - export EXEC_TEMPLATES_PATH=$(realpath --relative-to="../.." $(shipctl get_resource_state "$EXEC_TEMPLATES_REPO"))
            - export MICRO_PATH=$(realpath --relative-to="../.." $(shipctl get_resource_state "$MICRO_REPO"))
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - shipctl replace Dockerfile
            - echo "Building $IMG_NAME:$REL_VER"
            - docker build -t=$IMG_NAME:$REL_VER -f Dockerfile ./../..
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kscriptsbase_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "microCommitSha=$MICROREPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "microCommitSha=$MICROREPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL


  - name: kapi_build
    type: runSh
    steps:
      - IN: kermit_repo
      - IN: knodeInit_repo
      - IN: kreqKick_repo
      - IN: kexecTemplates_repo
      - IN: rt_creds
        switch: off
      - TASK:
          name: kapi_build
          runtime:
            options:
              env:
                - NODEINIT_REPO: "knodeInit_repo"
                - REQKICK_REPO: "kreqKick_repo"
                - EXEC_TEMPLATES_REPO: "kexecTemplates_repo"
                - IMG_OUT: "kapi_img"
                - RES_REPO: "kermit_repo"
                - IMG: "kapi"
                - REL_VER: "master"

          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")/nod/api
            - echo $(shipctl get_resource_state "$NODEINIT_REPO")
            - ls -atlh $(shipctl get_resource_state "$NODEINIT_REPO")
            - tar -czf node.tar.gz -C $(shipctl get_resource_state "$NODEINIT_REPO") .
            - zip -r node.zip $(shipctl get_resource_state "$NODEINIT_REPO")
            - tar -czf reqKick.tar.gz -C $(shipctl get_resource_state "$REQKICK_REPO") .
            - zip -r reqKick.zip $(shipctl get_resource_state "$REQKICK_REPO")
            - tar -czf execTemplates.tar.gz -C $(shipctl get_resource_state "$EXEC_TEMPLATES_REPO") .
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - echo "Building $IMG_NAME:$REL_VER"
            - docker build --no-cache -t=$IMG_NAME:$REL_VER .
            - echo "Pushing $IMG_NAME;$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kapi_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kwww_build
    type: runSh
    steps:
      - IN: kermit_repo
      - IN: rt_creds
        switch: off
      - TASK:
          name: kwww_build
          runtime:
            options:
              env:
                - IMG_OUT: "kwww_img"
                - RES_REPO: "kermit_repo"
                - IMG: "kwww"
                - REL_VER: "master"

          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")/nod/www
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - echo "Building $IMG_NAME:$REL_VER"
            - docker build -t=$IMG_NAME:$REL_VER .
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kwww_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: installer_build
    type: runSh
    steps:
      - IN: installer_repo
      - IN: rt_creds
        switch: off
      - TASK:
          name: installer_build
          runtime:
            options:
              env:
                - IMG_OUT: "installer_img"
                - RES_REPO: "installer_repo"
                - IMG: "installer"
                - REL_VER: "master"

          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - IMG_NAME="$RT_REGISTRY/$IMG_NAME"
            - echo "Building $IMG_NAME:$REL_VER"
            - docker build -t=$IMG_NAME:$REL_VER -f build/docker/Dockerfile.Ubuntu .
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - echo "----------- Pushing image to installer repo ---------"
            - jfrog rt docker-push $IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - echo "----------- Gathering build info --------------------"
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
            - echo "----------- Pushing tarfile to Artifactory ----------"
            - RT_BUCKET=$(shipctl get_integration_resource_field "rt_creds" "BUCKET")
            - TAR_FILENAME="pipelines-$REL_VER.tar.gz"
            - INSTALLER_REPO_DIR=$(shipctl get_resource_state "$RES_REPO")
            - echo "----------- Removing metadata from package ----------"
            - pushd $INSTALLER_REPO_DIR
            - rm -rf .git
            - rm -rf .gitignore
            - popd
            - tar -zcf "$TAR_FILENAME" -C "$INSTALLER_REPO_DIR" .
            - jfrog rt u "$TAR_FILENAME" "$RT_BUCKET" --build-name="$JOB_NAME" --build-number="$BUILD_NUMBER"
      - OUT: installer_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: kermit_deploy
    type: runSh
    dependencyMode: strict
    runtime:
      timeoutMinutes: 15
    steps:
      - IN: installer_img
      - IN: kwww_img
      - IN: kapi_img
      - IN: kscriptsbase_img
      - IN: kmicro_img
      - IN: rt_creds
        switch: off
      - IN: pipes_repo
        switch: off
      - IN: kermit_aws_pem
        switch: off
      - IN: kermit_saas_state
        switch: off
      - TASK:
          runtime:
            options:
              env:
                - RES_PEM: "kermit_aws_pem"
                - BASTION_USER: "ec2-user"
                - ONEBOX_USER: "centos"
                - DEPLOY_VERSION: "master"
          name: deploy_to_ob
          script:
            - export BASTION_IP=$(shipctl get_resource_version_key kermit_saas_state nat_pub_ip)
            - export ONEBOX_IP=$(shipctl get_resource_version_key kermit_saas_state onebox_priv_ip)
            - export INSTALLER_IMG=$(shipctl get_resource_version_key installer_img IMG_NAME)
            - export KWWW_IMG=$(shipctl get_resource_version_key kwww_img IMG_NAME)
            - export KAPI_IMG=$(shipctl get_resource_version_key kapi_img IMG_NAME)
            - export KMICRO_IMG=$(shipctl get_resource_version_key kmicro_img IMG_NAME)
            - export KSCRIPTSBASE_IMG=$(shipctl get_resource_version_key kscriptsbase_img IMG_NAME)
            - export RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - export RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - export RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - pushd $(shipctl get_resource_state "pipes_repo")
            - ./deployKermit.sh
            - popd

  - name: kreqProc_x8664_u16_build
    type: runSh
    steps:
      - IN: kreqProc_repo
      - IN: kexecTemplates_repo
      - IN: drydock_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: kreqProc_build
          runtime:
            options:
              env:
                - IMG_OUT: "kreqProc_img"
                - RES_REPO: "kreqProc_repo"
                - IMG: "kreqProc"
                - REL_VER: "master"
                - OS: Ubuntu_16.04
                - ARCH: x86_64

          script:
            - pushd $(shipctl get_resource_state "$RES_REPO")
            #            - npm install -g jshint@v2.9.7
            #            - pushd nod
            #            - jshint .
            #            - popd
            - IMG_NAME=$(shipctl get_resource_version_key "$IMG_OUT" "sourceName")
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - sed -i "s/{{%TAG%}}/$REL_VER/g" ./image/$ARCH/$OS/Dockerfile
            - docker build --pull --no-cache -t=$IMG_NAME:$REL_VER -f image/$ARCH/$OS/Dockerfile .
            - docker push $IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-u16reqproc"
            - docker tag $IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: kreqProc_img
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

### Build AWS AMIs

  - name: base_aws_x8664_u18
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: rt_creds
        switch: off
      - IN: shipbits_aws_pem
        switch: off
      - IN: knodeInit_repo
      - IN: pipes_repo
        switch: off
      - IN: u18java_x8664_img
      - IN: u18node_x8664_img
      - IN: u18cpp_x8664_img
      - IN: u18go_x8664_img
      - TASK:
          name: base_aws_x8664_u18
          runtime:
            options:
              env:
                - OS: "Ubuntu_18.04"
                - ARCHITECTURE: x86_64
                - SOURCE_AMI: "ami-07d0cf3af28718ef8"
                - VPC_ID: "vpc-01e8b399ee4f9f356"
                - SUBNET_ID: "subnet-06de95b9fc0434531"
                - REGION: "us-east-1"
                - SYSTEM_RUNTIME_LANGUAGE_VERSION: "master"
                - RUNTIME_VERSION: "master"
                - DOCKER_IMAGE_REGISTRY_URL: "pipelines-docker.jfrog.io"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "base"
                - DOCKER_VERSION: "18.09"
                - SSH_USERNAME: "ubuntu"
                - SSH_BASTION_HOST: "3.227.114.92"
                - SSH_BASTION_USERNAME: "ec2-user"
                - SSH_BASTION_PRIVATE_KEY_PATH: "/tmp/ship-bits.pem"
                - PEM_KEY_RES: "shipbits_aws_pem"
          script:
            - export RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - export RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - export RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - export SOURCE_REPOSITORY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - ./addPemKey.sh
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - export IMAGE_NAME=$(cat output.txt | awk -F, '$0 ~/artifact,0,id/ {print $6}' | cut -d':' -f 2)
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME" "SYSTEM_RUNTIME_LANGUAGE_VERSION=$SYSTEM_RUNTIME_LANGUAGE_VERSION"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: patch_aws_x8664_u18
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: pipes_repo
        switch: off
      - IN: base_aws_x8664_u18
      - IN: u18repLib_x8664_build
      - IN: u18_reqExec_x8664_pack
      - IN: kreqKick_repo
      - IN: knodeInit_repo
      - IN: kexecTemplates_repo
      - TASK:
          name: patch_aws_x8664_u18
          runtime:
            options:
              env:
                - OS: "Ubuntu_18.04"
                - ARCHITECTURE: x86_64
                - VPC_ID: "vpc-266f3241"
                - SUBNET_ID: "subnet-6df12f24"
                - SECURITY_GROUP_ID: "sg-f634518c"
                - REGION: "us-east-1"
                - RUNTIME_VERSION: "master"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "patch"
                - DOCKER_VERSION: "18.09"
                - BASE_IMG_RES: "base_aws_x8664_u18"
                - SSH_USERNAME: "ubuntu"
                # add comma separated regions where the AMI has to be copied to
                - AMI_REGIONS: "ap-south-1"
          script:
            - export SOURCE_AMI=$(shipctl get_resource_version_key "$BASE_IMG_RES" "IMAGE_NAME")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - export AWS_POLL_DELAY_SECONDS=10
            - export AWS_MAX_ATTEMPTS=600
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - cat manifest.json | jq .
            - |
              export amis=$(cat manifest.json | jq -r '.builds[0].artifact_id | split(",")')
              export ami_count=$(echo $amis | jq '. | length')
              for i in $(seq 1 $ami_count); do
                ami=$(echo $amis | jq '.['"$i-1"']')
                ami_region=$(echo $ami | jq -r 'split(":") | .[0]')
                ami_id=$(echo $ami | jq -r 'split(":") | .[1]')
                if [ "$ami_region" == "$REGION" ]; then
                  export IMAGE_NAME="$ami_id"
                fi
                shipctl put_resource_state_multi $JOB_NAME "$ami_region=$ami_id"
              done
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: base_aws_x8664_u16
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: rt_creds
        switch: off
      - IN: knodeInit_repo
      - IN: pipes_repo
        switch: off
      - IN: shipbits_aws_pem
        switch: off
      - IN: u16java_x8664_img
      - IN: u16node_x8664_img
      - IN: u16cpp_x8664_img
      - IN: u16go_x8664_img
      - TASK:
          name: base_aws_x8664_u16
          runtime:
            options:
              env:
                - OS: "Ubuntu_16.04"
                - ARCHITECTURE: x86_64
                - SOURCE_AMI: "ami-0cfee17793b08a293"
                - VPC_ID: "vpc-01e8b399ee4f9f356"
                - SUBNET_ID: "subnet-06de95b9fc0434531"
                - REGION: "us-east-1"
                - SYSTEM_RUNTIME_LANGUAGE_VERSION: "master"
                - RUNTIME_VERSION: "master"
                - DOCKER_IMAGE_REGISTRY_URL: "pipelines-docker.jfrog.io"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "base"
                - DOCKER_VERSION: "18.09"
                - SSH_USERNAME: "ubuntu"
                - SSH_BASTION_HOST: "3.227.114.92"
                - SSH_BASTION_USERNAME: "ec2-user"
                - SSH_BASTION_PRIVATE_KEY_PATH: "/tmp/ship-bits.pem"
                - PEM_KEY_RES: "shipbits_aws_pem"
          script:
            - export RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - export RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - export RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - export SOURCE_REPOSITORY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - ./addPemKey.sh
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - export IMAGE_NAME=$(cat output.txt | awk -F, '$0 ~/artifact,0,id/ {print $6}' | cut -d':' -f 2)
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME" "SYSTEM_RUNTIME_LANGUAGE_VERSION=$SYSTEM_RUNTIME_LANGUAGE_VERSION"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: patch_aws_x8664_u16
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: pipes_repo
        switch: off
      - IN: base_aws_x8664_u16
      - IN: u16repLib_x8664_build
      - IN: u16_reqExec_x8664_pack
      - IN: kreqKick_repo
      - IN: knodeInit_repo
      - IN: kexecTemplates_repo
      - TASK:
          name: patch_aws_x8664_u16
          runtime:
            options:
              env:
                - OS: "Ubuntu_16.04"
                - ARCHITECTURE: x86_64
                - VPC_ID: "vpc-266f3241"
                - SUBNET_ID: "subnet-6df12f24"
                - SECURITY_GROUP_ID: "sg-f634518c"
                - REGION: "us-east-1"
                - RUNTIME_VERSION: "master"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "patch"
                - DOCKER_VERSION: "18.09"
                - BASE_IMG_RES: "base_aws_x8664_u16"
                - SSH_USERNAME: "ubuntu"
                # add comma separated regions where the AMI has to be copied to
                - AMI_REGIONS: "ap-south-1"
          script:
            - export SOURCE_AMI=$(shipctl get_resource_version_key "$BASE_IMG_RES" "IMAGE_NAME")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - export AWS_POLL_DELAY_SECONDS=10
            - export AWS_MAX_ATTEMPTS=600
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - cat manifest.json | jq .
            - |
              export amis=$(cat manifest.json | jq -r '.builds[0].artifact_id | split(",")')
              export ami_count=$(echo $amis | jq '. | length')
              for i in $(seq 1 $ami_count); do
                ami=$(echo $amis | jq '.['"$i-1"']')
                ami_region=$(echo $ami | jq -r 'split(":") | .[0]')
                ami_id=$(echo $ami | jq -r 'split(":") | .[1]')
                if [ "$ami_region" == "$REGION" ]; then
                  export IMAGE_NAME="$ami_id"
                fi
                shipctl put_resource_state_multi $JOB_NAME "$ami_region=$ami_id"
              done
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: base_aws_x8664_c7
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: rt_creds
        switch: off
      - IN: knodeInit_repo
      - IN: pipes_repo
        switch: off
      - IN: shipbits_aws_pem
        switch: off
      - IN: c7java_x8664_img
      - IN: c7node_x8664_img
      - IN: c7cpp_x8664_img
      - IN: c7go_x8664_img
      - TASK:
          name: base_aws_x8664_c7
          runtime:
            options:
              env:
                - OS: "CentOS_7"
                - ARCHITECTURE: x86_64
                - SOURCE_AMI: "ami-ae7bfdb8"
                - VPC_ID: "vpc-01e8b399ee4f9f356"
                - SUBNET_ID: "subnet-06de95b9fc0434531"
                - REGION: "us-east-1"
                - SYSTEM_RUNTIME_LANGUAGE_VERSION: "master"
                - RUNTIME_VERSION: "master"
                - DOCKER_IMAGE_REGISTRY_URL: "pipelines-docker.jfrog.io"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "base"
                - DOCKER_VERSION: "18.09"
                - SSH_USERNAME: "centos"
                - SSH_BASTION_HOST: "3.227.114.92"
                - SSH_BASTION_USERNAME: "ec2-user"
                - SSH_BASTION_PRIVATE_KEY_PATH: "/tmp/ship-bits.pem"
                - PEM_KEY_RES: "shipbits_aws_pem"
          script:
            - export RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - export RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - export RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - export SOURCE_REPOSITORY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - ./addPemKey.sh
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - export IMAGE_NAME=$(cat output.txt | awk -F, '$0 ~/artifact,0,id/ {print $6}' | cut -d':' -f 2)
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME" "SYSTEM_RUNTIME_LANGUAGE_VERSION=$SYSTEM_RUNTIME_LANGUAGE_VERSION"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: patch_aws_x8664_c7
    type: runSh
    dependencyMode: strict
    steps:
      - IN: aws_ship_bits_creds
        switch: off
      - IN: github_creds
        switch: off
      - IN: pipes_repo
        switch: off
      - IN: base_aws_x8664_c7
      - IN: c7repLib_x8664_build
      - IN: c7_reqExec_x8664_pack
      - IN: kreqKick_repo
      - IN: knodeInit_repo
      - IN: kexecTemplates_repo
      - TASK:
          name: patch_aws_x8664_c7
          runtime:
            options:
              env:
                - OS: "CentOS_7"
                - ARCHITECTURE: x86_64
                - VPC_ID: "vpc-266f3241"
                - SUBNET_ID: "subnet-6df12f24"
                - SECURITY_GROUP_ID: "sg-f634518c"
                - REGION: "us-east-1"
                - RUNTIME_VERSION: "master"
                - GITHUB_USERNAME: "avinci"
                - PROVIDER: "aws"
                - IMG_TYPE: "patch"
                - DOCKER_VERSION: "18.09"
                - BASE_IMG_RES: "base_aws_x8664_c7"
                - SSH_USERNAME: "centos"
                # add comma separated regions where the AMI has to be copied to
                - AMI_REGIONS: "ap-south-1"
          script:
            - export SOURCE_AMI=$(shipctl get_resource_version_key "$BASE_IMG_RES" "IMAGE_NAME")
            - export GITHUB_API_KEY=$(shipctl get_integration_resource_field "github_creds" "TOKEN")
            - export AWS_ACCESS_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "ACCESSKEY")
            - export AWS_SECRET_KEY=$(shipctl get_integration_resource_field "aws_ship_bits_creds" "SECRETKEY")
            - pushd $(shipctl get_resource_state "pipes_repo")/machineImages/$PROVIDER/$ARCHITECTURE/$OS/$IMG_TYPE
            - cp -R ../../../templates/linux/* .
            - cp -R ../../../templates/$IMG_TYPE/* .
            - shipctl replace vars.json
            - packer validate -var-file=vars.json packer.json
            - export AWS_POLL_DELAY_SECONDS=10
            - export AWS_MAX_ATTEMPTS=600
            - packer build -machine-readable -var-file=vars.json packer.json 2>&1 | tee output.txt
            - cat manifest.json | jq .
            - |
              export amis=$(cat manifest.json | jq -r '.builds[0].artifact_id | split(",")')
              export ami_count=$(echo $amis | jq '. | length')
              for i in $(seq 1 $ami_count); do
                ami=$(echo $amis | jq '.['"$i-1"']')
                ami_region=$(echo $ami | jq -r 'split(":") | .[0]')
                ami_id=$(echo $ami | jq -r 'split(":") | .[1]')
                if [ "$ami_region" == "$REGION" ]; then
                  export IMAGE_NAME="$ami_id"
                fi
                shipctl put_resource_state_multi $JOB_NAME "$ami_region=$ami_id"
              done
            - '[ ! -z "$IMAGE_NAME" ]'
    on_success:
      - script: shipctl put_resource_state_multi $JOB_NAME "versionName=$IMAGE_NAME" "IMAGE_NAME=$IMAGE_NAME"
      - script: shipctl put_resource_state_multi $JOB_NAME "RUNTIME_VERSION=$RUNTIME_VERSION" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS" "DOCKER_VERSION=$DOCKER_VERSION"
    on_failure:
      - script: cat $(shipctl get_resource_state pipes_repo)/aws/$ARCHITECTURE/$OS/output.txt

  - name: u18repLib_x8664_build
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: kermit_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: reports_lib_build
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "Ubuntu_18.04"
                - TAG_VER: "master"
                - RES_REPO: "kermit_repo"
                - S3_URL: "s3://shippable-artifacts/pipelines-reports"
                - WEB_URL: "https://s3.amazonaws.com/shippable-artifacts/pipelines-reports"
                - REPORTS_PATH: "./gol/src/github.com/Shippable/reports"
                - RT_TARGET_REPO: pipelines-artifacts/reports
              imageName: drydock/u16golall
              imageTag: v7.2.4
          script:
            - source "/root/.gvm/scripts/gvm"
            - gvm use go1.11.5
            ## This removes the line from /etc/drydock/.env file that sources gvm command.
            - cd /etc/drydock/ && sed -i '$ d' .env
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - TAR_FILENAME="reports-$TAG_VER-$ARCHITECTURE-$OS.tar.gz"
            - REPO_DIR=$(shipctl get_resource_state "$RES_REPO")
            - export GOPATH="$REPO_DIR/gol"
            - pushd $REPO_DIR
            - pushd $REPORTS_PATH
            - ./package/$ARCHITECTURE/$OS/package.sh
            - tar -zcvf "$TAR_FILENAME" -C "$GOPATH/bin" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_URL/$TAG_VER/"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - RT_FILE_PATH=$RT_TARGET_REPO/$TAR_FILENAME
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $RT_FILE_PATH
            - popd
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "TAG_VER=$TAG_VER" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "WEB_URL=$WEB_URL" "S3_URL=$S3_URL" "TAR_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS"
        - shipctl put_resource_state_multi $JOB_NAME "RT_TARGET_REPO=$RT_TARGET_REPO" "RT_FILE_PATH=$RT_FILE_PATH"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: u16repLib_x8664_build
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: kermit_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: reports_lib_build
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "Ubuntu_16.04"
                - TAG_VER: "master"
                - RES_REPO: "kermit_repo"
                - S3_URL: "s3://shippable-artifacts/pipelines-reports"
                - WEB_URL: "https://s3.amazonaws.com/shippable-artifacts/pipelines-reports"
                - REPORTS_PATH: "./gol/src/github.com/Shippable/reports"
                - RT_TARGET_REPO: pipelines-artifacts/reports
              imageName: drydock/u16golall
              imageTag: v7.2.4
          script:
            - source "/root/.gvm/scripts/gvm"
            - gvm use go1.11.5
            ## This removes the line from /etc/drydock/.env file that sources gvm command.
            - cd /etc/drydock/ && sed -i '$ d' .env
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - TAR_FILENAME="reports-$TAG_VER-$ARCHITECTURE-$OS.tar.gz"
            - REPO_DIR=$(shipctl get_resource_state "$RES_REPO")
            - export GOPATH="$REPO_DIR/gol"
            - pushd $REPO_DIR
            - pushd $REPORTS_PATH
            - ./package/$ARCHITECTURE/$OS/package.sh
            - tar -zcvf "$TAR_FILENAME" -C "$GOPATH/bin" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_URL/$TAG_VER/"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - RT_FILE_PATH=$RT_TARGET_REPO/$TAR_FILENAME
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $RT_FILE_PATH
            - popd
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "TAG_VER=$TAG_VER" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "WEB_URL=$WEB_URL" "S3_URL=$S3_URL" "TAR_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS"
        - shipctl put_resource_state_multi $JOB_NAME "RT_TARGET_REPO=$RT_TARGET_REPO" "RT_FILE_PATH=$RT_FILE_PATH"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: c7repLib_x8664_build
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: kermit_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: reports_lib_build
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "CentOS_7"
                - TAG_VER: "master"
                - RES_REPO: "kermit_repo"
                - S3_URL: "s3://shippable-artifacts/pipelines-reports"
                - WEB_URL: "https://s3.amazonaws.com/shippable-artifacts/pipelines-reports"
                - REPORTS_PATH: "./gol/src/github.com/Shippable/reports"
                - RT_TARGET_REPO: pipelines-artifacts/reports
              imageName: drydock/u16golall
              imageTag: v7.2.4
          script:
            - source "/root/.gvm/scripts/gvm"
            - gvm use go1.11.5
            ## This removes the line from /etc/drydock/.env file that sources gvm command.
            - cd /etc/drydock/ && sed -i '$ d' .env
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - TAR_FILENAME="reports-$TAG_VER-$ARCHITECTURE-$OS.tar.gz"
            - REPO_DIR=$(shipctl get_resource_state "$RES_REPO")
            - export GOPATH="$REPO_DIR/gol"
            - pushd $REPO_DIR
            - pushd $REPORTS_PATH
            - ./package/$ARCHITECTURE/$OS/package.sh
            - tar -zcvf "$TAR_FILENAME" -C "$GOPATH/bin" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_URL/$TAG_VER/"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - RT_FILE_PATH=$RT_TARGET_REPO/$TAR_FILENAME
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $RT_FILE_PATH
            - popd
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "TAG_VER=$TAG_VER" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "WEB_URL=$WEB_URL" "S3_URL=$S3_URL" "TAR_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS"
        - shipctl put_resource_state_multi $JOB_NAME "RT_TARGET_REPO=$RT_TARGET_REPO" "RT_FILE_PATH=$RT_FILE_PATH"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: rhel7repLib_x8664_build
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: kermit_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: reports_lib_build
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "RHEL_7"
                - TAG_VER: "master"
                - RES_REPO: "kermit_repo"
                - S3_URL: "s3://shippable-artifacts/pipelines-reports"
                - WEB_URL: "https://s3.amazonaws.com/shippable-artifacts/pipelines-reports"
                - REPORTS_PATH: "./gol/src/github.com/Shippable/reports"
                - RT_TARGET_REPO: pipelines-artifacts/reports
              imageName: drydock/u16golall
              imageTag: v7.2.4
          script:
            - source "/root/.gvm/scripts/gvm"
            - gvm use go1.11.5
            ## This removes the line from /etc/drydock/.env file that sources gvm command.
            - cd /etc/drydock/ && sed -i '$ d' .env
            - REPO_COMMIT=$(shipctl get_resource_version_key "$RES_REPO" "shaData.commitSha")
            - TAR_FILENAME="reports-$TAG_VER-$ARCHITECTURE-$OS.tar.gz"
            - REPO_DIR=$(shipctl get_resource_state "$RES_REPO")
            - export GOPATH="$REPO_DIR/gol"
            - pushd $REPO_DIR
            - pushd $REPORTS_PATH
            - ./package/$ARCHITECTURE/$OS/package.sh
            - tar -zcvf "$TAR_FILENAME" -C "$GOPATH/bin" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_URL/$TAG_VER/"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - RT_FILE_PATH=$RT_TARGET_REPO/$TAR_FILENAME
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $RT_FILE_PATH
            - popd
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "TAG_VER=$TAG_VER" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "WEB_URL=$WEB_URL" "S3_URL=$S3_URL" "TAR_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$ARCHITECTURE" "OS=$OS"
        - shipctl put_resource_state_multi $JOB_NAME "RT_TARGET_REPO=$RT_TARGET_REPO" "RT_FILE_PATH=$RT_FILE_PATH"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: u18cpp_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u18cpp_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u18cpp_x8664_img
      - TASK:
          name: u18cpp_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u18cpp"
                - REL_VER: "master"
                - IMG_OUT: "u18cpp_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u18cpp_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u18cpp_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u18go_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u18go_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u18go_x8664_img
      - TASK:
          name: u18go_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u18go"
                - REL_VER: "master"
                - IMG_OUT: "u18go_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u18go_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u18go_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u18java_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u18java_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u18java_x8664_img
      - TASK:
          name: u18java_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u18java"
                - REL_VER: "master"
                - IMG_OUT: "u18java_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u18java_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u18java_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u18node_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u18node_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u18node_x8664_img
      - TASK:
          name: u18node_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u18node"
                - REL_VER: "master"
                - IMG_OUT: "u18node_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u18node_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u18node_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: c7node_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: c7node_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: c7node_x8664_img
      - TASK:
          name: c7node_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "c7node"
                - REL_VER: "master"
                - IMG_OUT: "c7node_x8664_img"
          script:
            - pushd $(shipctl get_resource_state c7node_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "c7node_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: c7go_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: c7go_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: c7go_x8664_img
      - TASK:
          name: c7go_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "c7go"
                - REL_VER: "master"
                - IMG_OUT: "c7go_x8664_img"
          script:
            - pushd $(shipctl get_resource_state c7go_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "c7go_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: c7cpp_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: c7cpp_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: c7cpp_x8664_img
      - TASK:
          name: c7cpp_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "c7cpp"
                - REL_VER: "master"
                - IMG_OUT: "c7cpp_x8664_img"
          script:
            - pushd $(shipctl get_resource_state c7cpp_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "c7cpp_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: c7java_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: c7java_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: c7java_x8664_img
      - TASK:
          name: c7java_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "c7java"
                - REL_VER: "master"
                - IMG_OUT: "c7java_x8664_img"
          script:
            - pushd $(shipctl get_resource_state c7java_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "c7java_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u16node_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u16node_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u16node_x8664_img
      - TASK:
          name: u16node_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u16node"
                - REL_VER: "master"
                - IMG_OUT: "u16node_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u16node_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u16node_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u16go_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u16go_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u16go_x8664_img
      - TASK:
          name: u16go_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u16go"
                - REL_VER: "master"
                - IMG_OUT: "u16go_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u16go_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u16node_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u16cpp_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u16cpp_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u16cpp_x8664_img
      - TASK:
          name: u16cpp_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u16cpp"
                - REL_VER: "master"
                - IMG_OUT: "u16cpp_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u16cpp_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u16cpp_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: u16java_x8664_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u16java_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - OUT: u16java_x8664_img
      - TASK:
          name: u16java_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u16java"
                - REL_VER: "master"
                - IMG_OUT: "u16java_x8664_img"
          script:
            - pushd $(shipctl get_resource_state u16java_repo)
            - REPO_COMMIT=$(shipctl get_resource_version_key "u16java_repo" "shaData.commitSha")
            - docker build -t=$IMG_REPO_NAME/$IMG_NAME:$REL_VER .
            - docker push $IMG_REPO_NAME/$IMG_NAME:$REL_VER
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker tag $IMG_REPO_NAME/$IMG_NAME:$REL_VER $RT_IMG_NAME:$REL_VER
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"
        - shipctl put_resource_state_multi $IMG_OUT "versionName=$REL_VER" "IMG_REPO_COMMIT_SHA=$REPO_COMMIT"

  - name: distrobase_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: distrobase_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - TASK:
          name: distrobase_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "distrobase"
                - REL_VER: "master"
                - IMG_JFROG_OUT: "distrobase_img"
          script:
            - pushd $(shipctl get_resource_state distrobase_repo)
            - export IMG_NAME="$IMG_REPO_NAME/$IMG_NAME"
            - ./build.sh
            - docker push "${IMG_NAME}:${REL_VER}"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-distrobase"
            - docker tag "${IMG_NAME}:${REL_VER}" $RT_IMG_NAME:$REL_VER
            - echo "Pushing $IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: distrobase_img
    on_success:
      script:
        - shipctl put_resource_state_multi $IMG_JFROG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: u16microbase_build
    type: runSh
    dependencyMode: strict
    steps:
      - IN: u16microbase_repo
      - IN: drydock_cli
      - IN: rt_creds
        switch: off
      - TASK:
          name: u16microbase_build
          runtime:
            options:
              env:
                - IMG_REPO_NAME: "drydock"
                - IMG_NAME: "u16microbase"
                - REL_VER: "master"
                - IMG_JFROG_OUT: "u16microbase_img"
          script:
            - pushd $(shipctl get_resource_state u16microbase_repo)
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - RT_REGISTRY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY")
            - RT_REGISTRY_KEY=$(shipctl get_integration_resource_field "rt_creds" "REGISTRY_KEY")
            - RT_IMG_NAME="$RT_REGISTRY/jfrog/pipelines-$IMG_NAME"
            - docker build -t=$RT_REGISTRY/jfrog/pipelines-$IMG_NAME:$REL_VER .
            - echo "Pushing $RT_REGISTRY/jfrog/pipelines-$IMG_NAME:$REL_VER to Artifactory"
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - jfrog rt docker-push $RT_IMG_NAME:$REL_VER $RT_REGISTRY_KEY --build-name=$JOB_NAME --build-number=$BUILD_NUMBER
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
      - OUT: u16microbase_img
    on_success:
      script:
        - shipctl put_resource_state_multi $IMG_JFROG_OUT "versionName=$REL_VER" "commitSha=$REPO_COMMIT" "IMG_NAME=$IMG_NAME" "IMG_TAG=$REL_VER"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: u18_reqExec_x8664_pack
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: reqExec_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: u18_reqexec_pack
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "Ubuntu_18.04"
                - ARTIFACTS_BUCKET: "s3://shippable-artifacts/pipelines-reqExec"
                - REL_VER: "master"
                - BINARY_DIR: "/tmp/reqExec"
          script:
            - pushd $(shipctl get_resource_state "reqExec_repo")
            - REPO_COMMIT=$(shipctl get_resource_version_key "reqExec_repo" "shaData.commitSha")
            - TAR_FILENAME="reqExec-$REL_VER-$ARCHITECTURE-$OS.tar.gz"
            - PACK_SCRIPT="./package/$ARCHITECTURE/$OS/package.sh"
            - S3_BUCKET_BINARY_DIR="$ARTIFACTS_BUCKET/$REL_VER/"
            - $PACK_SCRIPT
            - mkdir -p $BINARY_DIR && cp -r dist $BINARY_DIR
            - tar -zcvf "$TAR_FILENAME" -C "$BINARY_DIR" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_BUCKET_BINARY_DIR"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - TARGET_LOCATION=pipelines-artifacts/reqExec
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $TARGET_LOCATION/$TAR_FILENAME
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "S3_BUCKET=$S3_BUCKET_BINARY_DIR" "S3_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$RCHITECTURE" "OS=$OS"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: u16_reqExec_x8664_pack
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: reqExec_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: u16_reqexec_pack
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "Ubuntu_16.04"
                - ARTIFACTS_BUCKET: "s3://shippable-artifacts/pipelines-reqExec"
                - REL_VER: "master"
                - BINARY_DIR: "/tmp/reqExec"
          script:
            - pushd $(shipctl get_resource_state "reqExec_repo")
            - REPO_COMMIT=$(shipctl get_resource_version_key "reqExec_repo" "shaData.commitSha")
            - TAR_FILENAME="reqExec-$REL_VER-$ARCHITECTURE-$OS.tar.gz"
            - PACK_SCRIPT="./package/$ARCHITECTURE/$OS/package.sh"
            - S3_BUCKET_BINARY_DIR="$ARTIFACTS_BUCKET/$REL_VER/"
            - $PACK_SCRIPT
            - mkdir -p $BINARY_DIR && cp -r dist $BINARY_DIR
            - tar -zcvf "$TAR_FILENAME" -C "$BINARY_DIR" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_BUCKET_BINARY_DIR"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - TARGET_LOCATION=pipelines-artifacts/reqExec
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $TARGET_LOCATION/$TAR_FILENAME
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "S3_BUCKET=$S3_BUCKET_BINARY_DIR" "S3_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$RCHITECTURE" "OS=$OS"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: c7_reqExec_x8664_pack
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: reqExec_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: c7_reqexec_pack
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "CentOS_7"
                - ARTIFACTS_BUCKET: "s3://shippable-artifacts/pipelines-reqExec"
                - REL_VER: "master"
                - BINARY_DIR: "/tmp/reqExec"
          script:
            - pushd $(shipctl get_resource_state "reqExec_repo")
            - REPO_COMMIT=$(shipctl get_resource_version_key "reqExec_repo" "shaData.commitSha")
            - TAR_FILENAME="reqExec-$REL_VER-$ARCHITECTURE-$OS.tar.gz"
            - PACK_SCRIPT="./package/$ARCHITECTURE/$OS/package.sh"
            - S3_BUCKET_BINARY_DIR="$ARTIFACTS_BUCKET/$REL_VER/"
            - $PACK_SCRIPT
            - mkdir -p $BINARY_DIR && cp -r dist $BINARY_DIR
            - tar -zcvf "$TAR_FILENAME" -C "$BINARY_DIR" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_BUCKET_BINARY_DIR"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - TARGET_LOCATION=pipelines-artifacts/reqExec
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $TARGET_LOCATION/$TAR_FILENAME
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "S3_BUCKET=$S3_BUCKET_BINARY_DIR" "S3_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$RCHITECTURE" "OS=$OS"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL

  - name: rhel7_reqExec_x8664_pack
    type: runSh
    triggerMode: parallel
    dependencyMode: strict
    steps:
      - IN: reqExec_repo
      - IN: kermit_bits_cli
        switch: off
      - IN: rt_creds
        switch: off
      - TASK:
          name: rhel7_reqexec_pack
          runtime:
            options:
              env:
                - ARCHITECTURE: "x86_64"
                - OS: "RHEL_7"
                - ARTIFACTS_BUCKET: "s3://shippable-artifacts/pipelines-reqExec"
                - REL_VER: "master"
                - BINARY_DIR: "/tmp/reqExec"
          script:
            - pushd $(shipctl get_resource_state "reqExec_repo")
            - REPO_COMMIT=$(shipctl get_resource_version_key "reqExec_repo" "shaData.commitSha")
            - TAR_FILENAME="reqExec-$REL_VER-$ARCHITECTURE-$OS.tar.gz"
            - PACK_SCRIPT="./package/$ARCHITECTURE/$OS/package.sh"
            - S3_BUCKET_BINARY_DIR="$ARTIFACTS_BUCKET/$REL_VER/"
            - $PACK_SCRIPT
            - mkdir -p $BINARY_DIR && cp -r dist $BINARY_DIR
            - tar -zcvf "$TAR_FILENAME" -C "$BINARY_DIR" .
            - aws s3 cp --acl public-read "$TAR_FILENAME" "$S3_BUCKET_BINARY_DIR"
            - RT_URL=$(shipctl get_integration_resource_field "rt_creds" "RT_URL")
            - RT_API_KEY=$(shipctl get_integration_resource_field "rt_creds" "API_KEY")
            - RT_USER=$(shipctl get_integration_resource_field "rt_creds" "USER")
            - jfrog rt config --url "$RT_URL" --user "$RT_USER" --apikey "$RT_API_KEY" --interactive=false
            - TARGET_LOCATION=pipelines-artifacts/reqExec
            - jfrog rt upload --build-name=$JOB_NAME --build-number=$BUILD_NUMBER $TAR_FILENAME $TARGET_LOCATION/$TAR_FILENAME
            - jfrog rt bag $JOB_NAME $BUILD_NUMBER
            - popd
    on_success:
      script:
        - shipctl put_resource_state_multi $JOB_NAME "versionName=$REPO_COMMIT" "FILE_REPO_COMMIT_SHA=$REPO_COMMIT" "S3_BUCKET=$S3_BUCKET_BINARY_DIR" "S3_FILENAME=$TAR_FILENAME" "ARCHITECTURE=$RCHITECTURE" "OS=$OS"
        - echo "Publishing build info"
        - jfrog rt bp $JOB_NAME $BUILD_NUMBER --build-url $BUILD_URL
